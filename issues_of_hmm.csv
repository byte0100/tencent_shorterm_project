repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmmlearn,hmmlearn,383494714,Setting restrictions to transition matrix in Hmmlearn,open,,https://github.com/hmmlearn/hmmlearn/issues/316,"Hello!
I am working with medicine data, especially with people who have cancer. 
Let's say that I have some  1) pre-operation observations, 2) operation and 3) post-operation observations (and also possible 4) relapse to pre-operation observations ). I would like to omit hidden states to each of these categories. 
The problem is the following. I'd like to set restrictions to transition matrix so it wouldn't be possible to jump from pre-operation states straight to post-operation states ( and vice versa), without them going through ""operation"" state (or ""relapse"" state respectively)  first.
So, in a word, I would like to learn transition and emission matrix, where I know that some values are 0 (probabilities of going to pre-op to post-op states) and would like to learn the non-zero probabilities which I don't know. 
Could it be done with the current implementation of hmmlearn or if not, is there an easy way to add it somehow?"
hmmlearn,hmmlearn,382724214,covariance is ignored,open,,https://github.com/hmmlearn/hmmlearn/issues/315,"In the example file plot_hmm_sampling.py, the emissions of each component are constrained to a diagonal regardless of attempts to set the covariance matrix to be non-diagonal, to set different types (""full"" ""diag"" etc).    
Changing the line
```
## The covariance of each component`
covars = np.tile([[0.6,0.3],[0.3,0.6]], (4, 1, 1))
```
should fix this but does not. "
hmmlearn,hmmlearn,381920644,Refactor checks and add option to skip them,open,,https://github.com/hmmlearn/hmmlearn/issues/314,"Recently I've been using `hmmlearn` to do real-time classification of sensor time-series data, and it is working nicely, but after some profiling I noticed that about half of the processing time was being spent on repeated unnecessary calls to the function `_check` in `_BaseHMM` and its subclasses. So I propose a couple of changes:

- Refactor `_checks` into two separate functions, one to do the sanitization (e.g. casting to numpy arrays, etc), and other to do the checks and raise errors as appropriate. This is so we can...

- ... add a `skip_parameter_checks` boolean argument to add the option to enable/disable the checks prior to scoring, sampling, predicting and decoding. The checks can incur in heavy computations (e.g. `_utils._validate_covars` in `GaussianHMM`) that are unnecessary in repeated calls to score/sample/decode/predict with different data.

I've been working on the above changes [here](https://github.com/alegonz/hmmlearn/tree/refactor-checks), and could throw a pull request if the above sounds good.

What do you think?"
hmmlearn,hmmlearn,373173758,Which HMM to use for discrete emissions with custom probabilities on each symbol?,open,bug,https://github.com/hmmlearn/hmmlearn/issues/310,"I'm trying to run HMM.learn() on an HMM with a large number of sequences using multinomialHMM() class. 
  Previous issues cited the error: ""ValueError: expected a sample from a Multinomial distribution""  (hmm.py line 377).   My question is, is multinomial the correct HMM subclass to use if I want to specify the probability to see each emission (discrete symbol as an integer) in each state?   In other words, are we restricted to a true multinomail distribution?  

If as a simple example, if my symbols are 0-255 integers, and for state 5 I have uniform(10-20), is multinomial appropriate through setting emissionprobs_???

Thanks"
hmmlearn,hmmlearn,359222198,Is it possible to fit a multivariate GMHMM in hmmlearn?,open,,https://github.com/hmmlearn/hmmlearn/issues/296,"Hi Sergei,

Thanks for hmmlearn, it is really great!

I know it is possible to fit several sequences into hmmlearn but it seems to me that these sequences need to be drawn from the same distributions.
Is it possible to fit a GMHMM with several observations sequences drawn from different distributions in hmmlearn?
My use case : I would like to fit a GMHMM with K financial time series from different stocks and predict the market regime that generated the K stock prices at a specified time. So the matrix input has dimension N (number of dates) × K (number of stocks). 
Thanks in advance!

Axel"
hmmlearn,hmmlearn,341989031,Error training a MULTINOMIAL HMM on MULTIPLE SEQUENCES: index N is out of bounds for axis 1 with size N,open,,https://github.com/hmmlearn/hmmlearn/issues/293,"I'm facing an error when training MULTINOMIAL HMM on MULTIPLE SEQUENCES.
It seems that the same data and code works (as exemplified in the documentation) with continuous data, Gaussian HMM (hmm.GMMHMM); but doesn't work when only changing the model to be a Multinomial HMM.

Here's the simple code and data example:

```
X1 = [[1],[2],[2],[2],[2],[4],[2],[4],[2]]
X2 = [[1],[3],[4],[2],[4],[3],[4]]
X = np.concatenate([X1, X2])
lengths = [len(X1), len(X2)]
hmm.MultinomialHMM(n_components=2).fit(X, lengths)  
# this would work: hmm.GMMHMM(n_components=2).fit(X_example, lengths_example)  
```


That gives this Index Error:
```
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-129-34edee5616c2> in <module>()
      5 X_example = np.concatenate([X1_example, X2_example])
      6 lengths_example = [len(X1_example), len(X2_example)]
----> 7 hmm.MultinomialHMM(n_components=2).fit(X_example, lengths_example)

/Users/t/anaconda2/lib/python2.7/site-packages/hmmlearn/base.pyc in fit(self, X, lengths)
    427             curr_logprob = 0
    428             for i, j in iter_from_X_lengths(X, lengths):
--> 429                 framelogprob = self._compute_log_likelihood(X[i:j])
    430                 logprob, fwdlattice = self._do_forward_pass(framelogprob)
    431                 curr_logprob += logprob

/Users/t/anaconda2/lib/python2.7/site-packages/hmmlearn/hmm.pyc in _compute_log_likelihood(self, X)
    403 
    404     def _compute_log_likelihood(self, X):
--> 405         return np.log(self.emissionprob_)[:, np.concatenate(X)].T
    406 
    407     def _generate_sample_from_state(self, state, random_state=None):

IndexError: index 4 is out of bounds for axis 1 with size 4
```

I ran this also using Python 3 and had the same error.
How to fix this?

*Note: relates to Issue #154 
https://github.com/hmmlearn/hmmlearn/issues/154"
hmmlearn,hmmlearn,333977984,GMMHMM model not accepting 'lengths' parameter,open,,https://github.com/hmmlearn/hmmlearn/issues/287,"I've the latest hmmlearn version installed: 0.2.1
But I'm unable to pass 'lengths' parameter in my fit function while implementing a GMMHMM model.
Here's the code: 
    #hmmlearn version 0.2.1

    #HMM Models
    from hmmlearn import hmm
    model_anger = hmm.GMMHMM(n_components=1,n_mix=2, covariance_type=""diag"", 
    n_iter=1000).fit(anger,lengths=lengths_anger)

This gives me the following error:
    `ValueError: 'lengths' argument is not supported yet`
Note that my hmmlearn is already in its latest version.

But the same parameters work fine on Gaussianhmm model.
Can someone guide me on how can I model gmmhmm with multiple fitting sequences?
"
hmmlearn,hmmlearn,331039652,"ValueError: component 0 of 'full' covars must be symmetric, positive-definite",open,,https://github.com/hmmlearn/hmmlearn/issues/285,"Hi, I have looked into the previous issue on that but none of them solves my problem. 
I am using GuassianHMM but It doesn't seem to train the model properly.
```python
Traceback (most recent call last):
  File ""src/playground/HMM_hmmlearn.py"", line 49, in <module>
    prob, state_seq = model.decode(x_m.T)
  File ""/Users/bowenchen/State_Space_Model_on_Gene/ssm-env/lib/python3.6/site-packages/hmmlearn/base.py"", line 294, in decode
    self._check()
  File ""/Users/bowenchen/State_Space_Model_on_Gene/ssm-env/lib/python3.6/site-packages/hmmlearn/hmm.py"", line 165, in _check
    super(GaussianHMM, self)._check()
  File ""/Users/bowenchen/State_Space_Model_on_Gene/ssm-env/lib/python3.6/site-packages/hmmlearn/base.py"", line 524, in _check
    .format(self.transmat_.sum(axis=1)))
ValueError: rows of transmat_ must sum to 1.0 (got [1. 0. 1. 1.])

def initialize():
    g = 0
    while g < G:
        x_m[:, g] = abs(
            np.matrix(np.random.multivariate_normal((theta_m.T * y_m[:, g]).flatten().tolist()[0], np.eye(E)))).T
        g += 1

if __name__ == '__main__':
    initialize()
    model = hmm.GaussianHMM(n_components=K, covariance_type=""full"",n_iter=30)
    iterate_times = 5
    final_score = -math.inf
    scores = []
    print(x_m)
    while iterate_times > 0:
        model.fit(x_m.T)
        print(model.covars_)
        prob, state_seq = model.decode(x_m.T)
        scores.append(model.score(x_m.T))
        iterate_times -= 1
    print(scores)
```
And there is the print out variable
```
x_m
[[1.68185378e-01 1.02232212e+00 2.68666556e-01 8.49854862e-01
  4.19318133e-01]
 [1.04003708e-01 8.84099494e-01 1.64639541e+00 1.61692603e+00
  1.79678081e+00]
 [5.78889521e-01 6.46720353e-01 1.51399590e+00 1.87850812e-03
  3.36270387e+00]]

covars_
[[[ 0.01243624 -0.02659715  0.03280353]
  [-0.02659715  0.13925868 -0.1131393 ]
  [ 0.03280353 -0.1131393   0.10895525]]

 [[ 0.01        0.01        0.01      ]
  [ 0.01        0.01        0.01      ]
  [ 0.01        0.01        0.01      ]]

 [[ 0.01        0.01        0.01      ]
  [ 0.01        0.01        0.01      ]
  [ 0.01        0.01        0.01      ]]

 [[ 0.01        0.01        0.01      ]
  [ 0.01        0.01        0.01      ]
  [ 0.01        0.01        0.01      ]]]
```

Anyone have some ideas?
Thanks."
hmmlearn,hmmlearn,317049501,Multinomial,open,,https://github.com/hmmlearn/hmmlearn/issues/280,"I'm trying to use hmmlearn to get the most likely hidden state sequence from a Hidden Markov Model, given start probabilities, transition probabilities, and emission probabilities.

I have two hidden states and four possible emission values, so I'm doing this:
```

num_states = 2
num_observations = 4
start_probs = np.array([0.2, 0.8])
trans_probs = np.array([[0.75, 0.25], [0.1, 0.9]])
emission_probs = np.array([[0.3, 0.2, 0.2, 0.3], [0.3, 0.3, 0.3, 0.1]])

model = hmm.MultinomialHMM(n_components=num_states)
model.startprob_ = start_probs
model.transmat_ = trans_probs
model.emissionprob_ = emission_probs

seq = np.array([[3, 3, 2, 2]]).T

model.fit(seq)
log_prob, state_seq = model.decode(seq)
```

My stack trace points to the `decode` call and throws this error:

`ValueError: too many values to unpack (expected 2)`

I thought `decode` (looking at the docs) returns a log probability and the state sequence, so I'm confused.

Any idea?

Thanks!"
hmmlearn,hmmlearn,312277611,startprob_  of ghmm is negative or nan,open,,https://github.com/hmmlearn/hmmlearn/issues/276,"the following code in hmmlearn\base.py will lead to the negative value of startprob_.

This is an obvious risk and issue in the code. 
Is it fixed now?

```Python
    def _do_mstep(self, stats):
        """"""Performs the M-step of EM algorithm.

        Parameters
        ----------
        stats : dict
            Sufficient statistics updated from all available samples.
        """"""
        # The ``np.where`` calls guard against updating forbidden states
        # or transitions in e.g. a left-right HMM.
        if 's' in self.params:
            startprob_ = self.startprob_prior - 1.0 + stats['start']
            self.startprob_ = np.where(self.startprob_ == 0.0,
                                       self.startprob_, startprob_)
            normalize(self.startprob_)
        if 't' in self.params:
            transmat_ = self.transmat_prior - 1.0 + stats['trans']
            self.transmat_ = np.where(self.transmat_ == 0.0,
                                      self.transmat_, transmat_)
            normalize(self.transmat_, axis=1)
```"
hmmlearn,hmmlearn,311751070,initial K-Means clustering in 0.2.1 has insufficient samples,open,,https://github.com/hmmlearn/hmmlearn/issues/275,"# K-Means clustering error  (`0.2.1`)
I updated to version `0.2.1` to get around previous errors (see issue #78),
however when I've run it with some of my own data I start getting the following error:

``` python
ValueError: n_samples=2 should be >= n_clusters=3
```
Looking further it seems that just prior to this an initial k-means clustering occurs on [line 603](https://github.com/hmmlearn/hmmlearn/blob/master/hmmlearn/hmm.py#L603), using `n_components` as the number of clusters. In some circumstances the number of examples assigned to a cluster ends up being lower* than the value for `n_mix`, which results in an error during the second k-means clustering on [line 608](https://github.com/hmmlearn/hmmlearn/blob/master/hmmlearn/hmm.py#L608).

> ***Note:** so far this value for `n_clusters` has always been 2 when this error occurs

This becomes a problem when I'm still using relatively small values:
- `n_components = 6`
- `n_mix = 3`

## Error Log:
```
ValueError                                Traceback (most recent call last)
<ipython-input-5-0c105a155831> in <module>()
    110         pbar.update(1)
    111         # train x model
--> 112         zx = train_model(xi)
    113         Zx.append(zx)
    114         pbar.update(TS)

<ipython-input-5-0c105a155831> in train_model(x)
     53 def train_model(x):
     54     model = hmm.GMMHMM(N,M,covariance_type=COVAR, n_iter=ITERS)
---> 55     model.fit(x)
     56     z = model.predict(x)
     57     return z

/usr/local/lib/python3.5/dist-packages/hmmlearn-0.2.1-py3.5-linux-x86_64.egg/hmmlearn/base.py in fit(self, X, lengths)
    422         """"""
    423         X = check_array(X)
--> 424         self._init(X, lengths=lengths)
    425         self._check()
    426 

/usr/local/lib/python3.5/dist-packages/hmmlearn-0.2.1-py3.5-linux-x86_64.egg/hmmlearn/hmm.py in _init(self, X, lengths)
    606             kmeans = cluster.KMeans(n_clusters=self.n_mix,
    607                                     random_state=self.random_state)
--> 608             kmeans.fit(X[np.where(labels == label)])
    609             kmeanses.append(kmeans)
    610 

/usr/lib/python3/dist-packages/sklearn/cluster/k_means_.py in fit(self, X, y)
    810         """"""
    811         random_state = check_random_state(self.random_state)
--> 812         X = self._check_fit_data(X)
    813 
    814         self.cluster_centers_, self.labels_, self.inertia_, self.n_iter_ = \

/usr/lib/python3/dist-packages/sklearn/cluster/k_means_.py in _check_fit_data(self, X)
    787         if X.shape[0] < self.n_clusters:
    788             raise ValueError(""n_samples=%d should be >= n_clusters=%d"" % (
--> 789                 X.shape[0], self.n_clusters))
    790         return X
    791 

ValueError: n_samples=2 should be >= n_clusters=3
```"
hmmlearn,hmmlearn,308699758,Error for 'covariance_type'  in GaussianHMM model ,open,,https://github.com/hmmlearn/hmmlearn/issues/274,"X is a vector of samples, and I got a trained model from it. I make a copy of this model because I want to do some operations with this copy (like modify the order of the means, or the startprob, etc).  I create a new model from scratch and setup the model as you see in the code below. The problem is when I try to do a prediction of my X vector:

```python
import warnings
warnings.filterwarnings(""ignore"", category=DeprecationWarning)
warnings.filterwarnings(""ignore"", category=RuntimeWarning)
from hmmlearn.hmm import GaussianHMM
import numpy as np

#samples:
X = np.array([[-1.03573482, -1.03573482],
       [ 6.62721065, 11.62721065],
       [ 3.19196949,  8.19196949],
       [ 0.38798214,  0.38798214],
       [ 2.56845104,  7.56845104],
       [ 5.03699793, 10.03699793],
       [ 5.87873937, 10.87873937],
       [ 4.27000819, -1.72999181],
       [ 4.02692237, -1.97307763],
       [ 5.7222677 , 10.7222677 ]])``


# Trainning a new model over samples:
model = GaussianHMM(n_components=3, covariance_type=""diag"").fit(X)

# Create a new copy of the trained model: 
new_model = GaussianHMM(n_components=3, covariance_type=""diag"")
new_model.startprob_ = model.startprob_
new_model.transmat_ = model.transmat_ 
new_model.means_ = model.means_
new_model.covars_ = model.covars_

# Predict from X:
X_N, Z_N = new_model.predict(X) 
```
The predict function gives me the following error:
``` python
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""\Python\Python36\site-packages\hmmlearn\base.py"", line 334, in predict
    _, state_sequence = self.decode(X, lengths)
  File ""\Python36\site-packages\hmmlearn\base.py"", line 294, in decode
    self._check()
  File ""\Python36\site-packages\hmmlearn\hmm.py"", line 175, in _check
    self.n_components)
  File ""\lib\site-packages\sklearn\mixture\gmm.py"", line 763, in _validate_covars
    raise ValueError(""'diag' covars must have shape ""
ValueError: 'diag' covars must have shape (n_components, n_dim)
```

It seems like there is a problem in the 'covars' array due to the covariance_type. But since both model has 'covariance_type' = 'diag'. I don't realize why this error exists? Could you help me to fix this error."
hmmlearn,hmmlearn,306395924,Feature request: log likelihood of each sequence,open,enhancement,https://github.com/hmmlearn/hmmlearn/issues/272,"Hello,

The current implementation of HMMs provides quite a few functions returning the likelihood of the data under the trained model such as `predict_proba`, `score` and `score_samples`, but lacks a function which would return the likelihood of each action sequence (instead of a likelihood per sample, or per component). This feature is essential to several applications such as sequence-based anomaly detection.

A first draft of the function could be as follows:

```
def score_samples(self, X, lengths=None)
    check_is_fitted(self, ""startprob_"")
    self._check()

    X = check_array(X)
    logprob = []
    for i, j in iter_from_X_lengths(X, lengths):
        framelogprob = self._compute_log_likelihood(X[i:j])
        logprob.append(self._do_forward_pass(framelogprob)[0])
    return np.array(logprob)
```

What do you think?"
hmmlearn,hmmlearn,290024410,"cannot use ""covariance_type=full""",open,,https://github.com/hmmlearn/hmmlearn/issues/268,"I am using the hmmlearn-0.2.0 installed in my virtualenv by 

pip install hmmlearn

The code worked fine when use covariance_type = ""diag"".
But when change to covariance_type = ""full"", it raises error:

File ""/home/brianyao/home/brianyao/Envs/tf/lib/python3.5/site-packages/hmmlearn/hmm.py"", line 622, in _accumulate_sufficient_statistics
+ tmp_gmm.means_**2) * cvnorm
ValueError: operands could not be broadcast together with shapes (2,5,5) (2,5)

Apparently it means that the the covariance size is still like a diag so that the computed full covariance cannot be broadcasted. How to fix it? "
hmmlearn,hmmlearn,283691892,Is there any option for using lazy viterbi for decoding?,open,enhancement,https://github.com/hmmlearn/hmmlearn/issues/264,
hmmlearn,hmmlearn,282617452,"Training completes after 3 iterations. Score returns positive values. ""Full"" covariance matrices are not symmetric, positive-definite",open,,https://github.com/hmmlearn/hmmlearn/issues/263,"Hello,

My training data consists of MFCC coefficients, that is: an observations are 24 mfcc coefficients (incuding delta ones), sequence (audio file) has about 100 of observations, and there are 10 sequences. Here's the file 
[obs.txt](https://github.com/hmmlearn/hmmlearn/files/1564698/obs.txt). Now when I put these into GMMHMM, fitting finishes after 3 iterations (on diagonal covariances) and score returns a positive value(3-4 thousands). On full covariance_type I get the ValueError of these covariances not being symmetric, positive-definite. 

wtf?

I am using 0.2.1 version, and I've read how to handle multiple sequences.
Thanks in advance.


EDIT: It might well be that my data is a bit not-independent, i.e. singular."
hmmlearn,hmmlearn,278553607,"Known, Time-Varying Emission Probabilities",open,enhancement,https://github.com/hmmlearn/hmmlearn/issues/260,"I'm working in a simple setting where the means of the (Gaussian) emission distributions of the states are known beforehand, but change over time. The particular application is eye-tracking data, where I assume the subject is looking at one of several objects (and their eye-tracking data are normally distributed around the object), and would like to infer the object at which the subject is looking at each point in time, but the objects are _moving over time_.

My question is whether there is a simple way of using hmmlearn with these (known) time-varying means? I couldn't find any information about, say, running Viterbi with non-constant emission distributions.

If there were only one state, I could normalize the data by subtracting the mean of the emission distribution at each time point, but this mean is different for different states, so I don't think this kind of approach works.

For now, I can make reasonable guesses for the model parameters (the variance of the emission distributions, the starting probabilities and the transition matrix), but, eventually, I would also like the transition matrix to vary over time (e.g., the transition probability between two objects is probably a function of the distance between the two objects)."
hmmlearn,hmmlearn,275378110,GMMHMM conversion fail (version 0.2.1),open,,https://github.com/hmmlearn/hmmlearn/issues/256,"This works:
```
from hmmlearn.hmm import GMMHMM
import numpy as np
import random as r
# Create model
model = GMMHMM(n_components=2, n_mix=2, n_iter=500, tol=1e-4)
# Create data with two alternating states.
data_train = []
for _ in range(10000):
        data_train.append([r.choice((0, 2)) - 0.1 + r.random() * 0.2])
        data_train.append([r.choice((1, 3)) - 0.1 + r.random() * 0.2])
data_train = np.vstack(data_train)
# Train
model.fit(data_train)
print(model.transmat_)  # np.array([[0, 1], [1, 0]])
```
Changing only the data, this does not converge properly anymore:
```
from hmmlearn.hmm import GMMHMM
import numpy as np
import random as r
# Create model
model = GMMHMM(n_components=2, n_mix=2, n_iter=500, tol=1e-4)
# Create data with two alternating states.
data_train = []
for _ in range(10000):
        data_train.append([r.choice((0, 3)) - 0.1 + r.random() * 0.2])
        data_train.append([r.choice((1, 2)) - 0.1 + r.random() * 0.2])
data_train = np.vstack(data_train)
# Train
model.fit(data_train)
print(model.transmat_)  # Expecting np.array([[0, 1], [1, 0]])
```"
hmmlearn,hmmlearn,271084878,Working with multiple sequences - shape of input and sequences of different length,open,,https://github.com/hmmlearn/hmmlearn/issues/249,"I have 3 sequences A, B, C each containing 5 points (in other words same length).
```
A = np.array([10, 11, 12, 13, 14])
B = np.array([20, 21, 22, 23, 24])
C = np.array([30, 31, 32, 33, 34])

```
According to the documentation the input for the fit function has to be:
`X = np.concatenate((A, B, C))` and when I call the .fit function it has to be in the following form:
`model.fit(X, [len(A), len(B), len(C)])`

So in other words the input form must have `shape(len(A) + len(B) + len(C),1)`. Is that correct?

Additional questions:
1) In the documentation for `.fit` it mentions in the parameters that: 
 `X : array-like, shape (n_samples, n_features)` .
 This is coming in conflict with the above point about the shape and the input shape instead should be (in the case that the sequences have same length) `shape(len(A), 3)` for the above example. Am I missing something?

2) If yes, what is happening (how is it training) whenever we have sequences of different length? 

"
hmmlearn,hmmlearn,270451689,Is predict_proba() giving the emission probabilities?,open,,https://github.com/hmmlearn/hmmlearn/issues/248,"Supposing I have a GaussianHMM(n_components = 3) model. After I train it on my X dataset (X array with shape(5,1)). I run the command model.predict_proba(X) and it is giving out a 5 row by 3 column array. 

What exactly is that array showing me? Is it the emission probabilities in other words P(X|S) -where S is the state-?
"
hmmlearn,hmmlearn,270438026,Tagged release for 0.2.1,open,,https://github.com/hmmlearn/hmmlearn/issues/247,"It would seem that HMMLearn was given a version bump on [March 2, 2016](https://github.com/hmmlearn/hmmlearn/commit/a07261706510421c02f071d5e5d5a604a24dfffe), but no tag was created nor release built. Is that simply a packaging issue, or is 0.2.1 still in pre-release?

If 0.2.1 truly was released, I'd like to request that a tag be added and a release be created for it. My interest here is building a conda-forge package."
hmmlearn,hmmlearn,248711900,Handling 0's in emissionprob_,open,,https://github.com/hmmlearn/hmmlearn/issues/223,"It seems like that 0's are not tolerated in the Multinomial output emissions and
cause a renormalization:
For example,
(Pdb) model=hmm.MultinomialHMM(n_components=2)

(Pdb) model.emissionprob_=np.array([[1.0, 0],[0,1.0]])
(Pdb) model.emissionprob_
array([[  5.00000000e-01,   1.11022302e-16],
       [  1.11022302e-16,   5.00000000e-01]])

However if the entries are changed so then 0's are replaced with a small number this
renormalization does not happen.   Is this a bug or a feature?

(Pdb) model.emissionprob_=np.array([[1.0e-17, 1.0],[1.0,1.0e-17]])
(Pdb) model.emissionprob_
array([[  1.00000000e-17,   1.00000000e+00],
       [  1.00000000e+00,   1.00000000e-17]])

I think the output of the first case should be 

array([[  1.0   1.11022302e-16],
       [  1.11022302e-16,   1.0 ]])


"
hmmlearn,hmmlearn,244940667,Prediction of Multivariate Multinomial HMM,open,,https://github.com/hmmlearn/hmmlearn/issues/216,"I followed the tutorial and trained Multinomial HMM with training data shape (num_samples, num_feat = 4). For testing, I used the predict function on test_data (num_samples, num_feat=4). 
The predict/decode function throws an error, because dimension mismatch. It looks like the decoding output is (num_samples*num_feat, ), which is different from what I expected (num_samples).
When I switched to GaussianHMM, it works okay.
Can anyone have suggestion how to work on multivariate multinomial hmms?"
hmmlearn,hmmlearn,231930245,How would one add sample weights?,open,enhancement,https://github.com/hmmlearn/hmmlearn/issues/206,"Hi,

I see from the documentation that the fit() function doesn't allow us to specify weights for multiple samples. 

What I would like to do is to implement discriminative training, so not only reward better log likelihood for the representatives of the class being fitted, but at the same time punish log likelihood for representatives of classes I'd like to distinguish it from. 

It seems this could be achieved quite easily by using a weighted log likelihood in the optimization - could someone perhaps point me to the positions in the code where that change should happen?

I'll have a read myself as well of course, but sometimes there are non-trivial interactions between different code pieces

Right now I'm doing it by first fitting the class itself in the usual way, and then running gradient descent on the weighted log likelihood measure described above, computing the gradient brute force from the score() function. It seems to work, but is very slow - I'd much rather tap into the EM optimization of the fit() method if I can.

Thanks!"
hmmlearn,hmmlearn,229059818,GMM competability with Scikit-Learn,open,enhancement,https://github.com/hmmlearn/hmmlearn/issues/202,"The code used to have _gmm property inside the GMMHMM class containing a list of gmm models (per state). Now the entire GMMHMM was rewritten to be self contained without depending on scikit-learn. I advice to not follow this root but rather rely on the existing GMM implementation in scikit-learn. This is why:

1. hmmlearn users can enjoy progress done on the efficiency and features of the GMM class in scikit-learn
2. Once can use hmmlearn for a semisupevised scenario when you would like to pre-train some of the states using scikit-learn and then inject them into hmmlearn as starting point or as fixed states (not to be learned any more).

If owner agree, I support returning to the code version where _gmm was present and replace the internal gmm implementation to the the scikit-learn one. 
"
hmmlearn,hmmlearn,219023649,Number of observable symbols in MultinomialHMM?,open,,https://github.com/hmmlearn/hmmlearn/issues/193,"Hello,

I've been fiddling about with the MultinomialHMM class, and have a few questions:

It seems that the implementation of the model is unable to handle a set of observable symbols that is bigger than the number of states. I haven't found anywhere that this is a limitation of the mathematical model, so it seems a bit like an arbitrary limitation to me. Have I misunderstood something here? Maybe having more observable symbols than states makes no mathematical sense, and is redundant in some way? I honestly don't know and am curious.

The fit function's parameter X has the shape (n_samples, n_features), according to the code comments 
https://github.com/hmmlearn/hmmlearn/blob/master/hmmlearn/base.py#L411
and the API documentation,
http://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn.base._BaseHMM.fit
If n_features is the number of possible symbols emitted (as in, the size of the set which the model ""picks from"" whenever a symbol is to be emitted), should the shape really be (n_samples, n_features)? To me, (n_samples, 1) feels like a more reasonable shape, or if multi dimensional symbols are allowed, (n_samples, dim_features) or something similar. The way it is written now makes me think of some system where each sample can be some weighted combination of each symbol.

If someone could clear up my confusion, I would be very grateful!


"
hmmlearn,hmmlearn,216300647,log prob computed incorrectly within _decode_map,open,,https://github.com/hmmlearn/hmmlearn/issues/188,"It seems like the log probabilities are computed incorrectly when using MAP decoding. In particular, the log probabilities that are returned are positive, which should be impossible.

The problem seems trivial to fix. `posteriors` are not in the log domain, so all that needs to be changed is to modify `logprob = np.max(posteriors, axis=1).sum()` to read `logprob = np.log(np.max(posteriors, axis=1)).sum()`

```Python
def _decode_map(self, X):
        _, posteriors = self.score_samples(X)
        logprob = np.max(posteriors, axis=1).sum() # posteriors are not in log domain
        state_sequence = np.argmax(posteriors, axis=1)
        return logprob, state_sequence
```"
hmmlearn,hmmlearn,214735840,ValueError: startprob_ must sum to 1.0 (got nan),open,,https://github.com/hmmlearn/hmmlearn/issues/186,"    from hmmlearn.hmm import GMMHMM
    import numpy as np
    scores = np.loadtxt(""scores.txt"")
    model = GMMHMM(n_components=3).fit(scores)
    hidden_states = model.predict(scores)

After many deprecation warnings, it produces the following error: 

> File ""build/bdist.linux-x86_64/egg/hmmlearn/base.py"", line 336, in predict
>  File ""build/bdist.linux-x86_64/egg/hmmlearn/base.py"", line 296, in decode
 > File ""build/bdist.linux-x86_64/egg/hmmlearn/hmm.py"", line 713, in _check
 > File ""build/bdist.linux-x86_64/egg/hmmlearn/base.py"", line 519, in _check
> ValueError: startprob_ must sum to 1.0 (got nan)

My dependencies are all up to date. 

[scores.txt](https://github.com/hmmlearn/hmmlearn/files/848113/scores.txt)
"
hmmlearn,hmmlearn,211797437,Dimension of means_weight in GaussianHMM for multi-variate Gaussian,open,,https://github.com/hmmlearn/hmmlearn/issues/183,"Hi, I was trying to set the means_prior and means_weight parameters in the GaussianHMM and got operands not broadcasting errors. Looking into the code got me wondering if this is the right approach for multivariate Gaussian. 

The model has 3 hidden states and 4 observed variables. Thus my means_prior is an array of size (3, 4); and to my understanding, the precision matrix will be size (4,4) for each state, so an array of size (3,4,4).
But the operation to update mean is an element-wise product instead of dot product, and it leads to not broadcasting errors:
(I'm referring to Section 7 of this tutorial for multivariate Gaussian priors: https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf)

```
    def _do_mstep(self, stats):
        super(GaussianHMM, self)._do_mstep(stats)

        means_prior = self.means_prior
        means_weight = self.means_weight

        denom = stats['post'][:, np.newaxis]
        if 'm' in self.params:
            self.means_ = ((means_weight * means_prior + stats['obs'])
                           / (means_weight + denom))
```

Could you please let me know if I'm not understanding the prior parameters right? Thanks!"
hmmlearn,hmmlearn,207075878,Looking for maintainers,open,,https://github.com/hmmlearn/hmmlearn/issues/176,"The project would benefit from more people being involved. If you're interested in HMMs or just willing to help, please let me know!"
hmmlearn,hmmlearn,205972633,Requested features,open,enhancement,https://github.com/hmmlearn/hmmlearn/issues/173,"1. Some of my sequence is ~labeled so I need a way to fit those with certainty\probability and treat the other as hidden
2. I need super states where the state is frozen for K steps before transition (a way to low pass transitions) "
hmmlearn,hmmlearn,193036547,Baum-Welch for multinomial HMM converges to local optima,open,enhancement,https://github.com/hmmlearn/hmmlearn/issues/162,"In generating sequences from a multinomial (really, Bernoulli) HMM and then learning the parameters back with hmmlearn, the parameters are learned roughly correctly some of the time (about half) and very incorrectly the rest of the time. This holds true regardless of the number of sample sequences (numSeqs in the code below). For reference, Matlab's HMM fitting function correctly fits the parameters for the same model every time it is run. My best guess is that the Baum-Welch algorithm underlying the parameter fitting in hmmlearn is converging to local optima. It would be very helpful if there were safeguards to break out of local optima so that parameter fitting could be counted upon to work every time.


```
import numpy as np

from hmmlearn.hmm import MultinomialHMM

pT = .1
pL0 = .3
pG = .25
pS = .05

model = MultinomialHMM(n_components=2)
model.startprob_  = np.array([1.0-pL0, pL0])
model.transmat_ = np.array([[1.0-pT,pT],[0,1]])
model.n_features = 2
model.emissionprob_ = np.array([[1.0-pG, pG], [pS, 1.0 - pS]])


numSeqs = 100

for n in range(10):
  print '\niteration ', n

  seqs = []
  seqLengths = []

  for i in range(numSeqs):
#seqLength = np.random.randint(10,50)
    seqLength = 20
    seqLengths.append(seqLength)
    seq = model.sample(seqLength)[0]
    if len(seqs) > 0:
      seqs = np.append(seqs, seq, axis=0)
    else: seqs = seq

  seqs = np.array(seqs)
  seqLengths = np.array(seqLengths)

  model2 = MultinomialHMM(n_components=2)
  model2.fit(seqs, seqLengths)
  print 'transition: '
  print model2.transmat_
  print 'emission: '
  print model2.emissionprob_

```"
hmmlearn,hmmlearn,185176020,Multivariate/ multimodal features in GMMHMM useable?,open,,https://github.com/hmmlearn/hmmlearn/issues/156,"Hi,
I have a dataset consisting of distance and velocity data per timestamp. I want to use HMM to find hidden structures in the data.

My question is, since the data is multivariate (we have two different types of data: velocity & distance) is the GMMHMM implementation applicable to handle such data?

This is a sample of our first ~20 timestamps. The distances are measured in meter and the velocity in meter per second. These are the only 4 column we have 23 different sensordata.

Thumb-Index        Pinky-Index           Fingervelo (thumb)  Fingervelo (index)
0.093487084485  0.039904702223  0.0007190603322  0.0013886134282
0.093522404818  0.039904900593  0.0007609660011  0.0014664510701
0.093487851425  0.039900911873  0.0009863877167  0.0018679573468
0.093527787870  0.039905830881  0.0010779399503  0.0019553628300
0.093526270348  0.039906025977  0.0012963535427  0.0020395068512
0.093497323793  0.039892229422  0.0012817694377  0.0020726671216
0.093484272530  0.039919278473  0.0012561728751  0.0020016017980
0.093469382169  0.039910243743  0.0012419006154  0.0017438280147
0.093490066470  0.039909594234  0.0012324472698  0.0014684849035
0.093479554552  0.039892248932  0.0012194081736  0.0014708907653
0.093466888264  0.039906146181  0.0012262288664  0.0014098303113
0.093474543411  0.039901858760  0.0012153870294  0.0013777662069
0.093459734676  0.039911659470  0.0012248347770  0.0012121000059
0.093474055428  0.039902613620  0.0012389601630  0.0010723777412
0.093441780850  0.039910730507  0.0013161434346  0.0012781945538
0.093459680775  0.039903065701  0.0013158521094  0.0012395169565
0.093450931934  0.039904992448  0.0012882580190  0.0011760660893
0.093460714040  0.039894326672  0.0012552504877  0.0011199935882
0.093458104289  0.039905859763  0.0012474256789  0.0010587754396
0.093448636703  0.039904154664  0.0012324214415  0.0010157457124
0.093464570004  0.039896775435  0.0011965288131  0.0011234168209
0.093458878162  0.039903807452  0.0011902854263  0.0012821125348
0.093461117005  0.039878181231  0.0012875562876  0.0014518422822
0.093437452187  0.039897032532  0.0012806149029  0.0013874825483 
...                          

Thank you very much! :)
"
hmmlearn,hmmlearn,168557386,Add `model.get_stationary_distribution()`.,open,,https://github.com/hmmlearn/hmmlearn/pull/141,"See #96 (first part).
"
hmmlearn,hmmlearn,168553549,States relabeling,open,,https://github.com/hmmlearn/hmmlearn/issues/140,"A small practical addition would be a way to reorder the states of a HMM, e.g. `model.relabel_states(idxs)` where `idxs` is some permutation of `range(n_states)`.  This should reorder the entries in `transmat_`, `startprob_`, `means_`, etc. accordingly.

A typical use case would be to sort the states after a B-W fit, e.g. by lifetime, or by mean in the case of a 1D model (e.g., `model.relabel_states(np.argsort(model.means_.squeeze()))`).
"
hmmlearn,hmmlearn,168549926,It should be possible to copy a model's covariance to another model,open,enhancement,https://github.com/hmmlearn/hmmlearn/issues/139,"Basically

```
from hmmlearn.hmm import GaussianHMM
import numpy as np

data = np.random.rand(100)[:, None]
model1 = GaussianHMM(2, ""tied"").fit(data)
model2 = GaussianHMM(2, ""tied"", init_params=""stm"")
# e.g. to try to improve convergence with some other initialization of s/t/m
model2.covars_ = model1.covars_
model2.fit(data)
```

currently fails with

```
Traceback (most recent call last):
  File ""/tmp/foo.py"", line 8, in <module>
    model2.fit(data)
  File ""/usr/lib/python3.5/site-packages/hmmlearn/base.py"", line 422, in fit
    self._check()
  File ""/usr/lib/python3.5/site-packages/hmmlearn/hmm.py"", line 175, in _check
    self.n_components)
  File ""/usr/lib/python3.5/site-packages/sklearn/mixture/gmm.py"", line 731, in _validate_covars
    raise ValueError(""'tied' covars must have shape (n_dim, n_dim)"")
ValueError: 'tied' covars must have shape (n_dim, n_dim)
```

I _think_ we can currently get around it by copying `model._covars_` instead, but in practice I think the `.covars_` property setter should take care of adapting the covariance to the right shape.
"
hmmlearn,hmmlearn,167926760,MultinomialHMM - how can I set prior emission probabilities?,open,,https://github.com/hmmlearn/hmmlearn/issues/138,"The init method of MultinomialHMM can get the prior of start probabilities and transition probabilities but not the emission probabilities. This may cause the association between state index in the model and states of the underlying model to be not consistent. E.g. let's say state 0 has p_emission of [0.1, 0.9](corresponding to symbol 0 and 1, respectively), whereas state 1 has probability of emission [0.9, 0.1]. We would like to have emissions of 1's to be associated with state 0. 
"
hmmlearn,hmmlearn,167919084,Converged = True but Parameters are Nan,open,,https://github.com/hmmlearn/hmmlearn/issues/137,"```
import numpy as np
from hmmlearn.hmm import MultinomialHMM

startprob_prior = np.array([0.5, 0.5]) # guess
transmat_prior = np.array([[0.9, 0.1], [0.3, 0.7]]) # guess
#data is binary, 0\1 with bursts of 1's
x = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] # data
x = np.array(x).reshape(-1,1) # make it in the desirable format
hmm = MultinomialHMM(n_components=2, verbose=True, startprob_prior=startprob_prior, transmat_prior=transmat_prior)
hmm.fit(x)
print(hmm.monitor_.converged) # returns True
print(hmm.transmat_) # returns 2x2 matrix of NaN
```

Why doesn't it converges? clearly the 1's comes in bulks.
"
hmmlearn,hmmlearn,163363396,Online EM,open,,https://github.com/hmmlearn/hmmlearn/issues/132,"References:
- Liang & Klein [""Online EM for Unsupervised Models""](https://cs.stanford.edu/~pliang/papers/online-naacl2009.pdf),
- O. Cappe [""Online EM Algorithm for Hidden Markov Models""](https://arxiv.org/pdf/0908.2359v2.pdf).
"
hmmlearn,hmmlearn,160739618,Fitting multiple time series,open,,https://github.com/hmmlearn/hmmlearn/issues/128,"Hi,

   I have a bunch of time series obtained from approximately 100 sensors (all measuring different quantities but for the same process).  Can the GaussianHMM handle this case or is it limited to univariate time series, like in the Yahoo stock example?  

   You did say in the documentation that there is a way to input ""multiple sequences"" into the code by concatenating them into a single array, but I wasn't sure if by multiple sequences you meant multiple time series, since in the example the two 1D sequences (X1 and X2) are of different length.   Any insight would be appreciated.
"
hmmlearn,hmmlearn,159836472,Document the input format of MultinomialHMM,open,,https://github.com/hmmlearn/hmmlearn/issues/125,"See #124 for the problem.

I think adding a proper example to ""Examples"" section might be enough.
"
hmmlearn,hmmlearn,154125402,Check that means_ and covars_ have conformable shapes,open,,https://github.com/hmmlearn/hmmlearn/issues/121,"A minimal example with a cryptic error message:

``` python
h = hmm.GaussianHMM(n_components=2, init_params=""st"")
h.means_ = [[1], [2]]
h.covars_ = [[1, 2, 3], [1, 2, 3], [1, 2, 3]]  # Should be (2, 1).
h.fit(np.random.normal(size=(1024, 1)))
```

Originally reported in [this](http://stackoverflow.com/questions/37095512/hmmlearn-gmmhmm-error/37150236#37150236) StackOverflow question.
"
hmmlearn,hmmlearn,152051162,Prior on emission means,open,,https://github.com/hmmlearn/hmmlearn/issues/118,"Hello again - Your package is really helpful. Could you please provide an example or some documentation for how to specify priors on the means of emission distributions when using GaussianHMM. In my particular case, I expect a 4 state model where two of the means are known very well and the other two are not at all.

Thank so much!
"
hmmlearn,hmmlearn,150496813,API simplification,open,,https://github.com/hmmlearn/hmmlearn/issues/113,"Currently, an HMM has the following methods to run a forward-backward or a Viterbi on a sequence (or series of sequences) and extract some information from it:
- decode(X, lengths, algorithm) -> logprob, state_sequence
- predict(X, lengths) -> state_sequence
- predict_proba(X, lengths) -> posteriors
- score(X, lengths) -> logprob
- score_samples(X, lengths) -> logprob, prosteriors

I can see that this is done to match the scikit-learn API but there is quite a bit of redundancy there (plus some confusion, e.g. `decode` takes an ""algorithm"" argument but `predict` uses the ""decoder"" attribute; meanwhile `decode` returns the log-prob for whichever algorithm is in use whereas `score` seems to always return the FB score (not sure of that?)), so I'd like to propose two new methods:
- run_forward_backward(X, lengths) -> ForwardBackwardLattice object
- run_viterbi(X, lengths) -> ViterbiLattice object

with both Lattice classes having a `state_sequence` and a `score` attribute, and the ForwardBackwardLattice having an additional `posteriors` attribute.

Thoughts?
"
hmmlearn,hmmlearn,150269197,ValueError: rows of transmat_ must sum to 1.0,open,,https://github.com/hmmlearn/hmmlearn/issues/110,"Hello,

I downloaded the latest 0.2.1 from Master on April 20, 2016. I have a set of time series data with 3600 data points. I want to use GaussianHMM to find the Gaussian emission parameters of the HMM. With smaller n_components value, the program runs fine. However, if I set n_components = 15, I get the following error messages:

```
Traceback (most recent call last):
  File ""myhmm2.py"", line 44, in <module>
    hidden_states=mymodel.predict(arr)
  File ""build/bdist.linux-x86_64/egg/hmmlearn/base.py"", line 334, in predict
  File ""build/bdist.linux-x86_64/egg/hmmlearn/base.py"", line 294, in decode
  File ""build/bdist.linux-x86_64/egg/hmmlearn/hmm.py"", line 165, in _check
  File ""build/bdist.linux-x86_64/egg/hmmlearn/base.py"", line 524, in _check
ValueError: rows of transmat_ must sum to 1.0 (got [ 1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.])
```

I modified the sample code (Gaussian HMM of stock data) and had the following code:

```
from hmmlearn.hmm import GaussianHMM

mymodel=GaussianHMM(n_components=5, covariance_type=""diag"", n_iter=10000)
mymodel.fit(arr)

hidden_states=mymodel.predict(arr)
print (""hidden states"")
print hidden_states

print(""Transition matrix"")
print(mymodel.transmat_)
print()
```

Can someone please comment? Thank you.

David
"
hmmlearn,hmmlearn,147062746,negative transmat problem,open,,https://github.com/hmmlearn/hmmlearn/issues/108,"Hi, I am using `hmmlearn` to train a left-right hmm model with dozens of wav files.

```
self.pi_prior = np.array([1, 0.0, 0.0, 0.0, 0.0])
self.A_prior = np.array([[0.6, 0.4, 0.0, 0.0, 0.0],
                         [0.0, 0.5, 0.5, 0.0, 0.0],
                         [0.0, 0.0, 0.4, 0.6, 0.0],
                         [0.0, 0.0, 0.0, 0.5, 0.5],
                         [0.0, 0.0, 0.0, 0.0, 1.0]])

h = GMMHMM(n_components=5, n_mix=7, covariance_type = 'diag',
            startprob_prior=self.pi_prior, transmat_prior=self.A_prior, 
            n_iter=10, verbose=False, init_params='wmc', params='tmcw')

h.startprob_ = self.pi_prior
h.transmat = self.A_prior
```

However, the result after training is

```
 [[ 11.62414007 -10.62414007   0.          0.           0.        ]
 [ -0.           0.1013012    0.8986988   -0.          -0.        ]
 [  0.           0.           0.36092202   0.63907798   0.        ]
 [  0.           0.           0.          -0.38736272   1.38736272]
 [  0.           0.           0.           0.           1.        ]]
```

1) I don't get it. why there're negative elements in the `transmat` ?
2) Also, if I set the `init_params` to empty `' '`, it will raise the `ValueError` in line 607 `hmm.py`.
     I suppose this is because the `sklearn.mixture.GMM` module, right?
"
hmmlearn,hmmlearn,140747269,comparison vs pomegranate,open,,https://github.com/hmmlearn/hmmlearn/issues/104,"Hello again!

I wrote up this simple comparison vs pomegranate. I wanted to know if you thought it was fair to hmmlearn and if there are any other features you think I should include.

https://github.com/jmschrei/pomegranate/blob/master/benchmarks/pomegranate_vs_hmmlearn.ipynb

Thanks!
"
hmmlearn,hmmlearn,140524915,fit function,open,,https://github.com/hmmlearn/hmmlearn/issues/102,"Hello!

I am running benchmarks between hmmlearn and pomegranate and am a bit confused by the hmmlearn fit function. I want to randomly create samples centered around the mean with Gaussian noise, and so I have the following function:

``` Python
def initialize_components(n_components, n_dims, n_seqs):
    """"""
    Initialize a transition matrix for a model with a fixed number of components,
    for Gaussian emissions with a certain number of dimensions, and a data set
    with a certain number of sequences.
    """"""

    transmat = numpy.abs(numpy.random.randn(n_components, n_components))
    transmat = (transmat.T / transmat.sum( axis=1 )).T

    start_probs = numpy.abs( numpy.random.randn(n_components) )
    start_probs /= start_probs.sum()

    means = numpy.random.randn(n_components, n_dims)
    covars = numpy.ones((n_components, n_dims))

    seqs = numpy.zeros((n_seqs, n_components, n_dims))
    for i in range(n_seqs):
        seqs[i] = means + numpy.random.randn(n_components, n_dims)

    return transmat, start_probs, means, covars, seqs
```

However, hmmlearn doesn't take in 3d matrices for training, so I reshape it in the following manner:

``` Python
model.fit(seqs.reshape(n_seqs*n_components, n_dims), lengths=[n_components]*n_seqs)
```

When I do this, however, I'm getting much better fitting using pomegranate than hmmlearn when I score the first sample across models of increasing size.

```
          hmmlearn                         pomegranate
     before          after              before        after         
-172.590755241 -150.674249834      -172.590755241 -140.328019465
-330.942321074 -305.405266989      -330.942321074 -292.6867446
-501.606355128 -461.649889541      -501.606355128 -438.376776408
-659.670598829 -624.678536179      -659.670598829 -586.925381218
-840.081365548 -819.949309789      -840.081365549 -767.317241328
```

My understanding is either that I'm using the hmmlearn `fit` method incorrectly, or that you're putting priors on your distributions to regularize training. Or both! Let me know if you have any insight. Both run Baum-Welch for one iteration only. 
"
hmmlearn,hmmlearn,138762170,Sparse Transitions,open,,https://github.com/hmmlearn/hmmlearn/issues/100,"It would be neat if this library was able to optimize sparse transitions (e.g. if there are O(# states) transitions) - currently Viterbi takes O((#states)^2) time regardless of whether a number of transitions are invalid. 

I'm not too familiar with Cython, so I'm not sure if it's possible to simply swap out the transition matrix with a sparse representation like CSR and be able to loop through only the non-zero entries in a way that also works for non-sparse matrices (i.e. being able to use the same code for sparse and non-sparse matrices). If this is possible, I can take a shot at implementing it. 
"
hmmlearn,hmmlearn,137256315,More algorithms for state sequence decoding,open,enhancement,https://github.com/hmmlearn/hmmlearn/issues/98,"See J. Lember & A. Koloydenko [""Bridging Viterbi and Posterior Decoding: A Generalized
Risk Approach to Hidden Path Inference Based on Hidden Markov Models""](http://www.jmlr.org/papers/volume15/lember14a/lember14a.pdf).
"
hmmlearn,hmmlearn,136193986,Stationary distribution of hidden states and emissions,open,,https://github.com/hmmlearn/hmmlearn/issues/96,"An API for retrieving the stationary distribution of hidden states and emissions could be useful.  Something like:

```
hmm.get_stationary_distribution() -> (n_states,) array
hmm.get_emission_pdf(x) -> (n_states, len(x)) array
```

Thoughts?
"
hmmlearn,hmmlearn,119348979,simple multinomial example,open,,https://github.com/hmmlearn/hmmlearn/issues/70,"Hi there!

Using the latest master of hmmlearn, I tried running a simple MultinomialHMM example (code below) that results in the following error:

> >  File ""build/bdist.macosx-10.5-x86_64/egg/hmmlearn/base.py"", line 307, in decode
> > ValueError: could not broadcast input array from shape (6) into shape (1)

Could you please tell me what i am doing wrong?  My expectation is that applying Viterbi should give me the most probable hidden sequence.  However passing a list of observation doesn't work unlike passing a single value which does.

Thanks!

Vlad

``` python
from __future__ import division
import numpy as np
from hmmlearn import hmm

states = [""Rainy"", ""Sunny""]
n_states = len(states)

observations = [""walk"", ""shop"", ""clean""]
n_observations = len(observations)

start_probability = np.array([0.6, 0.4])

transition_probability = np.array([
  [0.7, 0.3],
  [0.4, 0.6]
])

emission_probability = np.array([
  [0.1, 0.4, 0.5],
  [0.6, 0.3, 0.1]
])

model = hmm.MultinomialHMM(n_components=n_states)
model.startprob=start_probability
model.transmat=transition_probability
model.emissionprob=emission_probability

# predict a sequence of hidden states based on visible states
bob_says = [0, 2, 1, 1, 2, 0]
model = model.fit(bob_says)
logprob, alice_hears = model.decode(bob_says, algorithm=""viterbi"")
print ""Bob says:"", "", "".join(map(lambda x: observations[x], bob_says))
print ""Alice hears:"", "", "".join(map(lambda x: states[x], alice_hears))
```
"
hmmlearn,hmmlearn,83497650,Add a method to check if parameters are identifiable,open,,https://github.com/hmmlearn/hmmlearn/pull/31,"Sometimes EM seems to converge but the parameters are unidentifiable. i.e, two parameters are supposed be distinct but have the same distribution. We need to specify some identification condition(e.g, the distribution must be different) for validation.

To know more about this issue please go to the following links:
http://www.researchgate.net/post/Does_the_EM_algorithm_always_converge
http://en.wikipedia.org/wiki/Identifiability
"
hmmlearn,hmmlearn,70369121,Implement FDR estimation and control,open,enhancement,https://github.com/hmmlearn/hmmlearn/issues/25,"For some applications HMM states (or their combinations) represent distinct hypotheses (in the statistical sense). In such a setting we might be interested in:
- estimating FDR for the predictions produced by the model,
- constructing a vector of predictions satisfying an upper bound on the FDR.

See [this work](http://www-stat.wharton.upenn.edu/~tcai/paper/FDR-HMM.pdf) by W. Sun and T. Cai for details.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMM,guyz,216559481,How to estimate covariance in logs?,open,,https://github.com/guyz/HMM/issues/2,"Hi,

while calculating covariance matrix in logs, i have modified the **_reestimateMixtures()** method in the file **_ContinuousHMM.py** as follows.

`              numer = numpy.matrix(numpy.zeros( (self.d,self.d), dtype=self.precision))
                denom = numpy.matrix(numpy.zeros( (self.d,self.d), dtype=self.precision))
                for t in xrange(len(observations)):
> 

    vector_as_mat = numpy.matrix( (observations[t]-self.means[j][m]), dtype=self.precision )
     numer += (self._eta(t,len(observations)-1)*numpy.exp(gamma_mix[t][j][m])*numpy.dot( vector_as_mat.T, vector_as_mat))
     denom += (self._eta(t,len(observations)-1)*numpy.exp(gamma_mix[t][j][m]))

 covars_new[j][m] = numer/denom
 covars_new[j][m] = covars_new[j][m] + cov_prior[j][m]`


Problem is while training, some times the numer and denom in the above method are becoming zero's. and as a result, my covariance matrix contains all the Nan values. 

Can anyone please help me, how to avoid covariance matrix not to contain nan values.  
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
tensorflow_hmm,dwiel,365253599,Is this something worth trying for speech ?,open,,https://github.com/dwiel/tensorflow_hmm/issues/12,"As a research work, I am trying to build a DNN-HMM model for low resource speech. Do you think I can use this for the hmm implementation for training and decoding since there is no library in tensorflow supporting HMM?

I am really thinking of scaling this hybrid model and keeping it neater than a traditional kaldi pipeline though accuracy is something I am not worried about currently.

Please lemme know."
tensorflow_hmm,dwiel,341886241,Update tensorflow version,open,,https://github.com/dwiel/tensorflow_hmm/issues/11,"Thanks for sharing 👍 , however, quite a lot of tf function calls here are deprecated."
tensorflow_hmm,dwiel,147801073,readthedocs doc generation,open,,https://github.com/dwiel/tensorflow_hmm/issues/3,"set up readthedocs documentation generatation
"
tensorflow_hmm,dwiel,147801030,allow variable length sequences with new tensorflow scan,open,,https://github.com/dwiel/tensorflow_hmm/issues/2,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Easy_HMM,tostq,359736206,Python3 支持,open,,https://github.com/tostq/Easy_HMM/pull/2,"1. Python3支持, 主要更新 print, has_key函数调用
1. Matplotlib在2.0移除了finance模块, 更新这部分数据获取方式, 数据来源[mpl_finance](https://github.com/matplotlib/mpl_finance/tree/master/examples/data), 保存在data文件夹下
1. 所有三个例子Python3.6.5跑通, 分词例子运行时间相对较长, 这个和语料有关系, 各位同学注意. "
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
StochHMM,KorfLab,321586794,where to find GC_SKEW.fa,open,,https://github.com/KorfLab/StochHMM/issues/16,"Hi, I am very interested in this project and want to use GC_SKEW example  in my study. I know that it's enormous work to estimate HMM parameters, so I want to use your GC_SKEW.hmm directly. But where to find GC_SKEW.fa. I have read your related paper and but still don't know how to get raw data. Can you give me some clue?  "
StochHMM,KorfLab,310508331,Integrating into an R package,open,,https://github.com/KorfLab/StochHMM/issues/15,"Hello,
I really appreciate it if you give me a precise instruction on how to integrate StochHMM into an R package. I have binomial data (y, n) of size one million and sometimes n and y are 0, meaning they are missing but I want to impute them using HMM. 
Thank you in advance,"
StochHMM,KorfLab,261675606,Uploaded Requested seqTools.h and seqTools.cpp,open,,https://github.com/KorfLab/StochHMM/pull/13,
StochHMM,KorfLab,50647704,Mistakes in minimal example,open,,https://github.com/KorfLab/StochHMM/issues/7,"I think there are mistakes in https://github.com/KorfLab/StochHMM/wiki/Developing-your-own-application
- hmm.import(file,NULL); should be hmm.import(model_file,NULL);
- The trellis and traceback constructor expect model pointers and not references
- print_gff expects at least a string
- in general everything belongs to the StochHMM namespace -> add using or namespace 
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
matlab-hmm,qiuqiangkong,216273026,隐马尔可夫模型,open,,https://github.com/qiuqiangkong/matlab-hmm/issues/2,"您好，是这样的，我看到了您的工具箱，冒昧的打扰您一下，因为我最近在研究HMM
我遇到了下面问题
1.连续型隐马尔可夫（CHMM）中 我的观测序列是4维的，这个到底是多观测序列，还是一个观测序列啊
2.对于多维数据，进行kmeans聚类，以及求取前向后向算法，这个具体该怎么对应，能否向您请教请教
麻烦您了！
Hello, yes, i saw your toolbox, take the liberty of bothering you because i recently studied HMM
I have encountered the following questions
1. The Continuous Hidden Markov (CHMM) in my observation sequence is 4-dimensional, this in the end is a multi-observation sequence, or an observation sequence ah
2. For multidimensional data, kmeans clustering, and seeking forward backward algorithm, the specific how the corresponding, whether to ask you to ask
"
matlab-hmm,qiuqiangkong,167235199,GMM-HMM (Cross-validation) ,open,,https://github.com/qiuqiangkong/matlab-hmm/issues/1,"Dear Mr.Qiuqiang Kong,

I would like to do Cross Validation to choose the best parameters such as # of states and  # of observation.
-My data is 9 dimensions because 3 for camera data and 6 for IMU dataset. is it possible to use more than 2 dimensions dataset?
-how can i use the test dataset in your code, did you provide the code or i should write by myself?
 thanks in advance.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMM,ldanduo,218673800,Tag mistake,open,,https://github.com/ldanduo/HMM/issues/1,"line 46 of hmm_train.py:
        outpout_str.append('S')
should be:
        outpout_str.append('E')


"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
finalseg,fxsjy,234549976,打开新词发现时，如果遇到新字会出错,open,,https://github.com/fxsjy/finalseg/issues/4,"finalseg.cut(“抽丝剥茧之后”, find_new_word=True)
由于“茧”不存在于prob_emit.py中，会报错：
ValueError: max() arg is an empty sequence
"
finalseg,fxsjy,35738307,Performance,open,,https://github.com/fxsjy/finalseg/issues/2,"Hi, 
I forked you project and rune the `time python test.py` and here is the output:

``` bash
<segmentation results>
python test.py  2.16s user 0.13s system 99% cpu 2.291 total
```

The script took **~2.2s** to return the result with you default text. I was expecting something a lot faster, is it in the range of expected time ? 

Longer text or compiling into byte code give similar times.
### Longer text

> 特色条目1899年肯塔基州州长选举于1899年11月7日举行，旨在选出第33任肯塔基州州长，在任州长共和党人威廉·O·布拉德利因州宪法限制而不能竞选连任。民主党人在路易斯维尔举行的提名大会陷入一片混乱和争议，最终选择了州参议员威廉·格贝尔作为候选人。对格贝尔的政治手段感到愤怒的一派民主党人自称“诚信选举联盟”，之后另开提名大会提名前州长约翰·Y·布朗参选。共和党提名的是州检察长威廉·S·泰勒，但在任州长布拉德利心中另有人选，所以在竞选中只给予泰勒很少的支持。普选结果，泰勒以不过1500票的优势胜出。这一结果受到选举舞弊的指控，不过令人意外的是，由格贝尔所提法案建立、而且所有成员都是他钦点铁杆民主党人的州选举委员会还是认可了泰勒的胜利。愤怒的民主党议员利用州议会中的多数席位建立委员会来调查有关选举舞弊的指控。共和党占绝对优势的东肯塔基州开始有武装人员涌入州首府，希望能阻止民主党人窃取选举成果。格贝尔于1900年1月30日受到枪击并随后进入附近酒店接受治疗，之后委员会发布报告，州议会通过宣布足够数量的选票无效来令格贝尔和贝克汉姆当选。格贝尔于1月31日宣誓就职，但2月2日就因伤重不治。副州长J·C·W·贝克汉姆继任州长职务，并与泰勒展开了旷日持久的司法大战。最终贝克汉姆胜诉，泰勒逃往印第安纳州，以免受到涉嫌谋杀格贝尔的指控。共有16人受到起诉，5人出庭受审，其中两人无罪释放。剩下3人中有两人先后数次定罪，但审判过程中违规行为随处可见，他们此后都获州长赦免。格贝尔的遇刺至今仍然没有确切结论。 
### Compiling

```
python -m py_compile ./cfdict.py
time python cfdict.pyc
```
"
finalseg,fxsjy,7243016,no setup.py,open,,https://github.com/fxsjy/finalseg/issues/1,"git add it?

if has setup.py , we can python setup.py install
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmmer,EddyRivasLab,375794078,Translated search -> develop for H3 (redux),open,,https://github.com/EddyRivasLab/hmmer/pull/149,"This pull request brings translated search tools into the develop branch, with the intent of making the translated search tools available in a future H3 release. Two tools are included in this batch of commits:
hmmsearcht: search protein profile(s) against nucleotide sequence database
nhmmscant: search nucleotide sequence(s) against a protein profile database

Both tools are available as standalone executables, implemented in corresponding .c files. The variant nhmmscant is implemented in the hmmpgmd daemon as well. The hmmsearcht option was not included in the daemon for the same reason an nhmmer daemon has not yet been implemented: the daemon's sharding strategy won't trivially extend to the scale of common nucleotide data sets. This strikes me as future work that will likely wait for a new daemon architecture in H4.

The implementation is a straightforward one, in which each DNA sequence is converted into all possible ORFs (using easel functions), and each such ORF is compared at the protein level to the protein profile(s). Consideration of codons, frameshift errors, etc is left to the future.

The tools currently accept only protein profiles, not protein sequences. This makes the tool analogous to hmmsearch and nhmmscan. Auto-detection of input format to allow for protein sequences or protein alignments is left for future work.

The standalone executables are documented in new man pages and new sections in the Userguide, and a tutorial has also been added to the Userguide.

A side note: one could possibly have hoped for an inverse pair of tools:
nhmmert : search DNA profile(s) against a protein sequence database
hmmscant : search protein sequence(s) against a database of DNA profiles
Neither is included here because of the complexity of considering codons in nucleotide profiles.

~~
Issues flagged during review of a previous pull request have been addressed. This involved removal of bit-rotted MPI code (these tools do not support MPI at present), improved error handling in the daemon, including man pages missing in the committed code, and changing the program name phmmert -> hmmscant. "
hmmer,EddyRivasLab,364212356,Compilation of v3.2.1 fails due to lib easel not linked,open,,https://github.com/EddyRivasLab/hmmer/issues/148,"Thanks for providing this great tools.

I had the following errors when trying to compile hmmer v3.2.1 with GNU Make 3.82 /
gcc version 4.8.5 20150623 (Red Hat 4.8.5-16) (GCC):

```
...
     SUBDIR miniapps
     GEN esl-afetch
     GEN esl-alimanip
/tmp/ccfuqGxq.o: In function `main':
esl-alimanip.c:(.text.startup+0x23c0): undefined reference to `esl_arr2_Destroy'
esl-alimanip.c:(.text.startup+0x23db): undefined reference to `esl_arr2_Destroy'
esl-alimanip.c:(.text.startup+0x2401): undefined reference to `esl_free'
esl-alimanip.c:(.text.startup+0x240e): undefined reference to `esl_free'
collect2: error: ld returned 1 exit status
make[2]: *** [esl-alimanip] Error 1
make[1]: *** [all] Error 2
make: *** [all] Error 2
```

I.e. gcc command cannot find lib easel. This is one of the gcc commands (working directory: `easel/miniapps`) that fails:

```gcc -std=gnu99 -O2 -msse2 -pthread  -DHAVE_CONFIG_H -L/ccb/sw/lib -L.. -I. -I.. -I. -I./.. -o esl-alimask esl-alimask.c -leasel  -lm```

I could make it compile by replacing `-leasel` with the path to the library (`../libeasel.a`). I'm not sure why it would not work before, because the original command line does include the directory of the library (`-L..`). It seems to me it's a GCC bug, but wanted to report it. FYI, after fixing minimap the same ld errors appeared in other parts of the HMMER source code, which required the same 'fix'.
"
hmmer,EddyRivasLab,331615443,Compilation fails on ppc64le,open,H4,https://github.com/EddyRivasLab/hmmer/issues/142,"HMMER3 compilation fails ungracefully on PowerPC ppc64le (little-endian) machines.

PowerPC Altivec/VMX instructions are used on both big-endian or little-endian platforms, but our code currently just refers to a ""VMX implementation"", despite the fact that our vector implementations are sensitive to byte order. To make things more confusing, the HMMER3 and HMMER4 VMX implementations are currently written for _different_  byte orders.

The (older) HMMER3 Altivec/VMX implementation assumes _*big-endian*_ architecture, e.g. Power 7; Power8 and Power9 machines can be booted into bigendian mode. The (newer) HMMER4 Altivec/VMX implementation assumes  _*little-endian*_ architecture. Easel's test for VMX support does not distinguish, and some of Easel's `esl_vmx` utilities assume _*little-endian*_; the only reason this is working for us is that H3 does not use these utilities, calling VMX instructions directly.

It appears that bigendian PowerPC (e.g. 32-bit powerpc, 64-bit ppc) are on their way out, and littleendian PowerPC (e.g. ppc64le) is likely to take over. IBM announced in ~2015-16 that they are moving toward only supporting LE on Power 8/9 Linux platforms, and they seem to be pushing their partners (Ubuntu, SUSE, Red Hat) hard to switch to LE exclusively. Ubuntu and SUSE now appear to support only LE; Red Hat, Debian, Fedora, openSUSE appear to support LE and BE; Fedora has announced that they are transitioning toward LE and deemphasizing BE.

We will continue supporting H3 on BE (32bit powerpc and ppc64). We have no plans to introduce a ppc64le port of H3, because we'd rather focus on HMMER4 development.

Conversely, HMMER4 will support ppc64le when it is released, and we have no current plans to introduce a ppc64 or powerpc (32bit) port of H4 -- but we could, if there were demand for it.

The HMMER3 configure script does not currently distinguish between BE and LE when it activates a VMX implementation, and as a result, compilation fails on an LE machine, hence I'm opening this issue as a bug. "
hmmer,EddyRivasLab,203425686,H4 Daemon uses hack to detect type of data file,open,H4,https://github.com/EddyRivasLab/hmmer/issues/122,"The current H4 daemon prototype uses a hack based on the first line of a data file to determine whether it is loading an HMM database or a dsqdata database.  This should be converted to use the routines in Easel that do the full, approved, format detection.

Priority: medium, needs to be done before daemon considered complete, but not blocking other development."
hmmer,EddyRivasLab,203424481,HMMER can't read data files that were written on a machine of opposite endianness,open,H4,https://github.com/EddyRivasLab/hmmer/issues/121,"HMMER's data files are sensitive to the endianness of the machine they were written on, and cannot be read on a machine of the other endianness.  Currently, the magic numbers at the beginning of the data files are sensitive to endianness, so HMMER will signal an error when directed to read a data file of the opposite endianness, but it would be better if we could just correct for endianness."
hmmer,EddyRivasLab,203423852,HMMER4 doesn't handle POWER architecture when booted in big-endian mode,open,H4,https://github.com/EddyRivasLab/hmmer/issues/120,"HMMER currently only works on POWER when booted in little-endian mode.  

We should have big-endian versions of the filters and similar ""detect at run-time"" switching to the way we handle AVX and AVX-512.

Priority: low, just needs to get done before release"
hmmer,EddyRivasLab,203423343,H4 needs Altivec versions of forward and backward filters,open,H4,https://github.com/EddyRivasLab/hmmer/issues/119,"The checkpointed versions of the forward and backward filters are new in h4, and we don't have AltiVec implementations of them yet.  

Currently classified as ""needs to get done before H4 release, but low priority""

"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
pyhmm,ananthpn,223532463,I think there is an error in the Viterbi algorithm,open,,https://github.com/ananthpn/pyhmm/issues/2,"in line 99 of myhmm.py, I think it should be self.A[y][y0] instead of self.A[y0][y]"
pyhmm,ananthpn,164394479,forward_backward for multiple observation sequence,open,,https://github.com/ananthpn/pyhmm/issues/1,"The note in the Readme says it can be used for multiple input sequences. I don't see any code that does that. Different observation sequences can't be joined together for training the model as discussed in Section 5B in L.Rabiner's paper.  
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
NPBayesHMM,michaelchughes,248700119,DataGuide.pdf missing,open,,https://github.com/michaelchughes/NPBayesHMM/issues/3,"Been trying to understand how to use it, but I can't find the DataGuide.pdf in 'HOME/doc/' directory mentioned in QuickStartGuide.pdf. Is it removed? "
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,adrianulbona,220231261,Remove lombok as compile dependency,open,,https://github.com/adrianulbona/hmm/issues/2,
hmm,adrianulbona,198450416,implement hmm model training algorithm,open,,https://github.com/adrianulbona/hmm/issues/1,https://en.wikipedia.org/wiki/Baum–Welch_algorithm
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Speech_Recognition,drbinliang,300012559,问题,open,,https://github.com/drbinliang/Speech_Recognition/issues/3, 在github上看到了你的代码，并且下载了，但是对于文件应该怎么运行还有一些问题，可以加微信询问吗？18379470820。是认真的。
Speech_Recognition,drbinliang,265260483,"ValueError: Expected 2D array, got 1D array instead:",open,,https://github.com/drbinliang/Speech_Recognition/issues/2,"Step.1 Training data loading... done!

Step.2 Training model...
Traceback (most recent call last):
  File ""main.py"", line 123, in <module>
    main()
  File ""main.py"", line 96, in main
    speechRecognizerList = training(trainSpeechList)
  File ""main.py"", line 46, in training
    speechRecognizer.getHmmModel()
  File ""/home/hjian/google/hawking/Speech_Recognition/src/utils.py"", line 91, in getHmmModel
    model.fit(self.trainData)   # get optimal parameters
  File ""build/bdist.linux-x86_64/egg/hmmlearn/base.py"", line 423, in fit
  File ""/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py"", line 410, in check_array
    ""if it contains a single sample."".format(array))
ValueError: Expected 2D array, got 1D array instead:
array=[ array([[  9.40209590e+00,   2.31332069e+00,  -4.39288388e-01,
          4.48900498e-01,  -4.38783780e-01,   7.17864611e-01,
          1.34303265e-01,  -8.46420413e-01,  -9.96619422e-02,
         -2.11934015e-01,   2.94443307e-01,  -2.60837321e-01,
         -5.68513326e-02],
       [  9.42433231e+00,   1.97682124e+00,  -3.76114448e-01,
          5.84995514e-01,  -7.75079962e-01,   6.33918830e-01,
          2.55590945e-01,  -8.52373971e-01,   8.62750128e-02,
          1.64632304e-04,   1.61292471e-01,  -1.98488390e-01,
"
Speech_Recognition,drbinliang,226252320,ValueError: setting an array element with a sequence.,open,,https://github.com/drbinliang/Speech_Recognition/issues/1,"![image](https://cloud.githubusercontent.com/assets/10117199/25701782/6a27638c-3100-11e7-8319-2360a328cb62.png)
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMMs,lopatovsky,282149162,[Question] Support for emission probabilites as a PDF?,open,,https://github.com/lopatovsky/HMMs/issues/6,"Firstly, sorry if this is not the correct place to submit questions such as this one. 

I have been reviewing all the Python package for implementing discrete-time HMMs and came across your work. First of all, thanks. Very neat library (`plot_hmm` is great!). 

I am wondering if it's possible to have emission probabilities defined as a PDF (say, multinomial), instead of the current matrix form. In my use case, emissions take continuous values. Is this possible or is it out of the library's scope? Thanks! "
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,mikeizbicki,58460619,hmm-0.2.1.1 does not compile,open,,https://github.com/mikeizbicki/hmm/issues/12,"See http://hydra.cryp.to/build/574131/log/raw for a complete build log.
"
hmm,mikeizbicki,55601692,several commits for cleanup,open,,https://github.com/mikeizbicki/hmm/pull/11,
hmm,mikeizbicki,27139308,Data.Number.LogFloat.(/): argument out of range,open,,https://github.com/mikeizbicki/hmm/issues/1,"Given the states:
[""AT"",""NP"",""NN"",""JJ"",""VBD"",""NR"",""IN""]
And the events:
[""The"",""Fulton"",""County"",""Grand"",""Jury"",""said"",""Friday"",""an"",""investigation"",""of""]
Training a simpleHMM with baumWelch on the same list of events,result in 
Data.Number.LogFloat.(/): argument out of range

I think that problem was reported first at http://izbicki.me/blog/using-hmms-in-haskell-for-bioinformatics#comment-798 , is there a more recent library to use for hidden markov models? I don't mind having a go at trying to fix this, but some guidance would be appreciated!

Thanks!
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMM-MAR,OHBA-analysis,197133246,Stochastic state knock out,open,,https://github.com/OHBA-analysis/HMM-MAR/issues/7,"Attached is input for `hmmmar`
 
```
hmmmar(V,t,options)
```

The input corresponds to 5 channels, 10 subjects, 400 samples per subject, 3 actual states. When run, for each of the initialisations (single subject) it switches off 5 states i.e. it correctly infers the 3 states. However, in the final run, all 8 states are still present in the output. If stochastic is disabled (e.g. `options.BIGNbatch=10`) then it also correctly gets the 3 states. 

Expected behaviour is that the stochastic run also knocks out the states

[debug.mat.zip](https://github.com/OHBA-analysis/HMM-MAR/files/668445/debug.mat.zip)"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,AustinRochford,47864217,Modified the viterbi to make underflow resistant,open,,https://github.com/AustinRochford/hmm/pull/1,"Replaced probability multiplication by log-summation. This bumps the underflow limit from 1E-308 (~1000 parsed characters) to 1E-(1.7E308)
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,dtkirsch,87109799,How to use?,open,,https://github.com/dtkirsch/hmm/issues/2,"Any suggestions how to use?  I'm trying to do something like this
good_examples = [""happy birthday"", ""to you"", ""there boy""]
bad_examples = [""asdfasdfa"", ""asdfasdf""]

a = Hmm.new
a.train = (what here?)
a.classify ""happy tuesday kevin"" -> ""good"" (how to get results out?)

Thanks :)
"
hmm,dtkirsch,10340419,"Instead of recomputing log()s of @a, @b, and @pi as needed, this does it...",open,,https://github.com/dtkirsch/hmm/pull/1,"... once, lazily.  @log_a, @log_b, @log_pi are calculated the first time they're needed. These are also invalidated (then recalculated, as needed) by the various training methods.

I'm using HMM to estimate the key of a musical phrase, in https://github.com/jimlindstrom/multi_markov_composer/blob/key-detection/lib/note_queue.rb. When repeatedly calling decode() and likelihood() for different sequences, but without retraining, I noticed that log() was being called a LOT. Precalculating the logs makes my usage of HMM about 2x faster.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
seqHMM,helske,336353855,multi-channel HMM plot,open,,https://github.com/helske/seqHMM/issues/31,"I’ve constructed a multi-channel (9 substances) HMM. Each subject (n=283) was interviewed on the frequency of substance use at weeks 0, 4, 8, 12, 24, 36, and 48. Possible responses for each substance were: Never, Oncer or Twice, Monthly, Weekly, Daily or Almost Daily (coded 0, 2, 3, 4, 6). So each substance has 5 possible responses. 

Essentially with your program I’ve been able to find 5 different states of combinations of drug use: 1) no to minimal substance use, 2) use of poppers and viagra, 3) use of marijuana and poppers, 4) use of multiple drugs including meth, 5) use of multiple drugs without meth. 

Where my difficulty has come is plotting the default plot for the HMM. When I use the command plot(hmm_mvad), I get the following error: 
Error in seqHMM::colorpalette[[k]] : attempt to select less than one element in get1index <real>

This also happens if I try to use the mc_to_sc command. 

I’ve been able to get the plot command to work with the tutorial code and with my data if I build one-channel models. I am assuming I have an issue with my multi-channel models. 

Thanks!

```{r}
#Convert from long to wide for each drug, then create into sequence data
ecstasy <- dcast(prep, PTID ~ VISITNUM, value.var=c(""DUAU5_1""))
ecstasy[277,2] <- 6
ecstasy_seq <- seqdef(ecstasy [,2:8],start=0,labels=c(""Never"",""Once or twice"",""Monthly"",""Weekly"",""Daily or Almost daily""))
ghb <- dcast(prep, PTID ~ VISITNUM, value.var=c(""DUAU5_2""))
ghb_seq <- seqdef(ghb[,2:8],start=0,labels=c(""Never"",""Once or twice"",""Monthly"",""Weekly"",""Daily or Almost daily""))
mj <- dcast(prep, PTID ~ VISITNUM, value.var=c(""DUAU5_4""))
mj_seq <- seqdef(mj[,2:8],start=0,labels=c(""Never"",""Once or twice"",""Monthly"",""Weekly"",""Daily or Almost daily""))
meth <- dcast(prep, PTID ~ VISITNUM, value.var=c(""DUAU5_5""))
meth_seq <- seqdef(meth[,2:8],start=0,labels=c(""Never"",""Once or twice"",""Monthly"",""Weekly"",""Daily or Almost daily""))
popper <- dcast(prep, PTID ~ VISITNUM, value.var=c(""DUAU5_8""))
popper_seq <- seqdef(popper[,2:8],start=0,labels=c(""Never"",""Once or twice"",""Monthly"",""Weekly"",""Daily or Almost daily""))
cocaine <- dcast(prep, PTID ~ VISITNUM, value.var=c(""DUAU5_9""))
cocaine_seq <- seqdef(cocaine[,2:8],start=0,labels=c(""Never"",""Once or twice"",""Monthly"",""Weekly"",""Daily or Almost daily""))
opioids <- dcast(prep, PTID ~ VISITNUM, value.var=c(""DUAU5_12""))
opioids_seq <- seqdef(opioids[,2:8],start=0,labels=c(""Never"",""Once or twice"",""Monthly"",""Weekly"",""Daily or Almost daily""))
benzo <- dcast(prep, PTID ~ VISITNUM, value.var=c(""DUAU5_14""))
benzo_seq <- seqdef(benzo[,2:8],start=0,labels=c(""Never"",""Once or twice"",""Monthly"",""Weekly"",""Daily or Almost daily""))
ed <- dcast(prep, PTID ~ VISITNUM, value.var=c(""DUAU5_15""))
ed_seq <- seqdef(ed[,2:8],start=0,labels=c(""Never"",""Once or twice"",""Monthly"",""Weekly"",""Daily or Almost daily""))

#Add color palette
attr(ecstasy_seq, ""cpal"") <- colorpalette[[10]]
attr(ghb_seq, ""cpal"") <- colorpalette[[10]]
attr(mj_seq, ""cpal"") <- colorpalette[[10]]
attr(meth_seq, ""cpal"") <- colorpalette[[10]]
attr(popper_seq, ""cpal"") <- colorpalette[[10]]
attr(cocaine_seq, ""cpal"") <- colorpalette[[10]]
attr(opioids_seq, ""cpal"") <- colorpalette[[10]]
attr(benzo_seq, ""cpal"") <- colorpalette[[10]]
attr(ed_seq, ""cpal"") <- colorpalette[[10]]

#Plot by drug
ssplot(list(""Ecstasy"" = ecstasy_seq, ""GHB"" = ghb_seq, ""Meth"" = meth_seq, ""Marijuana"" = mj_seq, ""Popper"" = popper_seq, 
            ""Cocaine"" = cocaine_seq, ""Opioids"" = opioids_seq, ""Benzo"" = benzo_seq, ""Viagra"" = ed_seq), ylab.pos = c(1, 2))
ssplot(list(ecstasy_seq[1:10,], ghb_seq[1:10,], meth_seq[1:10,],mj_seq[1:10,],popper_seq[1:10,],cocaine_seq[1:10,],
            opioids_seq[1:10,],benzo_seq[1:10,],ed_seq[1:10,]),
       sortv = ""from.start"", sort.channel = 1, type = ""I"",
       ylab = c(""Ecstasy"", ""GHB"", ""Meth"", ""Marijuana"",""Popper"",""Cocaine"",""Opioids"",""Benzo"",""Viagra""),
       xlab = ""Time points"", title = ""Ten first sequences"",
       title.n = FALSE, ylab.pos = c(1, 2),cex.legend=0.5, with.legend=""right.combined"")
```
```{r}
#Build model and estimate probabilities
hmm <- build_hmm(observations = mc_obs, 
          n_states = 5, 
          channel_names=c(""Ecstasy"", ""GHB"", ""Meth"", ""Marijuana"", ""Popper"",""Cocaine"",""Opioids"",""Benzo"",""Viagra""))
hmm_fit <- fit_model(hmm,control_em = list(restart = list(times = 50)))
hmm_mvad <- hmm_fit$model
```
#Plots
```{r}
##Pie chart of initial probabilities
pie <- as.data.frame(round(hmm_mvad$initial_probs,3))
pie <- rownames_to_column(pie, var = ""Stage"")
names(pie)[names(pie) == ""round(hmm_mvad$initial_probs, 3)""] <- ""Probability""
hmm.pie <- ggplot(pie, aes(x="""", y=Probability, fill=Stage))+
  geom_bar(width = 1, stat = ""identity"")+
  coord_polar(""y"", start=0)+geom_col(position = 'fill')+
  geom_text(aes(y = Probability/4 + c(0, cumsum(Probability)[-length(Probability)]), 
                label = percent(Probability)), size=5, position = position_fill(vjust = 0.5))
hmm.pie
##Bar chart of substances frequency to each state
hmm.est.emission <- as.data.frame(hmm_mvad$emission_probs)
hmm.est.emission <- tibble::rownames_to_column(hmm.est.emission, var=""stage"")
hmm.est.emission <- melt(hmm.est.emission, id='stage')
hmm.est.emission <- separate(hmm.est.emission,col=""variable"",into=c(""substance"",""frequency""),remove=F)
hmm_p <- ggplot(hmm.est.emission,aes(x = substance, y = value, fill = frequency))
hmm_p <- hmm_p + geom_bar(stat = ""identity"", position = ""stack"")+
  scale_color_manual(labels = c(""Never"",""Once or twice"",""Monthly"",""Weekly"",""Daily or Almost daily""))+
  facet_wrap(~ stage)+
  theme(axis.text.x = element_text(angle = 90, vjust = 0,size = 12, hjust = 0))
hmm_p
#Distribution plots for observed and hidden 
#ssplot(hmm_mvad, plots = ""both"", type = ""d"", sortv = ""mds.hidden"", cex.legend=0.5, with.legend=""right.combined"",
#       title = ""Observed and hidden state sequences"", xlab = ""Time points"", ylab.pos = c(1, 2))
ssplot(hmm_mvad, plots = ""hidden.paths"", type = ""d"", sortv = ""mds.hidden"", cex.legend=1, with.legend=""right.combined"",
       title = ""Hidden state sequences"", xlab = ""Time points"", with.missing=T)
ssplot(hmm_mvad, plots = ""obs"", type = ""d"", sortv = ""mds.hidden"", cex.legend=1, with.legend=""right.combined"",
       title = ""Observed state sequences"", xlab = ""Time points (week)"", ylab.pos = c(1, 2), 
       with.missing=TRUE, missing.color=""white"", with.missing.legend = TRUE)
#mc_to_sc(hmm_mvad, combine_missing = FALSE, all_combinations = FALSE)
plot(hmm_mvad)
```"
seqHMM,helske,302793318,"How to fix a model with warning: in fit_model, backward probabilities contain non-finite values,NLOPT_FAILURE",open,,https://github.com/helske/seqHMM/issues/29,"I have a single channel sequence. I built an HMM based on assumed 4 states, using arbitrary initial values, transition probabilities, emission probabilities. I fit the model using global_step=TRUE, local_step=TRUE.
The EM algorithm failed. The local optimization terminated. Is there a way to adjust the parameters to avoid the warning? Note that I was able to plot and print the hmm (maybe incorrect because of the warning?? ) 
Thanks for your help"
seqHMM,helske,274140794,No summary for latent class model?,open,,https://github.com/helske/seqHMM/issues/25,"There is no summary function for latent class model.

```
set.seed(123)
obs <- seqdef(rbind(
  matrix(sample(letters[1:3], 5000, TRUE, prob = c(0.1, 0.6, 0.3)), 500, 10),
  matrix(sample(letters[1:3], 2000, TRUE, prob = c(0.4, 0.4, 0.2)), 200, 10)))

# Initialize the model
set.seed(9087)
model <- build_lcm(obs, n_clusters = 2)

# Estimate model parameters
fit <- fit_model(model)
fit
```
"
seqHMM,helske,230142922,Error in gridPLT() : Figure region too small and/or viewport too large,open,,https://github.com/helske/seqHMM/issues/23,"Hi,

I have been following **4. Examples with life course data** section of your [documentation](https://cran.r-project.org/web/packages/seqHMM/vignettes/seqHMM.pdf) and I cannot plot the state distributions.

When I do:

```R
ssplot(list(""Marriage"" = marr_seq, ""Parenthood"" = child_seq,
            ""Residence"" = left_seq))
```

I get:

> Error in gridPLT() : Figure region too small and/or viewport too large

I have a big screen and I have tried to maximize the plot section, but it does not work."
seqHMM,helske,221668314,ssplot creates two windows,open,,https://github.com/helske/seqHMM/issues/21,"I am trying to create an ssplot within Rmd and knit to pdf.  It seems to be producing a blank window in addition to my plot and in the knitted pdf the blank plot window appears ruining formatting.

head(noh)
   PID NoH1 NoH2 NoH3 NoH4 NoH5 NoH6 NoH7 NoH8 NoH9 NoH10 NoH11 NoH12
1 1001    1    0    1    0    1    0    1    0    0     0     0     0
2 1004    1    0    1    1    0    1    1    0    1     0     1     0
3 1005    0    0    0    0    0    0    0    0    1     0     0     0
4 1007    0    1    1    0    0    1    1    1    1     1     1     0
5 1011    0    0    0    0    0    0    0    0    0     0     0     1
6 1056    0    0    0    0    0    0    0    0    0     0     0     0
noh_seq<-seqdef(noh[,2:13],start=1, labels = c(""Heavy"",""Not Heavy""))
attr(noh_seq, ""cpal"") <- c(""violetred2"", ""darkgoldenrod2"")
ssplot(noh_seq,
        sortv = ""from.start"", sort.channel = 1, type = ""I"",
        ylab = c(""Patients""),
        xtlab = 1:12, xlab = ""Week"", title = ""All Patients"")"
seqHMM,helske,164550520,ltext param in multichannel HMM plots?,open,,https://github.com/helske/seqHMM/issues/18,"Hi,

I am trying to plot a MC-HMM using the plot function from this package, and I want to add my custom labels to the legend (instead of the alphabet symbols).

I've tried the param ""ltext"" but I do not know how to associate each element of my custom legend with the combined state it represents.

Any idea of how this param works?

Thanks in advance!
"
seqHMM,helske,161396375,One channel full of NA's results in a seqfault,open,,https://github.com/helske/seqHMM/issues/17,"Hi,

Recently, I found this error when executing my code for training HMMs with sequence data.

``` r, engine='bash', count_lines
 *** caught segfault ***
address (nil), cause 'unknown'

Traceback:
 1: .Call(""seqHMM_EM"", PACKAGE = ""seqHMM"", transitionMatrix, emissionArray,     initialProbs, obsArray, nSymbols, itermax, tol, trace, threads)
 2: EM(model$transition_probs, emissionArray, model$initial_probs,     obsArray, model$n_symbols, em.con$maxeval, em.con$reltol,     em.con$print_level, threads)
 3: fit_model(mc_init_mod, em_step = TRUE, control_em = control_em,     threads = threads)
 4: FUN(X[[i]], ...)
 5: lapply(pieces, .fun, ...)
 6: structure(lapply(pieces, .fun, ...), dim = dim(pieces))
 7: llply(nstates, function(J) {    mc_init <- hmmHelper.randomInitialVector(J)    mc_trans <- hmmHelper.randomTransitionMatrix.hmm(J)    if (!is.data.frame(seqData)) {        mc_emissions <- llply(seqData, function(channel) {            simulate_emission_probs(n_states = J, n_symbols = length(alphabet(channel)),                 n_clusters = 1)        })    }    else {        mc_emissions <- simulate_emission_probs(n_states = J,             n_clusters = 1, n_symbols = length(alphabet(seqData)))    }    mc_init_mod <- build_hmm(observations = seqData, initial_probs = mc_init,         transition_probs = mc_trans, emission_probs = mc_emissions,         channel_names = names(seqData))    mc_fit <- fit_model(mc_init_mod, em_step = TRUE, control_em = control_em,         threads = threads)    mc_fit$model}, .progress = ifelse(verbose, ""text"", ""none""))
 8: CIIDSHelper.trainHMMs(seqData = notClassData, nstates = possibleStates,     control_em = control_em, threads = threads)
 9: FUN(X[[i]], ...)
10: lapply(pieces, .fun, ...)
11: structure(lapply(pieces, .fun, ...), dim = dim(pieces))
12: llply(unique(dataLabels), function(classLabel) {    if (!is.data.frame(seqData)) {        classData <- llply(seqData, function(channel) channel[dataLabels ==             classLabel, ])        notClassData <- llply(seqData, function(channel) channel[dataLabels !=             classLabel, ])    }    else {        classData <- seqData[dataLabels == classLabel, ]        notClassData <- seqData[dataLabels != classLabel, ]    }    if (verbose)         print(paste(""Training HMMs for class"", classLabel))    classHMMs <- CIIDSHelper.trainHMMs(seqData = classData, nstates = possibleStates,         control_em = control_em, threads = threads)    if (verbose)         print(paste(""Training HMMs for NOT class"", classLabel))    notClassHMMs <- CIIDSHelper.trainHMMs(seqData = notClassData,         nstates = possibleStates, control_em = control_em, threads = threads)    list(YES = classHMMs, NO = notClassHMMs)}, .progress = ifelse(verbose, ""text"", ""none""))
13: CIIDSHelper.trainDualHMMClassifier(seqData = trainingData, dataLabels = DATA_LABELS[names(DATA_LABELS) !=     simulation], possibleStates = NSTATES, control_em = CONTROL_EM,     threads = THREADS)
14: .fun(piece, ...)
15: (function (i) {    piece <- pieces[[i]]    if (.inform) {        res <- try(.fun(piece, ...))        if (inherits(res, ""try-error"")) {            piece <- paste(utils::capture.output(print(piece)),                 collapse = ""\n"")            stop(""with piece "", i, "": \n"", piece, call. = FALSE)        }    }    else {        res <- .fun(piece, ...)    }    progress$step()    res})(1L)
16: .Call(loop_apply_, as.integer(n), f, env)
17: loop_apply(n, do.ply)
18: llply(names(DATA_LABELS), function(simulation) {    if (!is.data.frame(seqData)) {        testData <- llply(seqData, function(channel) channel[rownames(channel) %in%             simulation, ])        trainingData <- llply(seqData, function(channel) channel[!(rownames(channel) %in%             simulation), ])    }    else {        testData <- seqData[rownames(seqData) %in% simulation,             ]        trainingData <- seqData[!(rownames(seqData) %in% simulation),             ]    }    print(paste(""###### Training Dual HMM Classifier ######""))    dualHMMClassifier <- CIIDSHelper.trainDualHMMClassifier(seqData = trainingData,         dataLabels = DATA_LABELS[names(DATA_LABELS) != simulation],         possibleStates = NSTATES, control_em = CONTROL_EM, threads = THREADS)    print(paste(""###### Testing HMM Classifier with different configurations ######""))    result <- llply(names(dualHMMClassifier), function(classLabel) {        CIIDSHelper.seqHMM.llr(seq = testData, baseHMM = dualHMMClassifier[[classLabel]]$YES,             constrainedHMM = dualHMMClassifier[[classLabel]]$NO)    })    names(result) <- names(dualHMMClassifier)    result}, .progress = ""text"")
19: eval(expr, envir, enclos)
20: eval(ei, envir)
21: withVisible(eval(ei, envir))
22: source(""src/CIIDS/5.3_bestConfigurationDHMMClasiffier.R"")
```

I suppose it is something internal in the seqHMM_EM process, may be related to memory troubles. The sequence data object I am using comprises a total of 146 sequences with 2 channels, sized 12.3 Mb

Any idea of what's going on here?

Thanks in advance!
"
seqHMM,helske,159994950,Problems with BIC function,open,,https://github.com/helske/seqHMM/issues/15,"Hi,

I am applying the BIC function over a set of objects with class ""hmm"", obtained by apply the function fit_model of this package.

When I run my code in RStudio, everything works fine and the BIC values are computed, but when I try to run the code in a R shell, I encounter this error:

> Error in log(object$K) : non-numeric argument to mathematical function

I suppose that the BIC function used is not the same depending on the R environment. What is the BIC function used in this package?

Thanks in advance!
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMMWeka,marcogillies,249808975,Doesnt work in the latest WEKA version 3.8.1 and 3.9.1,open,,https://github.com/marcogillies/HMMWeka/issues/2,"author, can you please try to fix it for us?
result on an input is ""problem evaluating classifier: null"""
HMMWeka,marcogillies,153970640,Example with .arff,open,,https://github.com/marcogillies/HMMWeka/issues/1,"Hi, 

Could you provide some example based on data imported from .arff file?
I just would like to have some solid example.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMM-Action-Recognition,drbinliang,171980190,Is there any document illustrates this code,open,,https://github.com/drbinliang/HMM-Action-Recognition/issues/1,"I would like to use your code, but i am wondering if you have any paper can describe your work in more details. 
In this code you used GMM-HMM as a classifier and you trained every action individually ""i mean you created 20 models that represent the whole activities in MSR3D Action dataset"", did you? 

thanks in advance  
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmmac,cmawhorter,378036133,Accept both date and x-date for skew checks,open,,https://github.com/cmawhorter/hmmac/pull/20,"Most browsers (tested on Firefox 60.2.0esr, Chrome 70.0.3538.77) do not automatically include date in the HTTP header (and forbid date to be changed/added programmatically). Therefore, Hmmac in its current form cannot be used with skew check, increasing the chance for replay attacks.
I modified lib/hmmac.js to also consider the 'x-date' header for skew check in case 'date' is not set.
Would be great to get this into the library (which I otherwise like a lot) - I hope it's still somewhat maintained.

Cheers,
Grischa"
hmmac,cmawhorter,302863772,Case sensitivity of authorization header,open,bug,https://github.com/cmawhorter/hmmac/issues/19,"Our request is failing the validate step because our authorization header is in title case. It looks like [the validate function](https://github.com/cmawhorter/hmmac/blob/master/lib/hmmac.js#L78) is looking for that header in all lower case - `req.original.headers['authorization']`.

According to the [http spec](https://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.2), header names are case-insensitive. Can the validate function be updated to ignore case?

"
hmmac,cmawhorter,233335933,Use crypto.timingSafeEqual when available,open,,https://github.com/cmawhorter/hmmac/issues/17,https://nodejs.org/api/crypto.html#crypto_crypto_timingsafeequal_a_b
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
sepp,smirarab,367250168,"Add Selenocysteine (""U"") to the set of amino acids",open,,https://github.com/smirarab/sepp/issues/39,"It would be great if you could add Selenocysteine (""U"") to the set of acceptable amino acids."
sepp,smirarab,244743862,UPP: backbone filtering issues,open,,https://github.com/smirarab/sepp/issues/26,"`-M` and `-T` options have two issues

1. `-T` is confusing. There is no way to increase minimum and maximum to 0 and infinity. Reducing minimum should not reduce maximum (it should increase it). 

2- If the sequences and the backbone share some sequence names, we should generate an error. 

@namphuon or @smirarab should fix. "
sepp,smirarab,229003751,pypi/anaconda install,open,,https://github.com/smirarab/sepp/issues/21,"Any chance that sepp will be installable sometime soon by simply using pypi or anaconda? With the current installation instructions, it's a pain to get sepp working with existing anaconda environments. Thanks!"
sepp,smirarab,222300561,Support for pplacer refpkgs ,open,,https://github.com/smirarab/sepp/issues/17,"Hi Siavash,

First, the SEPP, TIPP, UPP and HIPPI suite look very interesting and I'm excited to try them out - SEPP in particular

I'm wondering if it would it be possible to add support for pplacer refpkgs rather than using RAxML_info files to set the pplacer parameters? I have pplacer refpkgs created with trees from FastTree and it would useful to able to work with these files directly.

Regards,

Joel"
sepp,smirarab,78129788,TIPP zip file fails to work on MAC,open,,https://github.com/smirarab/sepp/issues/12,"MAC file directory is case insensitive so TIPP's reference directory does not work correctly for it because of the pyrG and pyrg folders.  Change the name and make them not clash.
"
sepp,smirarab,78126295,Default UPP alignment size to 10,open,enhancement,https://github.com/smirarab/sepp/issues/11,"Current alignment size set to 10%, needs to be set to 10.
"
sepp,smirarab,62416347,UPP: log (INFO) the number of sequences deemed fragmentary,open,enhancement,https://github.com/smirarab/sepp/issues/9,"We can output the number of fragmentary sequences to INFO.

We can also output their names (maybe to LOG?)
"
sepp,smirarab,12299269,"SEPP hangs, and fails to finish or proceed on rare instances",open,bug,https://github.com/smirarab/sepp/issues/5,"chdir to this directory
/scratch/cluster/namphuon/metagen/test2

Run this command to reproduce it:

/lusr/bin/python run_tipp.py -t
/projects/sate5/metagen/data/refpkg/rpsB.refpkg/sate.taxonomy -a
/projects/sate5/metagen/data/refpkg/rpsB.refpkg/sate.fasta -r
/projects/sate5/metagen/data/refpkg/rpsB.refpkg/sate.taxonomy.RAxML_info
-x 1 -at 0.95 -pt 0.95 -tx
/scratch/cluster/namphuon/metagen/refpkg/rpsB.refpkg/all_taxon.taxonomy
-txm /scratch/cluster/namphuon/metagen/refpkg/rpsB.refpkg/species.mapping
-adt /projects/sate5/metagen/data/genes/rpsB/results/RAXML_SATE/sate.tree.ml
-A 50 -d /scratch/cluster/namphuon/metagen/test2 -p
/scratch/cluster/namphuon/metagen/test2/temp -f
/projects/sate5/metagen/data/fragments4/rpsB.fas -c
/projects/sate5/metagen/scripts/configs/old.config -o
tipp.align_95_size_100.rpsB -cp
/scratch/cluster/namphuon/metagen/test2/checkpoint -cpi 600
"
sepp,smirarab,12067290,Making alignment outputs optional,open,enhancement,https://github.com/smirarab/sepp/issues/4,"In SEPP, alignment might not be of any interest. Creating a merged alignment can take unnecessary time. That output should be made optional. 
"
sepp,smirarab,10959849,Not Breaking edges in placement decomposition,open,enhancement,https://github.com/smirarab/sepp/issues/2,"Breaking edges in the placement decomposition creates weird situations where something is on an edge that doesn't even exists in the actual tree. 

We should change this such that placement decomposition does not remove edges. We could have some overlap between subsets if need be. 
"
sepp,smirarab,10956417,Merger outputs in old json format,open,enhancement,https://github.com/smirarab/sepp/issues/1,"Currently our merger code outputs .json files in old format. Convert it to the new format. 
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
signalAlign,ArtRand,353747933,make test errors,open,,https://github.com/ArtRand/signalAlign/issues/17,"Hi,

I'm trying to install signalAlign but I get some errors after make test ( I already copied motif.py and run make in externalTools):

OK (56 tests)

ok
test_pysam (__main__.signalAlignLibTests) ... ERROR
test_zymo_reads (__main__.SignalAlignAlignmentTest) ... FAIL
test_pUC_r9_reads_5mer (__main__.SignalAlignAlignmentTest) ... FAIL
test_pUC_r9_reads_6mer (__main__.SignalAlignAlignmentTest) ... FAIL
test_Ecoli1D_reads_5mer (__main__.SignalAlignAlignmentTest) ... FAIL
test_EM (__main__.signalAlign_EM_test) ... ok

======================================================================
ERROR: test_pysam (__main__.signalAlignLibTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""./testSignalAlign"", line 55, in test_pysam
    query=single_read)
  File ""/gpfs42/robbyfs/homes/aplic/noarch/build_custom/signalAlign/bin/signalAlignLib.py"", line 306, in exonerated_bwa
    query_start, query_end, reference_start, reference_end, cigar_string = parse_cigar(aln[11], int(aln[9]))
IndexError: list index out of range

======================================================================
FAIL: test_zymo_reads (__main__.SignalAlignAlignmentTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""./testSignalAlign"", line 121, in test_zymo_reads
    reference=ZYMO_REFERENCE, kmer_length=6, extra_args=""--2d "")
  File ""./testSignalAlign"", line 97, in check_alignments
    should=len(glob.glob(true_alignments + ""*.tsv""))))
AssertionError: Didn't make all alignments got 0 should be 6

======================================================================
FAIL: test_pUC_r9_reads_5mer (__main__.SignalAlignAlignmentTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""./testSignalAlign"", line 139, in test_pUC_r9_reads_5mer
    extra_args=""-T=../models/testModelR9_5mer_acegot_template.model ""
  File ""./testSignalAlign"", line 97, in check_alignments
    should=len(glob.glob(true_alignments + ""*.tsv""))))
AssertionError: Didn't make all alignments got 0 should be 10

======================================================================
FAIL: test_pUC_r9_reads_6mer (__main__.SignalAlignAlignmentTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""./testSignalAlign"", line 160, in test_pUC_r9_reads_6mer
    kmer_length=6, extra_args=""--2d "")
  File ""./testSignalAlign"", line 97, in check_alignments
    should=len(glob.glob(true_alignments + ""*.tsv""))))
AssertionError: Didn't make all alignments got 0 should be 10

======================================================================
FAIL: test_Ecoli1D_reads_5mer (__main__.SignalAlignAlignmentTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""./testSignalAlign"", line 151, in test_Ecoli1D_reads_5mer
    extra_args=""-T=../models/testModelR9p4_5mer_acegt_template.model "")
  File ""./testSignalAlign"", line 97, in check_alignments
    should=len(glob.glob(true_alignments + ""*.tsv""))))
AssertionError: Didn't make all alignments got 0 should be 3

----------------------------------------------------------------------
Ran 7 tests in 97.314s

FAILED (failures=4, errors=1)
#cd ././sonLib/bin && ./sonLibTests

Do you know what I'm doing wrong?

Many thanks,
Marc"
signalAlign,ArtRand,309086086,Not outputting methylation calls,open,,https://github.com/ArtRand/signalAlign/issues/14,"Hi,
I have run signalAlign from source and also using the docker option. However, it is only creating the tempFiles_alignment directory. I have checked that bwa is installed in the PATH.
Any suggestions on what is causing this?"
signalAlign,ArtRand,281412077,install issue import motif,open,,https://github.com/ArtRand/signalAlign/issues/13,"Hi,
I'm trying to install signalAlign but get the following error on testing:

#cd .//bin && ./signalAlignLibTests
cd .//bin && ./testSignalAlign
Traceback (most recent call last):
  File ""./testSignalAlign"", line 12, in <module>
    from signalAlignLib import get_bwa_index, exonerated_bwa, exonerated_bwa_pysam
  File ""/home/cep46/signalAlign/bin/signalAlignLib.py"", line 13, in <module>
    from motif import getMotif
ImportError: No module named motif
Makefile:39: recipe for target 'test' failed
make: *** [test] Error 1

I have added signalAlign directory to my pythonpath - and have been able to find classes serviceCourse. Where should the motif module be located?

Thanks,

Clare"
signalAlign,ArtRand,280648888,Maketest Problem,open,,https://github.com/ArtRand/signalAlign/issues/12,"I think I have installed all the required module:
user@x499-desktop:/media/user/1- 10TB/Yijun_Tian/signalAlign$ pip list
DEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.
adium-theme-ubuntu (0.3.4)
h5py (2.7.1)
numpy (1.13.3)
pandas (0.21.0)
pip (9.0.1)
pycurl (7.43.0)
pyliblzma (0.5.3)
pysam (0.13)
pysqlite (1.0.1)
python-dateutil (2.6.1)
pytz (2017.3)
rpm-python (4.12.0.1)
scour (0.32)
setuptools (20.7.0)
six (1.10.0)
unity-lens-photos (1.0)
urlgrabber (3.9.1)
wheel (0.29.0)
yum-metadata-parser (1.1.4)

But after I make test, always went wrong like this:
user@x499-desktop:/media/user/1- 10TB/Yijun_Tian/signalAlign$ make test
#cd .//bin && ./signalAlignLibTests
cd .//bin && ./testSignalAlign
Traceback (most recent call last):
  File ""./testSignalAlign"", line 11, in <module>
    from alignmentAnalysisLib import get_first_sequence
  File ""/media/user/1- 10TB/Yijun_Tian/signalAlign/bin/alignmentAnalysisLib.py"", line 8, in <module>
    from serviceCourse.parsers import read_fasta
ImportError: No module named serviceCourse.parsers
Makefile:39: recipe for target 'test' failed
make: *** [test] Error 1

Do you know why?
"
signalAlign,ArtRand,228229644,Compatible with albacore,open,,https://github.com/ArtRand/signalAlign/issues/9,It would be interesting to give compatibility with Albacore basecaller output
signalAlign,ArtRand,225005184,Installation error,open,,https://github.com/ArtRand/signalAlign/issues/8,"After completing make, successfully, when I tried 'make test', it gives me the below error 
# make test
_#cd .//bin && ./signalAlignLibTests
cd .//bin && ./testSignalAlign
test_signalAlign_library (__main__.LibTest) ... Running test case test_first_kmer_index
Running test case test_second_kmer_index
Running test case test_sixth_kmer_index
Running test case test_multiset_creation
Running test case test_word_id_to_multiset_id
Running test case test_kmer_id
Running test case test_serialization
Running test case test_nhdp_serialization
Running test case test_mle_params
Running test case test_distr_metrics
Running test case test_nhdp_distrs
Running test case test_bands
Running test case test_diagonal
Running test case test_logAdd
Running test case test_Sequence
Running test case test_referenceSequence
Running test case test_eventSequence
Running test case test_loadNanoporeRead
Running test case test_1dNanoporeRead
Running test case test_getSplitPoints
Running test case test_hdCellConstruct
Running test case test_hdCellConstructWorstCase
Running test case test_dpDiagonal
Running test case test_dpMatrix
Running test case test_getBlastPairs
sh: 1: ./cPecanLastz: not found
pclose failed when getting rid of lastz pipe with value 32512 and command ./cPecanLastz --hspthresh=1800 --chain --strand=plus --gapped --format=cigar --gap=100,100 --ambiguous=iupac,100,100 /tmp/stTmp20592_0 /tmp/stTmp20592_1
FAIL
test_pysam (__main__.signalAlignLibTests) ... ok
test_zymo_reads (__main__.SignalAlignAlignmentTest) ... ok
test_pUC_r9_reads_5mer (__main__.SignalAlignAlignmentTest) ... ok
test_pUC_r9_reads_6mer (__main__.SignalAlignAlignmentTest) ... ok
test_Ecoli1D_reads_5mer (__main__.SignalAlignAlignmentTest) ... ok
test_EM (__main__.signalAlign_EM_test) ... ok

======================================================================
FAIL: test_signalAlign_library (__main__.LibTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""./testSignalAlign"", line 37, in test_signalAlign_library
    self.assertTrue(result == 0, ""signalAlign Library Tests Fail"")
AssertionError: signalAlign Library Tests Fail

----------------------------------------------------------------------
Ran 7 tests in 137.662s

FAILED (failures=1)
#cd ././sonLib/bin && ./sonLibTests_


"
signalAlign,ArtRand,191335634,Metrichor version compatibility,open,,https://github.com/ArtRand/signalAlign/issues/7,"Hi Art,
I'd like to use signalAlign with R9 (not R9.4) data called with dragonet 1.23.0 can the program be modified for this please?
Thanks,
Jack"
signalAlign,ArtRand,172820490,methylation calling ,open,,https://github.com/ArtRand/signalAlign/issues/4,"Hi Art,

I'm trying to call methylation sites in the ecoli test data but there are no results in the output directory. I took out the --twoWay flag, since it was causing an error (& deprecated in favour of -x twoWay?), but otherwise the command is as in the manual:

> ./runSignalAlign -d ../tests/minion_test_reads/ecoli/ -r ../tests/test_sequences/E.coli_K12.fasta -T ../../HDP_models/ecoli_models/template_trained.hmm -C ../../HDP_models/ecoli_models/complement_trained.hmm -tH ../../HDP_models/ecoli_models/template.multisetPrior2.nhdp -cH ../../HDP_models/ecoli_models/complement.multisetPrior2.nhdp -smt=threeStateHdp -q ../tests/test_regions/test_sites_bal_1.tgt -p ../tests/test_regions/test_labels_bal_1.tsv -x twoWay -s -o ../../ 2> ../../a.err

It seems to run, with the following a.err: 

> Got 11 files to align to
> 
> #  signalAlign - finished alignments

But the ../../tempFiles_alignment/ directory then contains only the reference files and fastas in  directories labelled tempFiles__readname_. What type of output should this command be producing or is there another way of calling methylated sites? 

Thanks
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
nodehmm,liwenzhu,319221639,unable to run code ,open,,https://github.com/liwenzhu/nodehmm/issues/6,"hi i tried all the command to run as you mention in the docs  but it give me error 
i tried all the commands to run it 
npm run-script test-cov
npm test
npm run-script prepublish

all the above commands give errors 
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMM-python,Eilene,251302990,作者您可以给出这段代码的题目吗？,open,,https://github.com/Eilene/HMM-python/issues/1,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
mapmatching,simonscheider,382990726,The probability calculation is different from the paper,open,,https://github.com/simonscheider/mapmatching/issues/9,"Why is the transition probability calculation different from the paper? 
How do you consider it?
Thank you very much"
mapmatching,simonscheider,382286713,Install on linux,open,,https://github.com/simonscheider/mapmatching/issues/8,Is there a way to use this tool on linux? The installation provided only works for windows.
mapmatching,simonscheider,378597054,Environmental requirements,open,,https://github.com/simonscheider/mapmatching/issues/7,"Dear Simonscheider, 
I am very interested in this tool!
Could we use this tool on ArcGIS 10.5 in Windows 10 (64 bit)? 

Many Thanks in advances,
Mat "
mapmatching,simonscheider,368727918,unable to load test data,open,,https://github.com/simonscheider/mapmatching/issues/6,"I installed the mapmatch toolbox succesfully, but I get an Memory-error-message. Any idea how to avoid this?



 "
mapmatching,simonscheider,329688585,nothing happened after running the tool,open,,https://github.com/simonscheider/mapmatching/issues/5,arcgis 10.5。install networkx、GDAL，and run the tool successfully。but nothing exported after running the tool.
mapmatching,simonscheider,301997036,Can ArcCatalog 10.2 use the toolbox,open,,https://github.com/simonscheider/mapmatching/issues/4,"I successfully installed gdal2.2.3 and networkx2.1. And I also followed the instructions to install mapmatcher-1.0.win32.exe, but I met the problem in the following picture.
![2](https://user-images.githubusercontent.com/26688791/36934546-7c07ddce-1f26-11e8-8163-0976d92c7a46.PNG)
I found nothing in mapMatch tool. My version of arcCatalog is 10.2. I would appreciate it if you can reply.Thanks.

"
mapmatching,simonscheider,231930168,"Error: getSegmentInfo, 'OBJECTID' Field name",open,enhancement,https://github.com/simonscheider/mapmatching/issues/1,"I had ran the python toolbox in ArcGis for the test data provided, it ran successfully

I was testing on one of my data, it threw the following error:
in getSegmentInfo, line 434,  RuntimeError : a column specified that doe not exist.

I guess instead of using 'OBJECTID' as field name, we can use 'OID@' to access the objectid Field. ?
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,choge,158666701,"ValueError: operands could not be broadcast together with shapes (1,5) (78,) ",open,,https://github.com/choge/hmm/issues/1,"hi，I use my data 　as fellow: 
import numpy as np

import hmm
import pandas as pd
from pandas import DataFrame,Series

Pi=np.array([[0.1,0.2,0.3,0.2,0.2]])
A= np.array([
  [0.1,0.2,0.3, 0.2,0.2],
  [0.1,0.2,0.3, 0.3,0.1],
  [0.3,0.2,0.1, 0.3,0.1],
  [0.3,0.2,0.3, 0.1,0.1],
  [0.2,0.2,0.2, 0.3,0.1]
])

def func(i, j):
    return (i_0+1/78) \* (j_0+1/78)

B = np.fromfunction(func, (len(Pi[0]),78))

data=pd.read_csv('gesture.csv',parse_dates=True)

def data_to_matrix(data):
    length= int(len(data)/26)

```
for i in range(length):
    gesture=data[:][26*i:(i+1)*26]
    series=pd.concat([pd.concat([gesture['x'],gesture['y']],ignore_index=True),gesture['z']],ignore_index=True)
    if i == 0:
        df=DataFrame(series)
    else:
        df = pd.concat([df, DataFrame(series)], axis=1, join_axes=[df.index])

sequence=df.values.T

return sequence
```

matrix=data_to_matrix(data)

obserstates=matrix[:int(len(matrix)*0.8),:]
print(len(obserstates))
print(obserstates.shape)

h =hmm.HMM(A, B, Pi)

print(h._t.shape,h._e.shape,h._i.shape)
h.baum_welch(obserstates, threshold=1e-6, pseudocounts=[1e-5, 1e-5, 1e-5])
print(h._t,h._e,h._i)
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
timit_tools,syhw,212687519,why didnt you tried with RNN?,open,,https://github.com/syhw/timit_tools/issues/3,
timit_tools,syhw,184855283,Missing implementation of viterbi?,open,,https://github.com/syhw/timit_tools/issues/2,"Hi,
in your readme i have read that i can pass pickled dbn into viterbi : ""python ../src/viterbi.py output_dbn.mlf /fhgfs/bootphon/scratch/gsynnaeve/TIMIT/test/test.scp ../tmp_train/hmm_final/hmmdefs --d ../dbn_5.pickle ../to_int_and_to_state_dicts_tuple.pickle""
However when i opened viterby.py file i noticed that --d flag is not even in part of code responsible for parsing input arguments. Did you by any chance forget to push code? How do I connect dbn with viterbi?
"
timit_tools,syhw,55139419,Problem with extraction of features,open,,https://github.com/syhw/timit_tools/issues/1,"Even though I have "" spectral "" folder and timit-tools folder in same directory it is displaying error that "" You need spectral (in the parent folder) "". Please help me in this issue. Below image is part of directory structure in my system.
![screenshot from 2015-01-22 16 22 18](https://cloud.githubusercontent.com/assets/5196135/5854359/f22494ce-a252-11e4-931c-df5389714396.png)
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
jitar,danieldk,167909903,"(Enhancement) Add ""hasCapitalizationInfo"" option",open,,https://github.com/danieldk/jitar/issues/3,"jitar seems to be tagset agnostic except for one line in HMMTagger which assumes that tags are prefixed with the capitalization info added by the `FrequenciesCollector` :

```
String tag = d_numberTags.get(tagNumber).substring(2);
```

An option to trim the first 2 characters or not would allow an arbitrary tagset to be used as well as supporting models created in earlier jitar versions. 
"
jitar,danieldk,167907876,(Enhancement) Add ability to exclude tags in SuffixWordHandler,open,,https://github.com/danieldk/jitar/issues/2,"The ability to specify tags that should not be predicted by the SuffixWordHandler would be useful to avoid the handler guessing tags that are unlikely/unwanted such as pronouns, numbers, articles, prepositions..
This looks it could be done by using the current `skip`  `HashSet` .
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
tree-hmm,uci-cbcl,76914135,Error with infer,open,,https://github.com/uci-cbcl/tree-hmm/issues/3,"When running the following command:
tree-hmm infer 15 --max_iter 5 --max_E_iter 10 --approx poc --epsilon 1e-5 --epsilon_e 1e-5 --run_local ""observations.chr_.chunk_.npy""

I got the following error:

normalize aggregation... total free energy is: 857878.004154
Traceback (most recent call last):
  File ""/home/gene245/cwlewis/tree-hmm/bin/tree-hmm"", line 9, in <module>
    treehmm.main()
  File ""/home/gene245/cwlewis/tree-hmm/treehmm/**init**.py"", line 155, in main
    args.func(args)  # do inference, downloading, or data conversion
  File ""/home/gene245/cwlewis/tree-hmm/treehmm/do_parallel.py"", line 151, in do_parallel_inference
    args.emit_probs[:] = sp.dot(sp.diag(1. / args.emit_sum), args.emit_probs)
ValueError: shapes (2,) and (15,2) not aligned: 2 (dim 0) != 15 (dim 0)

Any idea how to fix this? Thanks!!
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMM-POS,esbie,180613789,"RegEx infinite loop on word.matches(""(\\p{Punct}+|\\p{Digit}+)+"")",open,,https://github.com/esbie/HMM-POS/issues/1,"For sequence like ""_________________________-____"" this generates an infinite loop
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,datamicroscopes,69217217,What format are we inputting the data? ,open,question,https://github.com/datamicroscopes/hmm/issues/11,"Is this the same as the way we're taking in mixture model data?
"
hmm,datamicroscopes,67224879,Add a wrapper for doing beam sampling in Spark,open,enhancement,https://github.com/datamicroscopes/hmm/issues/9,"From @dpfau 
"
hmm,datamicroscopes,67224660,Add Hierarchical Pitman-Yor,open,enhancement,https://github.com/datamicroscopes/hmm/issues/8,"From @dpfau

Likely not for release @ericmjonas ?
"
hmm,datamicroscopes,67224199,Hash Map from State to Index,open,enhancement,https://github.com/datamicroscopes/hmm/issues/7,"From @dpfau 

Instead of always having the K active states be labeled 1-K, have some hash map that maps a state to its index, like ""assigments_"" in group_manager. That way I don't have to relabel every single state greater than the state being removed each time a state is removed after one iteration of MCMC.
"
hmm,datamicroscopes,67223688,Generalize to other observation models,open,enhancement,https://github.com/datamicroscopes/hmm/issues/6,"From @dpfau  

""You should be able to plug in any prior/likelihood pair, even non-conjugate as long as you provide a kernel for posterior sampling.""
"
hmm,datamicroscopes,67223472, Refactor to be consistent with the rest of datamicroscopes.,open,enhancement,https://github.com/datamicroscopes/hmm/issues/5,"According to @dpfau 

""Most of the sampler can be divided into a stick-breaking representation HDP class (the state) which we might actually want to move to common at some point, and code for beam sampling which does nothing to the state but change assignments""
"
hmm,datamicroscopes,67219974,Viz,open,enhancement,https://github.com/datamicroscopes/hmm/issues/4,
hmm,datamicroscopes,67217545,examples,open,enhancement,https://github.com/datamicroscopes/hmm/issues/3,"look at standard HMM examples (old faithful, nile flooding, dice)

[Old Faithful](http://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat)

[Nile Flow](http://vincentarelbundock.github.io/Rdatasets/csv/datasets/Nile.csv)
"
hmm,datamicroscopes,67216069,figure out public-facing API ,open,enhancement,https://github.com/datamicroscopes/hmm/issues/2,"queries, samples, etc.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
scala-hmm,skrusche63,253321566,License missing,open,,https://github.com/skrusche63/scala-hmm/issues/1,"Could you please add a license file to the repository? Although the classes themselves are licensed under GPL in the class descriptions, the pom.xml file is not (so far)."
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HiddenMarkovModel_TensorFlow,MarvinBertin,372861841,ExpandDims dim value error,open,,https://github.com/MarvinBertin/HiddenMarkovModel_TensorFlow/issues/3,"My problem is that I give observation sequence 3~5 states, the program can run, but when I give observation sequence 2 states, it raised some errors.

Below is error,

ValueError: dim 1 not in the interval [-1, 0]. for 'Train_Baum_Welch/EM_step-0/Re_estimate_transition/Smooth_gamma/ExpandDims' (op: 'ExpandDims') with input shapes: [], [] and with computed input tensors: input[1] = <1>.

Is it caused by the tensor dimension? Could you please help me to solve the issue? Appreciate for your help. 

Warm regards, 
Jacky"
HiddenMarkovModel_TensorFlow,MarvinBertin,241137848,running forward backward AttributeError ,open,,https://github.com/MarvinBertin/HiddenMarkovModel_TensorFlow/issues/1,"It seems in running forward backward algorithm there are errors: 

`AttributeError                            Traceback (most recent call last)
<ipython-input-17-1895d0c45e74> in <module>()
      2 model =  HiddenMarkovModel_FB(trans, emi, p0)
      3 
----> 4 results = model.run_forward_backward(obs_seq)
      5 result_list = [""Forward"", ""Backward"", ""Posterior""]
      6 

/Users/xxx/Desktop/hmm/forward_bakward.pyc in run_forward_backward(self, obs_seq)
    207         with tf.Session() as sess:
    208 
--> 209             forward, backward, posterior = self.forward_backward(obs_seq)
    210             sess.run(tf.initialize_all_variables())
    211             return sess.run([forward, backward, posterior])

/Users/xxx/Desktop/hmm/forward_bakward.pyc in forward_backward(self, obs_seq)
    188 
    189         # forward belief propagation
--> 190         self._forward(obs_prob_seq)
    191 
    192         # backward belief propagation

/Users/xxx/Desktop/hmm/forward_bakward.pyc in _forward(self, obs_prob_seq)
    119             prior_prob = tf.matmul(prev_prob, self.T)
    120             # forward belief propagation
--> 121             forward_score = tf.mul(prior_prob, tf.cast(obs_prob_seq[step, :], tf.float64))
    122             # Normalize score into a probability
    123             forward_prob = tf.reshape(forward_score / tf.reduce_sum(forward_score), [-1])

AttributeError: 'module' object has no attribute 'mul'`"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMMER2GO,sestaton,382655916,Program lookup in the PATH is reversed,open,,https://github.com/sestaton/HMMER2GO/issues/16,"The research of an external program in the PATH variable returns the last occurrence of the program instead of the first occurrence. Please correct this, it is not the right behaviour."
HMMER2GO,sestaton,189041179,can't fetch files from ftp.geneontology.org,open,,https://github.com/sestaton/HMMER2GO/issues/10,"Hi, I am having a problem when ""make test"" the program. It can't fetch files from the ftp.

Below is the detail log about the issue, I changed the code to DEBUG => 1 in the perl module where defining the $ftp connection. 

Please advise, thanks!

```
$ hmmer2go fetchmap -o pfam2go
Net::FTP>>> Net::FTP(3.09)
Net::FTP>>>   Exporter(5.72)
Net::FTP>>>   Net::Cmd(3.09)
Net::FTP>>>   IO::Socket::SSL(2.033)
Net::FTP>>>     IO::Socket::IP(0.37)
Net::FTP>>>       IO::Socket(1.38)
Net::FTP>>>         IO::Handle(1.35)
Net::FTP=GLOB(0x186ba5c8)<<< 220 (vsFTPd 2.2.2)
Net::FTP=GLOB(0x186ba5c8)>>> USER anonymous
Net::FTP=GLOB(0x186ba5c8)<<< 331 Please specify the password.
Net::FTP=GLOB(0x186ba5c8)>>> PASS ....
Net::FTP=GLOB(0x186ba5c8)<<< 230 Login successful.
Net::FTP=GLOB(0x186ba5c8)>>> CWD /pub/go/external2go
Net::FTP=GLOB(0x186ba5c8)<<< 250 Directory successfully changed.
Net::FTP=GLOB(0x186ba5c8)>>> FEAT
Net::FTP=GLOB(0x186ba5c8)<<< 211-Features:
Net::FTP=GLOB(0x186ba5c8)<<<  EPRT
Net::FTP=GLOB(0x186ba5c8)<<<  EPSV
Net::FTP=GLOB(0x186ba5c8)<<<  MDTM
Net::FTP=GLOB(0x186ba5c8)<<<  PASV
Net::FTP=GLOB(0x186ba5c8)<<<  REST STREAM
Net::FTP=GLOB(0x186ba5c8)<<<  SIZE
Net::FTP=GLOB(0x186ba5c8)<<<  TVFS
Net::FTP=GLOB(0x186ba5c8)<<<  UTF8
Net::FTP=GLOB(0x186ba5c8)<<< 211 End
Net::FTP=GLOB(0x186ba5c8)>>> SIZE pfam2go
Net::FTP=GLOB(0x186ba5c8)<<< 213 715160
Net::FTP=GLOB(0x186ba5c8)>>> PASV
Net::FTP=GLOB(0x186ba5c8)<<< 227 Entering Passive Mode (171,67,205,83,47,133).
Net::FTP=GLOB(0x186ba5c8)>>> RETR pfam2go
Net::FTP=GLOB(0x186ba5c8)<<< 425 Failed to establish connection.
get failed Failed to establish connection.
 will retry. at /home/bma/perl5/lib/perl5/HMMER2GO/Command/fetchmap.pm line 80.
Use of uninitialized value $lsize in numeric eq (==) at /home/bma/perl5/lib/perl5/HMMER2GO/Command/fetchmap.pm line 83.
Use of uninitialized value $lsize in concatenation (.) or string at /home/bma/perl5/lib/perl5/HMMER2GO/Command/fetchmap.pm line 83.
Failed to fetch complete file: pfam2go (local size: , remote size: 715160), will retry. at /home/bma/perl5/lib/perl5/HMMER2GO/Command/fetchmap.pm line 83.
Use of uninitialized value $lsize in numeric eq (==) at /home/bma/perl5/lib/perl5/HMMER2GO/Command/fetchmap.pm line 86.

```"
HMMER2GO,sestaton,125100445,Add option to 'run' subcommand to execute blastx,open,,https://github.com/sestaton/HMMER2GO/issues/5,"We should run similary- and model-based searches. Currently, only model-bases searches are used. The `run` command is easy to extend, this issue is a reminder to add this option.
"
HMMER2GO,sestaton,46048782,Add benchmarks,open,enhancement,https://github.com/sestaton/HMMER2GO/issues/2,"Include BLAST2GO, Trinotate...others? Add suggestions here. 

Importantly, I need to select a common set of analyses to perform to be fair.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
TitanCNA,gavinha,383376633,Can Titan deal with WES data?,open,,https://github.com/gavinha/TitanCNA/issues/47,"Hi
I want to use TitanCNA and PhyloWGS on my WES data. I noticed that these tools are designed for WGS, so I wonder that have you tried TitanCNA on WES data and does it work well?
Many thanks!

Yang"
TitanCNA,gavinha,382799627,selectSolution not selecting the best solution,open,,https://github.com/gavinha/TitanCNA/issues/46,"In a test sample I'm finding that `selectSolution` is not selecting ""what should biologically speaking"" be the best solution.

For some reason, it prefers this ploidy=4, clusters=3 solution:

![p4c3](https://user-images.githubusercontent.com/9220167/48794867-3796a000-ecc9-11e8-8f53-409caf3f3369.png)

```
Normal contamination estimate:	0.6033
Average tumour ploidy estimate:	3.869
Clonal cluster cellular prevalence Z=2:	1 0.7426
AllelicRatio binomial means for clonal cluster Z=1:	0.5 0.6237 0.6983 0.5198 0.7483 0.5828 0.784 0.642 0.5284 0.8109 0.6865 0.5622 0.8318 0.7212 0.6106 0.5332 0.8485 0.749 0.6494 0.5498 0.8623 0.7717 0.6811 0.5906 0.5362
logRatio Gaussian means for clonal cluster Z=1:	-1.184 -0.7739 -0.455 -0.455 -0.194 -0.194 0.02699 0.02699 0.02699 0.2186 0.2186 0.2186 0.3877 0.3877 0.3877 0.3877 0.539 0.539 0.539 0.539 0.6759 0.6759 0.6759 0.6759 0.6759
AllelicRatio binomial means for clonal cluster Z=2:	0.5 0.5864 0.6473 0.5147 0.6926 0.5642 0.7276 0.6138 0.5228 0.7554 0.6532 0.5511 0.7781 0.6854 0.5927 0.5278 0.7969 0.7121 0.6272 0.5424 0.8128 0.7346 0.6564 0.5782 0.5313
logRatio Gaussian means for clonal cluster Z=2:	-0.9585 -0.6849 -0.455 -0.455 -0.2568 -0.2568 -0.08252 -0.08252 -0.08252 0.07294 0.07294 0.07294 0.2133 0.2133 0.2133 0.2133 0.3411 0.3411 0.3411 0.3411 0.4586 0.4586 0.4586 0.4586 0.4586
logRatio Gaussian variance:	0.009996 0.009996 0.01096 0.01096 0.009909 0.009909 0.01014 0.01014 0.01014 0.01051 0.01051 0.01051 0.009999 0.009999 0.009999 0.009999 0.01001 0.01001 0.01001 0.01001 0.01007 0.01007 0.01007 0.01007 0.01007
Number of iterations:	5
Log likelihood:	-52150
S_Dbw dens.bw (LogRatio):	0.1997 
S_Dbw scat (LogRatio):	1.0000 
S_Dbw validity index (LogRatio):	1.1997 
S_Dbw dens.bw (AllelicRatio):	0.4919 
S_Dbw scat (AllelicRatio):	1.0000 
S_Dbw validity index (AllelicRatio):	1.4919 
S_Dbw dens.bw (Both):	0.6916 
S_Dbw scat (Both):	2.0000 
S_Dbw validity index (Both):	2.6916 
```

Over a more meaningful ploidy = 2 solution like this one:

![screen shot 2018-11-20 at 1 40 44 pm](https://user-images.githubusercontent.com/9220167/48795101-e5a24a00-ecc9-11e8-98fe-28bac7292424.png)


```
Normal contamination estimate:	0.401
Average tumour ploidy estimate:	1.911
Clonal cluster cellular prevalence Z=1:	1
AllelicRatio binomial means for clonal cluster Z=1:	0.5 0.7138 0.7995 0.53 0.8457 0.6152 0.8746 0.6873 0.5375 0.8944 0.7366 0.5789 0.9088 0.7725 0.6363 0.5409 0.9197 0.7998 0.6799 0.56 0.9283 0.8212 0.7142 0.6071 0.5428
logRatio Gaussian means for clonal cluster Z=1:	-1.279 -0.4744 0.03912 0.03912 0.4171 0.4171 0.7163 0.7163 0.7163 0.964 0.964 0.964 1.175 1.175 1.175 1.175 1.36 1.36 1.36 1.36 1.523 1.523 1.523 1.523 1.523
logRatio Gaussian variance:	0.009996 0.01122 0.01062 0.01062 0.01015 0.01015 0.01004 0.01004 0.01004 0.01 0.01 0.01 0.009996 0.009996 0.009996 0.009996 0.009996 0.009996 0.009996 0.009996 0.009996 0.009996 0.009996 0.009996 0.009996
Number of iterations:	5
Log likelihood:	-60370
S_Dbw dens.bw (LogRatio):	0.1482 
S_Dbw scat (LogRatio):	1.0000 
S_Dbw validity index (LogRatio):	1.1482 
S_Dbw dens.bw (AllelicRatio):	0.4143 
S_Dbw scat (AllelicRatio):	1.0000 
S_Dbw validity index (AllelicRatio):	1.4143 
S_Dbw dens.bw (Both):	0.5626 
S_Dbw scat (Both):	2.0000 
S_Dbw validity index (Both):	2.5626 
```

What parameters are used to select the optimal variant and how can we adjust this? LOH of chromosome arms 1p/19q (as in the second, non-selected solution) is a well recognized marker of this cancer type. 

UPDATE: I guess it looks at the log-likelihood. Why do you think the first solution was scored higher than the second solution?
"
TitanCNA,gavinha,379163153,computeSDbwIndex correct?,open,,https://github.com/gavinha/TitanCNA/issues/43,"In file https://github.com/gavinha/TitanCNA/blob/master/R/utils.R
in function computeSDbwIndex code:

```r
else if (S_Dbw.method == ""Tong""){
			###### NOTE #######
			## The authors originally used ((N-ni)/N), but this is incorrect ##
			## We want a weighted-sum ##
			scat.Ci[i] <- ((ni) / N) * (var.Ci/var.D)
```
Are your sure? 

In paper (Youngok Kim and Soowon Lee. A clustering validity assessment Index., 2003,, 602–608):

If two clusters have the same variance but different numbers of tuples, the variance
of cluster with more tuples has more effect on the scatter function than the one with
less tuples. By minimizing the influence of small clusters or noisy clusters, Scat* can
be more robust than Scat of S_Dbw index.

Alexander Lashkov"
TitanCNA,gavinha,373195263,Using GATK CNV with TITAN,open,,https://github.com/gavinha/TitanCNA/issues/38,"Hi!

I'm wondering how to use the new GATK4 copy number segmentation as input to TITAN. I guess it should be possible (see eg. https://software.broadinstitute.org/gatk/documentation/article?id=11088) but I'm not sure how. I was able to use a recent pipeline to convert its output to a format that mirrors the Allelic CapSeg output, which could be used for eg. ABSOLUTE. But I'd like to use TITAN.

Any advice?

Thanks!"
TitanCNA,gavinha,358204666,prevalenece -> prevalence ,open,,https://github.com/gavinha/TitanCNA/pull/34,Should be modified on https://bioconductor.org/packages/release/bioc/html/TitanCNA.html as well. Thanks!
TitanCNA,gavinha,352288602,choosing maxCN value,open,,https://github.com/gavinha/TitanCNA/issues/28,"Hi Gavin,

I had a general question about TitanCNA, I saw that there is a maxCN value under 'ichorCNA_maxCN' and this has a default value of 8. For the tumor sample that I'm working with, I know that the CN's can be up to 100 but I am not sure what the maxCN value would be. I saw that the newest version of TitanCNA has the function correctIntegerCN(). Does the new function correctIntegerCN() take care of this issue of not knowing what your maxNC should be and does it simply replace the maxCN when it is higher than inputed value? 

I also read that this new function re-calculates the CN by ""correcting log ratio based on purity and ploidy and then convert to decimal CN value"" and so are the purity and ploidy also values that I should be specifying myself (since the CN value is calculated off of that) or are these also estimated?

Thanks,
Paulina "
TitanCNA,gavinha,350559402,TITAN and phyloWGS (getting majorCN and minorCN),open,,https://github.com/gavinha/TitanCNA/issues/27,"Hi Gavin,

I was able to successfully run TITAN on my BAM files, however, I am trying to run the CNV information through phyloWGS (which says that it is compatible with titan's cnv file format), but it says that my input cnv file should include the following fields: 
chromosome
Start_Position
End_Position
MajorCN
MinorCN

I saw in the documentation of the R package that by doing 

> segs <- outputTitanSegments(results, id = ""test"", convergeParams, filename = NULL, igvfilename = NULL)
> head(segs)


you can get a output with the following sections:
1. Sample: Name of sample
2. Chromosome, Start_Position.bp., End_Position.bp.: Coordinates of segment 3. Length.snps.: Number of SNPs in the segment
4. Median_Ratio: Median allelic ratio across SNPs in the segment
5. Median_logR: Median log ratio across SNPs in the segment
6. TITAN_state, TITAN_call, Copy_Number: Same as defined above
7. MinorCN: Copy number of minor allele
8. MajorCN: Copy number of major allele
9. Clonal_Cluster, Cellular_Frequency: Same as defined above

From running the snakemake I found some output files that had the Chr,  start, end and copy.number (specifically my tumor_sample_1.seg file in the results/ichorCNA/sample1 folder) however, I can't seem to find them in any of my output files with MinorCN or MajorCN. I've never used R before and was wondering if this was the only way to get this output or if I can get it from the snakemake output?   

Thanks,
Paulina
"
TitanCNA,gavinha,346154224,seqlevelsStyle error,open,,https://github.com/gavinha/TitanCNA/issues/25,"Hello!

When i trying to run snakemake version of pipeline using hg38 genome, when this command is executed:
```
Rscript ../R_scripts/titanCNA.R --hetFile results/titan/tumCounts/tumor_sample_1.tumCounts.chrom_fix.txt --cnFile results/ichorCNA/tumor_sample_1/tumor_sample_1.correctedDepth.txt --outFile results/titan/hmm/titanCNA_ploidy3/tumor_sample_1_cluster1.titan.txt --outSeg results/titan/hmm/titanCNA_ploidy3/tumor_sample_1_cluster1.segs.txt --outParam results/titan/hmm/titanCNA_ploidy3/tumor_sample_1_cluster1.params.txt --outIGV results/titan/hmm/titanCNA_ploidy3/tumor_sample_1_cluster1.seg --outPlotDir results/titan/hmm/titanCNA_ploidy3/tumor_sample_1_cluster1/ --libdir ../../ --id tumor_sample_1 --numClusters 1 --numCores 1 --normal_0 0.5 --ploidy_0 3 --chrs ""c(1:22)"" --estimateNormal map --estimatePloidy True --estimateClonality True  --centromere /home/vsvekolkin/projects/ichorCNA/inst/extdata/GRCh38.GCA_000001405.2_centromere_acen.txt --alphaK 2500 --txnExpLen 1e15 --plotYlim ""c(-2,4)""
```
Following error is produced:
```
Error in (function (classes, fdef, mtable)  : 
  unable to find an inherited method for function ‘seqinfo’ for signature ‘""integer""’
Calls: seqlevelsStyle<- ... seqlevels -> seqlevels -> seqlevels -> seqinfo -> <Anonymous>
Execution halted
```
I believe this is error is caused by parameter --chrs ""c(1:22)"", which in turn affects GenomeInfoDb , which is used in following line of runTitanCNA script:
```
seqinfo <- Seqinfo(genome=genomeBuild)
seqlevelsStyle(chrs) <- genomeStyle
```
Parameter creates integer array, i tried to convert it into character array using: --chrs ""paste(c(1:22))""

And i get following error:
```
Running TITAN...
titan: Loading data results/titan/tumCounts/tumor_sample_1.tumCounts.chrom_fix.txt
titan: Loading GC content and mappability corrected log2 ratios...
titan: Extracting read depth...
Removed 168 centromeric positions
Removed Chrs: 
titan: Loading default parameters
titan: Using 1 cores.
titan: Parameter estimation
Error in chrsI[[i]] <- which(data$chr == chrs[i]) : 
  attempt to select less than one element in integerOneIndex
Calls: runEMclonalCN
Execution halted
```
I believe this is caused by incorrect filtering in removeCentromere function, but i cannot fix cause of error. Any help is appreciated."
TitanCNA,gavinha,324637289,runTitanCNA and selectSolution error,open,,https://github.com/gavinha/TitanCNA/issues/24,"Hi, 
I am trying to run titanCNA on matched tumor-normal WES samples. I am following snakemake workflow but I am also running it inside of a HPC cloud. I downloaded both titanCNA and ichorCNA. 
-My first question is does the parameters of both config files (titan and ichor) need to be matched? I know titan snakefile invokes the others but in titan config file there is still a path for ichor script and libdir so I thought it migth affect. 
-When I changed the binsize from 10.000 to 100.000 or 1.000.000 the workflow gives error on earlier stages of the jobs. When I keep it at 10.00 I still get an error at runTitanCNA which is:

```
Error in job runTitanCNA while creating output files results/titan/hmm/titanCNA_ploidy2/RTF8_D_cluster2/, results/titan/hmm/titanCNA_ploidy2/RTF8_D_cluster2.titan.txt, results/titan/hmm/titanCNA_ploidy2/RTF8_D_cluster2.params.txt, results/titan/hmm/titanCNA_ploidy2/RTF8_D_cluster2.segs.txt, results/titan/hmm/titanCNA_ploidy2/RTF8_D_cluster2.seg.
RuleException:
CalledProcessError in line 53 of /mnt/lustre/sarihae/HNSCC_WES/titanCNA/scripts/snakemake/TitanCNA.snakefile: returned non-zero exit status 1.
  File ""/mnt/lustre/sarihae/HNSCC_WES/titanCNA/scripts/snakemake/TitanCNA.snakefile"", line 53, in __rule_runTitanCNA
  File ""/cm/shared/apps/python/3.6.2/lib/python3.6/concurrent/futures/thread.py"", line 55, in run
Removing output files of failed job runTitanCNA since they might be corrupted:
results/titan/hmm/titanCNA_ploidy2/RTF8_D_cluster2/
Will exit after finishing currently running jobs.
Exiting because a job execution failed. Look above for error message
```

I looked at the config file and changed libdirs from `path/to/titanCNA/R/ path/to/ichorCNA-master/R/` to `path/to/titanCNA/ path/to/ichorCNA-master/` and now I get an error at selectSolution which looks like:

```
Error in job selectSolution while creating output file results/titan/hmm/optimalClusterSolution.txt.
RuleException:
CalledProcessError in line 70 of /mnt/lustre/sarihae/HNSCC_WES/titanCNA/scripts/snakemake/TitanCNA.snakefile: returned non-zero exit status 2.
  File ""/mnt/lustre/sarihae/HNSCC_WES/titanCNA/scripts/snakemake/TitanCNA.snakefile"", line 70, in __rule_selectSolution
  File ""/cm/shared/apps/python/3.6.2/lib/python3.6/concurrent/futures/thread.py"", line 55, in run
Will exit after finishing currently running jobs.
Exiting because a job execution failed. Look above for error message
```
What would you think it's causing my problem, is it related to input parameters, or sth else? I'd really appreciate any kind of help and guidance cause I am a little bit confused with this workflow.
Thanks in advance,
Irem
"
TitanCNA,gavinha,318949232,Parameter Setting: maxPloidy,open,,https://github.com/gavinha/TitanCNA/issues/23,"Would you please suggest how to set some parameters?

I am working on 26 samples from 13 ovarian cancer patients but not sure if my choice of parameters was good enough. Currently, I set the maxPloidy at 4 but I found that 9 out of 26 samples were estimated to have Ploidy at 4. Do I need to increase the max Ploidy greater?

To break my questions down further,
1. Do I need to change max Ploidy higher?
2. How Ploidy and N of Cluster are related in your algorithm? 
    Would you mind to point a place, which section/page of a paper, if you already described in your publication?
3. Are there any other parameters need to be updated?
4. About model anomalies.
  I have three samples, whose purity is less than 0.3.
  Also, I got two samples converge to log-lik = - Inf.
 
  What are your suggestions on these anomalies? Do I need to dump them?

Lastly, I am using ""snakemake"" as you suggested before  and it's fantastic!
Thank you again your answers in advance (please!) and also for developing and maintaining this wonderful application with us!

Bests,
Lijin"
TitanCNA,gavinha,307893298,getPositionOverlap function,open,,https://github.com/gavinha/TitanCNA/issues/21,"Hello Gavin,

I am following your tutorial but got into a trouble from the very beginning step.
Would you mind to take a closer look at ""getPositionOverlap"" function? 
I installed this package a few weeks ago.

Thank you for your help.
Lijin

> logR <- getPositionOverlap(chr=data$chr, posn=data$posn, dataVal=cnData) 
> head(logR)
[1] NA NA NA NA NA NA
> tail(logR)
[1] NA NA NA NA NA NA
> 
"
TitanCNA,gavinha,290007158,Edits to snakemake files,open,,https://github.com/gavinha/TitanCNA/pull/17,
TitanCNA,gavinha,287911953,Script input error,open,,https://github.com/gavinha/TitanCNA/issues/16,"Hi,

I've decided to move ahead in hg38 with a smaller bin size of mapability and GC% as the mapability file is much quicker to make at smaller BED intervals. The first TenX script doesn't appear to be pulling in the haplotype.R script. Any advice on what I'm doing wrong?

Rscript getMoleculeCoverage.R 
       --id TMI40424 
       -l TitanCNA/scripts/
       -d Final_GCandMAP/ 
       -t TMI40424_Tumor.10kb.bxTile.chrX.bed 
       -n TMI40424_Germline.10kb.bxTile.chrX.bed 
       --minReadsPerBX=4 
       --chrs \""X\"" 
       --outDir TMI40424/ 
       --centromere GRCh38.p13_centromere_UCSC-gapTable.txt


Error: 

Loading required package: grid
Error: could not find function ""loadBXcountsFromBEDDir""
Execution halted


Best,
Jake"
TitanCNA,gavinha,281209052,Mappability track,open,,https://github.com/gavinha/TitanCNA/issues/15,"Hi,

Is there any chance that you have an hg38 mappability file fo rthe 10x Whole Genome Sequencing pipeline (map_hg19_10kb.wig)? I can make them via the gem-mappability tool, but a 10kb length is super inefficient in processing time.
"
TitanCNA,gavinha,255532009,"""grep"" in selectSolution can find the wrong files",open,,https://github.com/gavinha/TitanCNA/issues/10,"I had some samples called `foo_CAP` and `foo_CAP-DEEP`.

The code in `selectSolution.R` does a grep on the file list (also valid for ploidy of 3 and 4):

```r
phi2Samples <- grep(id, phi2Files, value=T)
```

However in this case it will match **both** files and cause inconsistent results to be written. As the pattern in default cases is id plus an underscore, a solution like this may work:

```r
phi2Samples <- grep(paste0(id, ""_""), phi2Files, value=T)
```

Exact string matching won't help as it will still catch the shorter string contained in the long one."
TitanCNA,gavinha,255231016,Provide a list of TitanCall abbreviations in an easily-accessible form,open,,https://github.com/gavinha/TitanCNA/issues/8,"As the subject says, having to hunt a supplementary table in a manuscript to see what an abbreviation indicates is awkward (and, IIRC, there is no ""supplementary table 2"" in the TitanCNA paper).

Ideally this should be in the README.md or in the repo (or in the help files)."
TitanCNA,gavinha,255074850,correctReadDepth requires S4Vectors but it is not imported,open,,https://github.com/gavinha/TitanCNA/issues/6,As subject says. 
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
PairHMM,MauricioCarneiro,269909119,Possible misalignment in PairHMM evaluation,open,,https://github.com/MauricioCarneiro/PairHMM/issues/35,"I was studying your implementation as a reference, sticking to the SCALAR version (not being confident with SSE and AVX), but I encountered a possible inconsistency in the evaluation of the results (which may be present in the other implementations as well). I could not verify it myself, since I haven't the reference results.

Before applying the PairHMM algorithm to a read + haplotype, the haplotype is padded and reversed.
Let MRL denote the maximum (original) read length in our batch of data.
The ""padded haplotype"" consists of:
1) heading zeroes, exactly MRL + 1
2) reversed haplotype
3) trailing zeroes, exactly MRL.

When executing the algorithm on a read of length MRL, you start comparing the first character of the haplotype and the first character of the read, which seems very reasonable.
Another reasonable point is that exactly ""read_length + haplotype_length - 1"" diagonals have to be calculated FOR ANY READ.
Since the haplotype is padded w.r.t. MRL, when evaluating PairHMM on shorter reads, you initialize the index ""d"" to ""MRL -RL"", where RL is the actual read original length.
Namely:
""for (auto d = fd; d != mrl + hl - 1; ++d) { ... }""
However, the effect should be the following. Given that you scan the input read in sequence, namely:
""for (auto r = 1u; r != rows; ++r) { ... }""
and that the haplotype is indexed even through d, namely:
""const auto hap_base = haplotype.bases[hap_offset+r-d];""
you end up comparing the first character of a shorter read to a character of the haplotype which is not the first (it should be instead the ""MRL - RL""th).
Since you for sure evaluate the right number of diagonals, you compute some of them out of the boundaries of the haplotype/scoring matrix.
This yields (possibly) wrong results.

I propose the following approach:
""for (auto d = 0; d != rl + hl - 1; ++d) { ... }""
in order to 1) align the read and haplotype to the first character and 2) evaluate the right number of diagonals.
I obviously get different results for some of the samples, while the great part of them coincide.

Since I don't know exactly if I'm missing something from the algorithm, I just though to ask you."
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
infer-hmm,oliparson,302884531,"The ZeroState should probably be parametrized by ProbInit, not with CPTTrans",open,,https://github.com/oliparson/infer-hmm/issues/4,"Hi,

I used your model as an example to build a Markov Mixture model. I reused a lot from your code, except the emissions part. I noticed that ZeroState variable (used only at t==0) is parametrized with CPTTrans, which is indexed by ZeroState. I think the Variable.Discrete should be parametrized only by ProbInit since this is what you do when you generate sample data.

I noticed this problem only today when I was not able to recover the ProbInit posterior parameters. Hope this helps.

https://github.com/oliparson/infer-hmm/blob/3d9dba0158f43a37937df20dc8935783b45e7584/HiddenMarkovModel/HiddenMarkovModel.cs#L105

Thanks again for sharing the code for the model! Keep up good work!"
infer-hmm,oliparson,301944890,Very low model probability,open,,https://github.com/oliparson/infer-hmm/issues/3,"Hi,

Thanks for sharing the code for the HMM model in Infer.NET! I learned a lot from it!

I have a question about the model probability for the test model. The inference seems to run just fine but I noticed very low model evidence, 1e-92.

Is this because only one observed sequence of emissions was observed? I believe in practice one will have a large number of sequences of emissions observed and will use them to fit the model.

Thanks!"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HanLP,hankcs,383393104,感知机分词识别的一些疑问,open,提问,https://github.com/hankcs/HanLP/issues/1030,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.0
我使用的版本是：1.7.0。

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
非常感谢作者开放自己的亿级训练结果，在使用中有些疑问希望能给予帮助：
1.使用感知机分词时对一些比较明显不是人名的词或符号识别为人名，如“否”，“归档”这些字或词单独出现时 识别为nr   ""普卡客户""中的“普卡”会被识别为地名等
2.在线演示的模型和1.6.8发布的模型是一个么，我看到之前有人问到这个，差异主要在现代汉语词典，不过使用的时候一般我在本地识别不如预期的时候，拿到在线演示上面测试时效果更好一些，有点不清楚这之间具体有哪些差异导致。
3.现在的使用上包含的人名地名较多，  常常会将地名识别成人名，我现在的解决办法只有加词典，请问下还有更好的方式解决么。 
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤
词典和模型使用的1.7.0版本，加了些地名的词典 但已确认出现问题的词的不在新加的词典中出现

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer();
        System.out.println(analyzer.analyze(""归档""));
        System.out.println(analyzer.analyze(""普卡客户""));
        System.out.println(analyzer.analyze(""谢师傅帮忙处理""))；
    }
```
### 期望输出
<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
归档/vn
普卡客户/n
谢/v 师傅/n 帮忙/v 处理/v
### 实际输出
归档/nr
普卡/ns 客户/n
谢/nr 师傅/n 帮忙/v 处理/v
<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
![qq 20181122140354](https://user-images.githubusercontent.com/18030444/48884619-2cc24380-ee60-11e8-8d9d-da0f8374c31f.png)
![qq 20181122140502](https://user-images.githubusercontent.com/18030444/48884620-2cc24380-ee60-11e8-96a7-d37ae4f49b3d.png)
![qq 20181122140536](https://user-images.githubusercontent.com/18030444/48884622-2d5ada00-ee60-11e8-89fb-acc6be19706c.png)
"
HanLP,hankcs,383352299,分词问题,open,,https://github.com/hankcs/HanLP/issues/1029,"对一篇含有“迈凯伦法拉利恩佐兰博基尼”、“迈凯伦塞纳迈凯伦卡宾”的文本采用标准分词时，分词的结果中包含这些长词。如果将这上述两个词连在一起，分词结果是两个词的结合。对于音译名的分词，连在一起的音译词会被当作一个词。是要用索引分词吗？
代码如下：
List termList1 = HanLp.segment(""迈凯伦法拉利恩佐兰博基尼, 迈凯伦塞纳迈凯伦卡宾"");
List termList2 = HanLp.segment(""迈凯伦法拉利恩佐兰博基尼迈凯伦塞纳迈凯伦卡宾"");
System.out.println(termList1);
System.out.println(termList2);
结果如下：
[迈凯伦法拉利恩佐兰博基尼/nrf, ，/w, 迈凯伦塞纳迈凯伦卡宾/nrf]
[迈凯伦法拉利恩佐兰博基尼迈凯伦塞纳迈凯伦卡宾/nrf]"
HanLP,hankcs,382934387,分词算法问题,open,,https://github.com/hankcs/HanLP/issues/1027,我想把 “机器人” 分词成 “机器” “机器人” “机” “器” “人” 这五个词应该选择哪个分词算法
HanLP,hankcs,382658858,android开发文件无法写入到data/dictionary/person/nrf.txt.trie.dat,open,,https://github.com/hankcs/HanLP/issues/1026,"当前最新版本号是：
我使用的版本是：1.7.0

## 我的问题
在android机上面做动态分词，经常会抛出
11-20 19:26:18.068 1366-2392/com.mtrobot.studio.mtrobotstudio_product E/CrashHandler: java.lang.IllegalAccessError: 不支持写入data/dictionary/person/nrf.txt.trie.dat！请在编译前将需要的数据放入app/src/main/assets/data

该异常直接导致应用程序崩溃，并且我们的系统是root权限的，不会发生读写失败的问题，查看系统日志发现nrf.txt.trie.dat从未写入成功过，请大牛帮忙解决,谢谢


"
HanLP,hankcs,380959061,构造PerceptronLexicalAnalyzer报错,open,提问,https://github.com/hankcs/HanLP/issues/1024,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
构造词法分析器，构造函数出错
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤



### 触发代码

```
		PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer(""D:/myeclipse-workplace/NLP/data-for-1.6.8/data/model/perceptron/pku199801/cws.bin"",
                HanLP.Config.PerceptronPOSModelPath,
                HanLP.Config.PerceptronNERModelPath);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->


### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
Exception in thread ""main"" java.lang.IllegalArgumentException: 错误的模型类型: 传入的不是词性标注模型，而是 CWS 模型
	at com.hankcs.hanlp.model.perceptron.PerceptronPOSTagger.<init>(PerceptronPOSTagger.java:37)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:50)
	at com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer.<init>(PerceptronLexicalAnalyzer.java:70)
	at com.NLP.DemoPerceptronLexicalAnalyzer.main(DemoPerceptronLexicalAnalyzer.java:24)

```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,380603719,pyhanlp JClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')能否加载.bin模型,open,提问,https://github.com/hankcs/HanLP/issues/1023,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

python调用hanlp时，WordVectorModel只能加载后缀为txt的词向量模型，能否加载后缀为bin为模型，如何加载

<!-- 请详细描述问题，越详细越可能得到解决 -->

### 触发代码

```
from pyhanlp import *
WordVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.WordVectorModel')
DocVectorModel = JClass('com.hankcs.hanlp.mining.word2vec.DocVectorModel')
model_file = 'E:/data/baike_26g_news_13g_novel_229g.txt'
word2vec = WordVectorModel(model_file)
doc2vec = DocVectorModel(word2vec)
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,380579286,Normalization词典CharTable.txt中①映射为数字1是否更合适？,open,提问,https://github.com/hankcs/HanLP/issues/1022,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.0
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->
分词前进行Normalization 操作，会根据 `data/dictionary/other/CharTable.txt`对每一个 字符 进行一一映射。我发现带圆圈的数字被映射为对应的汉字，是否修改为阿拉伯数字更合适？

我遇到的场景是 数字域名、号码等，阿拉伯数字是更合适的，不过是否有些场景更适合用汉字？

https://github.com/hankcs/HanLP/blob/master/data/dictionary/other/CharTable.txt#L40"
HanLP,hankcs,380102556,您好，请问在线演示中使用的是什么分词器呢，我试了很多没有发现相同的分词器,open,提问,https://github.com/hankcs/HanLP/issues/1021,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.7.0
我使用的版本是：1.7.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
您好，请问在线演示中使用的是什么分词器呢，我试了很多没有发现相同的分词器
我使用的是pyhanlp

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 我对两句话进行分词
s1 = '而这些伟人用行动阐述了实现理想的真谛。'
s2 = '为了攻克“哥德巴赫猜想”，他整天进行演算，遇到了许多挫折，但他都坚持下来了'
2. 在线演示的分词结果
![image](https://user-images.githubusercontent.com/32705832/48398994-593dd780-e75d-11e8-8c22-6db8ce44b405.png)
![image](https://user-images.githubusercontent.com/32705832/48399018-6955b700-e75d-11e8-94e4-3d2b960a30a1.png)





3. 我试的分词结果

为了/p, 攻克/v, “/w, 哥/n, 德/b, 巴赫/nrf, 猜想/v, ”/w, ，/w, 他/rr, 整天/d, 进行/vn, 演算/vn, ，/w, 遇到/v, 了/ule, 许多/m, 挫折/n, ，/w, 但/c, 他/rr, 都/d, 坚持/v, 下来/vf, 了/ule

而/c 这些/r 伟人/n 用/p 行动/vn 阐述/v 了/u 实现理想/n 的/u 真谛/n 。/w

### 触发代码

```
PerceptronLexicalAnalyzer = JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
analyzer = PerceptronLexicalAnalyzer()
print(analyzer.analyze(s))
print(analyzer.analyze(s1))
```
### 期望输出
像在线演示的分词结果一样
<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出
如上我的分词结果
<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,379848885,命名实体提取，时间词识别有误,open,提问,https://github.com/hankcs/HanLP/issues/1020,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
```
            CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
            Sentence wordList = analyzer.analyze(""下周四天气怎么样"");
```

输出结果：
![image](https://user-images.githubusercontent.com/3538629/48359900-f276da80-e6d8-11e8-9bd4-fff6e366d647.png)

该结果符合预期。

```
            CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
            Sentence wordList = analyzer.analyze(""下周三天气怎么样"");
            System.out.println(wordList);
```

输出结果

![image](https://user-images.githubusercontent.com/3538629/48359938-03275080-e6d9-11e8-8676-3cb230f9570b.png)

该结果不符合预期，应该将*下周三*识别为时间词。

请问怎么能提高时间词的识别效果？

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

### 触发代码

```
    public void testDateNer() {
        try {
            CRFLexicalAnalyzer analyzer = new CRFLexicalAnalyzer();
            Sentence wordList = analyzer.analyze(""下周三天气怎么样"");
            System.out.println(wordList);
        } catch (IOException e) {
            e.printStackTrace();
        }

    }
```


"
HanLP,hankcs,378623234,请问下这个里面是否可以用深度学习的算法来训练,open,,https://github.com/hankcs/HanLP/issues/1017,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,377405823,Translating docs and codebase to English,open,,https://github.com/hankcs/HanLP/issues/1012,Are there any forks of this repository that translate it into English ?
HanLP,hankcs,377061408,繁简转换 ：佘字转错 ：畲,open,,https://github.com/hankcs/HanLP/issues/1011,"
System.out.println(HanLP.convertToSimplifiedChinese(""佘诗曼""));
会误将 佘字转错 ：畲"
HanLP,hankcs,377016163,doc2vec的实现原理能否说一下？,open,,https://github.com/hankcs/HanLP/issues/1010,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
想使用doc2vec的接口计算句子的相似度，想仔细了解一下这个模块的原理？


<!-- 请详细描述问题，越详细越可能得到解决 -->
"
HanLP,hankcs,376811097,pyhanlp 在python3环境下data-for-1.6.8.zip 解压缩乱码,open,bug,https://github.com/hankcs/HanLP/issues/1009,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
pyhanlp 在第一次 import 的时候会下载并解压数据文件，实测python3 环境下，data-for-1.6.8使用`zipfile`直接解压会出现中文乱码（custom目录的几个文件）。python2 没有这个问题，参考后面两个链接。

## 复现问题
手工执行`static.__init__.py` 里面的代码
https://github.com/hankcs/pyhanlp/blob/master/pyhanlp/static/__init__.py#L241

### 触发代码

```python
with zipfile.ZipFile(""data-for-1.6.8.zip"", ""r"") as f:
  for fn in f.namelist():
    print(fn)
```
### 期望输出

正确的中文文件名

### 实际输出

乱码

## 其他信息
参见这两个 StackOverflow 问题：
https://stackoverflow.com/questions/41019624/python-zipfile-module-cant-extract-filenames-with-chinese-characters
https://stackoverflow.com/questions/37723505/namelist-from-zipfile-returns-strings-with-an-invalid-encoding

话说我解压了，再mac用系统自带的zip再打包，结果还是乱码：`zip -r data data`，使用7z压缩后就没问题了。`7z a -tzip data.zip data`。另外7z压缩的文件更小，推荐。

"
HanLP,hankcs,375765687,能否在依存句法分析的过程中使用指定分词器再做后续处理？,open,提问,https://github.com/hankcs/HanLP/issues/1008,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在体验过感知机分词后，确实感受到语料库的重要性，同时可以支持在线增量训练以后方便了许多。
目前我这边项目中需要使用依存句法分析后再做规则处理，目前考虑是使用nnparser，看了下之前的代码。个人理解是先分词后再进行句法分析处理，请问是否能够指定nnparser所使用的句法分析器，例如替换成感知机分词？（因为在感知机分词的模型增量训练了自己的专有词语，希望统一维护。）

我看到nnparser在26天前提交了构造函数支持传入segment。是否就是意图支持nnparser指定不同的分词器？
如果这个改动是这个意图的话，想请问一下大概何时会把这个feature给release一个版本出来？

如果短期不会放出这个release的话，是否我只需要根据这次提交所修改的nnparser的代码和abstractLexicalAnalyzer进行修改就能支持？（暂时看到这次提交只有这2个改动，不知是否还有其他需要修改的地方）
"
HanLP,hankcs,375345737,感知机训练对硬件的要求,open,提问,https://github.com/hankcs/HanLP/issues/1007,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v 1.6.8
我使用的版本是：v 1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 问题
使用结构化感知机标注框架训练自己的词性标注模型时内存溢出。
1.新闻语料文件夹大小1.7g左右
2.使用命令：java -Xmx30720m -cp hanlp-portable-1.6.8.jar:src/main/resources com.hankcs.hanlp.model.perceptron.Main -task POS -train true -model pos.bin -reference test
3.系统性能如下：
![image](https://user-images.githubusercontent.com/18524483/47700860-e3147d80-dc52-11e8-957a-65fc6c061bb1.png)
4.运行结果如下：
![a5f85674429ccc04acaf8366e186c03](https://user-images.githubusercontent.com/18524483/47700984-50281300-dc53-11e8-9e68-a6ab27f0e81c.png)
请问：
1.词性标注模型训练对内存的要求，另外特征总数达到六亿多这正常吗？
2.您当时训练分词模型large.bin的时候的硬件环境
期望尽快得到您的赐教，谢谢！





"
HanLP,hankcs,373425371,关于elasticsearch衍生项目的问题,open,提问,https://github.com/hankcs/HanLP/issues/1006,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
1.6.8

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

## 我的问题

[衍生项目的wiki](https://github.com/hankcs/HanLP/wiki/%E8%A1%8D%E7%94%9F%E9%A1%B9%E7%9B%AE) 里说elasticsearch 的两个插件都跑不起来

1.  hanlp-ext @hualongdata
2. https://github.com/shikeio/elasticsearch-analysis-hanlp

反而是这个没列出 https://github.com/KennFalcon/elasticsearch-analysis-hanlp 能跑起来。
第二个问题和这个衍生项目有关也给它开了issue ，它目前支持1.6.6， 我如果直接替换成1.6.8的data不会有问题吧？"
HanLP,hankcs,371965236,带表情符的CRF分词出现编码错误 UnicodeEncodeError,open,重复,https://github.com/hankcs/HanLP/issues/1003,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->
hanlp-1.6.3.jar
当前最新版本号是：
我使用的版本是：
pyhanlp
环境是python 3.6
<!--以上属于必填项，以下可自由发挥-->

## 我的问题
text = '主动提出拿宝宝椅，👍。十块钱😄'
seg_list = CRFnewSegment.seg(text)
seg_w_list = [term.word for term in seg_list]
comment_seg = ' '.join(seg_w_list)
print (comment_seg)

出现以下错误：UnicodeEncodeError: 'utf-8' codec can't encode character '\ud83d' in position 39: surrogates not allowed

而用HanLP.segment(text)则是正常的。去掉👍和😄，CRF也是正常的。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
 text = '主动提出拿宝宝椅，👍。十块钱😄'
seg_list = CRFnewSegment.seg(text)
seg_w_list = [term.word for term in seg_list]
comment_seg = ' '.join(seg_w_list)
print (comment_seg)
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
UnicodeEncodeError: 'utf-8' codec can't encode character '\ud83d' in position 39: surrogates not allowed
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,371916575,使用HanLP.segment()分词导致python停止工作,open,,https://github.com/hankcs/HanLP/issues/1002,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
使用HanLP.segment()对字符串进行分词时导致python停止工作。
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 触发代码

```
except_list = []
for i,item in enumerate(train_content):
    try:
        #item = text_seg(item,seg_tool='hanlp')
        item=HanLP.segment(item) # 触发问题语句
        item=[term.word for term in item]
        train_content[i] = item
        print('sent: ', i, ' ,', item)
        if i % 1000 == 0:
            print(str(i)+' lines had been segmented')
    except:
        except_list.append(i)
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->



![image](https://user-images.githubusercontent.com/44291990/47215523-46d9b380-d3d4-11e8-83ae-225f264c669c.png)
![image](https://user-images.githubusercontent.com/44291990/47215609-a041e280-d3d4-11e8-9a33-d0b4898eb8c6.png)




## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,371360159,新词发现在300mb的语料上报内存不够,open,提问,https://github.com/hankcs/HanLP/issues/1001,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：pyhanlp-0.1.44
我使用的版本是：pyhanlp-0.1.44

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
用pyhanlp的接口在300mb的语料上做新词发现，服务器内存为128g，报了out of memory，有什么优化或者解决的办法吗。
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
def hanlptest(file_path,file_out_path):
    file = open(file_path,'r',encoding='utf-8')
    fou = open(file_out_path,'w',encoding='utf-8')
    text = file.read()
    newwords = HanLP.extractPhrase(text,200)
    for item in newwords:
        fou.write(item+'\n')
    print(newwords)
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
新词
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
jpype._jexception.OutOfMemoryErrorPyRaisable: java.lang.OutOfMemoryError: GC overhead limit exceeded
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,370988834,hanlp的分词在sighan2005公共数据集上的表现,open,提问,https://github.com/hankcs/HanLP/issues/1000,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

请问有没有对比hanlp的分词效果在sighan2005公共数据集上的表现？谢谢~


"
HanLP,hankcs,370483803,加载CustomDictionary.txt.bin抛出java.lang.IllegalArgumentException异常,open,提问,https://github.com/hankcs/HanLP/issues/997,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
CustomDictionary添加了自定义词典(1.35GB), 并删除缓存重新生成了一个3GB+的CustomDictionary.txt.bin

当程序再次运行时... 加载CustomDictionary.txt.bin失败
readBytes中抛出java.lang.IllegalArgumentException异常

INFO: 自定义词典开始加载:../../myproject/data/dictionary/custom/CustomDictionary.txt
十月 16, 2018 3:12:39 下午 **com.hankcs.hanlp.corpus.io.IOUtil readBytes**
**WARNING: 读取../../myproject/data/dictionary/custom/CustomDictionary.txt.bin时发生异常java.lang.IllegalArgumentException**
十月 16, 2018 3:12:39 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
INFO: 以默认词性[n]加载自定义词典../../myproject/data/dictionary/custom/CustomDictionary.txt中……

尝试重新生成了一个小词典 是能正常运行的 是词典缓存过大导致?
<!-- 请详细描述问题，越详细越可能得到解决 -->

"
HanLP,hankcs,369988572,pyhanlp的新词发现没有涉及英文的新词,open,改进,https://github.com/hankcs/HanLP/issues/996,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：pyhanlp-0.1.44
我使用的版本是：pyhanlp-0.1.44

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
用pyhanlp的新词发现接口不会出现涉及英文单词的新词，同样使用左右熵和互信息的另外一个版本实现会发现这个词，是hanlp对所有的英文都过滤掉了吗？可以有什么办法保留英文吗？

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
def hanlptest(file_path,file_out_path):
    file = open(file_path,'r',encoding='utf-8')
    fou = open(file_out_path,'w',encoding='utf-8')
    text = file.read()
    newwords = HanLP.extractPhrase(text,200)
    for item in newwords:
        fou.write(item+'\n')
    print(newwords)
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
K线图
A股账户
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
无涉及英文的新词
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,369860135,安装完pyhanlp和jpype之后，ModuleNotFoundError: No module named '_jpype',open,,https://github.com/hankcs/HanLP/issues/995,"import from pyhanlp import *
报错：+
Traceback (most recent call last):
  File ""/home/vip/wusaifei/project/preprocess.py"", line 2, in <module>
    from pyhanlp import *
  File ""/home/vip/wusaifei/anaconda3/lib/python3.6/site-packages/pyhanlp/__init__.py"", line 11, in <module>
    from jpype import JClass, startJVM, getDefaultJVMPath, isThreadAttachedToJVM, attachThreadToJVM
  File ""/home/vip/wusaifei/anaconda3/lib/python3.6/site-packages/jpype/__init__.py"", line 17, in <module>
    from ._jpackage import *
  File ""/home/vip/wusaifei/anaconda3/lib/python3.6/site-packages/jpype/_jpackage.py"", line 18, in <module>
    import _jpype
ModuleNotFoundError: No module named '_jpype'"
HanLP,hankcs,368963765,com.hankcs.hanlp.utility.TestUtility找不到,open,提问,https://github.com/hankcs/HanLP/issues/993,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

当我在运行DemoSentimentAnalysis时，import com.hankcs.hanlp.utility.TestUtility报错，发现在utility中不存在TestUtility

"
HanLP,hankcs,368608765,可以用来做输入法的联想词嘛,open,,https://github.com/hankcs/HanLP/issues/992,
HanLP,hankcs,368094413,感知机无法对英文进行分词吗？,open,提问,https://github.com/hankcs/HanLP/issues/991,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

是否是英文训练的模型没有加入英文，所以无法对英文进行分词？

### 触发代码

```
   PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer();
   String str = ""Apache Commons is an Apache project focused on all aspects of reusable Java components. "";
   System.out.println(analyzer.analyze(str));
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[Apache/nx,  /w, Commons/nx,  /w, is/nx,  /w, an/nx,  /w, Apache/nx,  /w, project/nx,  /w, focused/nx,  /w, on/nx,  /w, all/nx,  /w, aspects/nx,  /w, of/nx,  /w, reusable/nx,  /w, Java/nx,  /w, components/nx, ./w,  /w]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
Apache Commons is an Apache project focused on all aspects of reusable Java components. /nr
```


"
HanLP,hankcs,367680517,如何改进获取摘要的结果,open,提问,https://github.com/hankcs/HanLP/issues/990,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
如何可以使得获取摘要的结果更为符合题意？
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先输入要获取摘要的内容，
       String answerString = ""一，辛亥革命给封建专制制度以致命的一击。二，辛亥革命推翻了“洋人的朝廷”, 沉重打击了帝国主义的侵略势力。三，辛亥革命为民族资本主义的发展创造了有利的条件。四，辛亥革命对近代亚洲各国被压迫民族的解放运动产生了比较广泛的影响，特别是对越南、印度尼西亚等国家的反对殖民主义的斗争起了推动作用。"";
2. 然后调用HanLP.extractSummary获取摘要
3. 接着对摘要结果进行分析。

### 触发代码

```
   List<String> summaryList = HanLP.extractSummary(answerString, 4, ""[，,。:：“”？?！!；;、]"");
		 for(String summary : summaryList){
			 System.out.println(summary);
		 }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
辛亥革命给封建专制制度以致命的一击。
辛亥革命推翻了“洋人的朝廷”, 沉重打击了帝国主义的侵略势力。
辛亥革命为民族资本主义的发展创造了有利的条件。
辛亥革命对近代亚洲各国被压迫民族的解放运动产生了比较广泛的影响。
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
辛亥革命推翻了
辛亥革命给封建专制制度以致命的一击
辛亥革命对近代亚洲各国被压迫民族的解放运动产生了比较广泛的影响
印度尼西亚等国家的反对殖民主义的斗争起了推动作用
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,367669597,语料分类训练准确率低，需要如何提高呢,open,提问,https://github.com/hankcs/HanLP/issues/989,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
我使用语料库 进行分割训练集和测试集的测试，为什么准确率这么低呢，是语料库的原因，
还是需要如何训练的问题呢。我该如何提高准确率
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤
耗时 7557 ms 加载了 9 个类目,共 1791 篇文档
     P       R      F1       A        
 34.82   66.83   45.78   82.41  互联网
 77.19   44.22   56.23   92.35   体育
 51.61   24.12   32.88   89.06   健康
 61.84   47.24   53.56   90.90   军事
 64.02   76.88   69.86   92.63   招聘
 59.52   12.56   20.75   89.34   教育
 26.04   69.35   37.86   74.71   文化
 47.06   12.06   19.20   88.72   旅游
 53.72   50.75   52.20   89.67   财经
 52.87   44.89   48.55   44.89  avg.
data size = 1791, speed = 36551.02 doc/s


### 触发代码

```
    public void testIssue1234() throws Exception
    {
        //CORPUS_FOLDER 是网上找的sogou分类库，不是mini版的
        DataSet trainingCorpus = new FileDataSet().                          // FileDataSet省内存，可加载大规模数据集
            setTokenizer(new HanLPTokenizer()).                               // 支持不同的ITokenizer，详见源码中的文档
            load(CORPUS_FOLDER, ""UTF-8"", 0.9);               // 前90%作为训练集
        IClassifier classifier = new NaiveBayesClassifier();
        classifier.train(trainingCorpus);
        IDataSet testingCorpus = new MemoryDataSet(classifier.getModel()).
            load(CORPUS_FOLDER, ""UTF-8"", -0.1);        // 后10%作为测试集
        // 计算准确率
        FMeasure result = Evaluator.evaluate(classifier, testingCorpus);
        System.out.println(result);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,365820594,动态更新完词典怎样写入文件CustomDictionary.txt.bin,open,,https://github.com/hankcs/HanLP/issues/988,"我是想动态更新词典用于现有的搜索，同时也写入文件，并重建CustomDictionary.txt.bin
这样就不用重启solr服务器，并重启solr服务器以后，也不会出问题。
找了里面的代码，但是没有找到， 是不是有其他办法可以实现此功能？"
HanLP,hankcs,365819183,词典不能用空格,open,,https://github.com/hankcs/HanLP/issues/987,"删除CustomDictionary.txt.bin以后，会重新生成CustomDictionary.txt.bin
但在生成CustomDictionary.txt.bin的时侯，出现如下错误：
`data/dictionary/custom/words-my.dic读取错误！java.lang.NumberFormatException: For input string: ""Jill""`
因为有一行记录为：
`Jack N' Jill`
不知道能不能用空格分隔自定义词典？或者有其他办法吗？"
HanLP,hankcs,365262059,希望能推出go版本,open,,https://github.com/hankcs/HanLP/issues/986,"希望能推出go版本
"
HanLP,hankcs,365218404,pyhanlp使用SafeJClass调用crf分词器意图多线程分词，报jvm错误,open,提问,https://github.com/hankcs/HanLP/issues/985,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：pyhanlp GitHub master
我使用的版本是：pyhanlp GitHub master


<!--以上属于必填项，以下可自由发挥-->

## 我的问题
      pyhanlp使用SafeJClass调用crf分词器意图多线程分词，报jvm错误


<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

class Divider(threading.Thread):
    def __init__(self, threadID, data):
        threading.Thread.__init__(self)
        self.threadID=threadID
        self.data=data
        CRFLexicalAnalyzer = SafeJClass(""com.hankcs.hanlp.model.crf.CRFLexicalAnalyzer"")
        #CustomDictionary = LazyLoadingJClass('com.hankcs.hanlp.dictionary.CustomDictionary')
        #HanLP = SafeJClass('com.hankcs.hanlp.HanLP')
        extend_dic = format_dic(external_dic_path)
        for word_e in extend_dic:
            CustomDictionary.add(word_e)  # 添加外部军事词典
        # 打开词性标注
        s = CRFLexicalAnalyzer().enableCustomDictionary(True).enablePartOfSpeechTagging(True)
        self.segment=s

    def run(self):
        divide_word(self.data,self.threadID,self.segment)


def divide_word(data,pid,segment,Debug=False):
    all_words=[]

    for index,article in enumerate(data):
        article_contont = article[""article_content""]
        article_contont = norm_article(article_contont)
        terms=segment.seg(article_contont)
        words=[term.word for term in terms]
        if Debug:
            print(words)
        all_words.append(words)
        if index%500==0:
            print(""Thread:"",pid,""divide...line:"",index)
    return all_words


    data=load_json(data_path)
    mid=int(len(data)*0.5)
    data1=data[:mid]
    data2=data[mid:]
    thread1 = Divider( ""Thread-1"", data1)
    thread2 = Divider( ""Thread-2"", data2)

    # 开启线程
    thread1.start()
    thread2.start()
    thread1.join()
    thread2.join()

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f7c4d07fbb0, pid=17521, tid=0x00007f7bc90ce700
#
# JRE version: OpenJDK Runtime Environment (8.0_181-b13) (build 1.8.0_181-8u181-b13-0ubuntu0.16.04.1-b13)
# Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [_jpype.cpython-36m-x86_64-linux-gnu.so+0x3cbb0]  JPJavaEnv::FindClass(char const*)+0x20
#
# Core dump written. Default location: /home/ygwang/workspace/hannlp/handler/core or core.17521
#
# An error report file with more information is saved as:
# /home/ygwang/workspace/hannlp/handler/hs_err_pid17521.log
[thread 140169611896576 also had an error]
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
Aborted (core dumped)



```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,364749331,hanlp打包问题,open,改进,https://github.com/hankcs/HanLP/issues/983,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

打包后找不到hanlp.properties

### 步骤

1. 首先:  下载hanlp源码(master分支),在com.hankcs.hanlp.HanLP类中加了一个main方法用于测试：
```
    public static void main(String[] args) {
        System.out.println(HanLP.segment(""商品和服务""));
    }
```
2. 然后:  打包，执行打包命令 `mvn clean package -Dmaven.test.skip=true`

3. 接着:  修改hanlp.properties中的root路径为data的父文件夹并放到target目录下，和hanlp-1.6.8-sources.jar、hanlp-1.6.8.jar在同一目录下，命令运行com.hankcs.hanlp.HanLP类的main方法:
```
java -cp hanlp-1.6.8.jar com.hankcs.hanlp.HanLP start
```

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[商品/n, 和/cc, 服务/vn]
```

期望和hanlp-1.6.8-release.zip里面打包出来的效果一样，两个jar包一个配置文件。

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
sep 28, 2018 1:47:03 PM com.hankcs.hanlp.HanLP$Config <clinit>
SEVERE: 没有找到hanlp.properties，可能会导致找不到data
========Tips========
请将hanlp.properties放在下列目录：
Web项目则请放到下列目录：
Webapp/WEB-INF/lib
Webapp/WEB-INF/classes
Appserver/lib
JRE/lib
并且编辑root=PARENT/path/to/your/data
现在HanLP将尝试从/Users/pan/github/HanLP-master/target读取data……
Sep 28, 2018 1:47:03 PM com.hankcs.hanlp.corpus.io.IOUtil readBytes
WARNING: 读取data/dictionary/CoreNatureDictionary.txt.bin时发生异常java.io.FileNotFoundException: data/dictionary/CoreNatureDictionary.txt.bin (No such file or directory)
Sep 28, 2018 1:47:03 PM com.hankcs.hanlp.dictionary.CoreDictionary load
WARNING: 核心词典data/dictionary/CoreNatureDictionary.txt不存在！java.io.FileNotFoundException: data/dictionary/CoreNatureDictionary.txt (No such file or directory)
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at com.hankcs.hanlp.seg.common.Vertex.newB(Vertex.java:455)
	at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:73)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:573)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
	at com.hankcs.hanlp.HanLP.segment(HanLP.java:626)
	at com.hankcs.hanlp.HanLP.main(HanLP.java:860)
Caused by: java.lang.IllegalArgumentException: 核心词典data/dictionary/CoreNatureDictionary.txt加载失败
	at com.hankcs.hanlp.dictionary.CoreDictionary.<clinit>(CoreDictionary.java:44)
	... 7 more
```


## 其他信息


"
HanLP,hankcs,364504744,关于标准分词的详细原理,open,,https://github.com/hankcs/HanLP/issues/981,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我现在想了解标准词性标注的原理，我知道，是用隐马尔科夫和比特算法实现，但我想知道的更具体些。
还有，转移矩阵是怎么得到的？是共现吗？
期望尽快能给答复，非常感谢了
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,364448891,是否能提供1.6.8的原始语料库,open,重复,https://github.com/hankcs/HanLP/issues/980,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

非常感谢作者无私的贡献了1.6.8版本，提供了1亿语料库训练的模型。目前项目组正在使用1.6.8版本提供的模型，对于常规的分词和词性识别效果基本达到预期，但是对于行业内的分词效果并不理想，请问是否能提供1.6.8的语料库，想基于这个语料库上增加行业内的语料，使其能满足行业内的分词需求。

"
HanLP,hankcs,364284338,请问1.6.8中大规模语料训练的感知机模型的命名实体识别ner.bin会提供嘛?,open,提问,https://github.com/hankcs/HanLP/issues/979,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：**1.6.8**
我使用的版本是：**1.6.8**


## 我的问题

我发现**data-for-1.6.8**数据包中data/model/perceptron/large目录下只有分词模型cws.bin,并未提供ner.bin,请问方便提供嘛?


"
HanLP,hankcs,363988478,n-gram关系由来,open,提问,https://github.com/hankcs/HanLP/issues/976,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
GitHub仓库版 master分支

当前最新版本号是： GitHub仓库版 master分支 
我使用的版本是： GitHub仓库版 master分支 

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我想咨询下hanlp的二元关系是怎么得到的？例如‘一@公益 1’，‘一@一对一 5’。我理解的二元关系是左元+右元能够是一个搭配之类的，但是这样的二元关系看起来有点怪，是不是我理解错了hanlp的二元关系？另外对于n-gram的搭配的‘出场顺序’也存在疑问，为什么整体上二元关系出现的频率都比较小，是跟语料有关系吗？烦请能够指点。非常感谢！
"
HanLP,hankcs,363489589,urllib.error.URLError: <urlopen error unknown url type: https>,open,,https://github.com/hankcs/HanLP/issues/974,"python 3.6
pyhanlp  0.1.44
在运行如下代码时，报错

from pyhanlp import *
text = """"
print(HanLP.segment(text))

error：File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/__init__.py"", line 116, in <module>
    _start_jvm_for_hanlp()
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/__init__.py"", line 38, in _start_jvm_for_hanlp
    from pyhanlp.static import STATIC_ROOT, hanlp_installed_data_version, HANLP_DATA_PATH
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 305, in <module>
    install_hanlp_jar()
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 191, in install_hanlp_jar
    version = hanlp_latest_version()[0]
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 68, in hanlp_latest_version
    return hanlp_releases()[0]
  File ""/usr/local/python3.6/lib/python3.6/site-packages/pyhanlp/static/__init__.py"", line 76, in hanlp_releases
    content = urllib.urlopen(""https://api.github.com/repos/hankcs/HanLP/releases"").read()
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 223, in urlopen
    return opener.open(url, data, timeout)
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 526, in open
    response = self._open(req, data)
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 549, in _open
    'unknown_open', req)
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 504, in _call_chain
    result = func(*args)
  File ""/usr/local/python3.6/lib/python3.6/urllib/request.py"", line 1388, in unknown_open
    raise URLError('unknown url type: %s' % type)
urllib.error.URLError: <urlopen error unknown url type: https>

"
HanLP,hankcs,360857594,hanlp-portable-1.6.8.jar包中的CustomDictionary.txt.bin只有3802KB，但是自己根据默认配置产生有16583KB,open,提问,https://github.com/hankcs/HanLP/issues/967,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题


我想添加一点自己的词到\data\dictionary\custom目录下，然后产生bin文件

然后放到hanlp-portable-1.6.8.jar中覆盖CustomDictionary.txt.bin

但是为了保证jar包大小不要增加太多

所以想基于hanlp-portable-1.6.8.jar默认选的几个词典文件,但是怎么尝试都得不到3802KB

所以请教hanlp-portable-1.6.8.jar产生的CustomDictionary.txt.bin是基于哪几个txt文件吗？

"
HanLP,hankcs,356718238,[提问]自建某个行业的语料及遇到的分词不准问题 ,open,提问,https://github.com/hankcs/HanLP/issues/957,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题


我想咨询一下楼主，如果想自建某个行业的语料，我需要哪些操作步骤？
我现在的操作是：

1. 先对行业数据进行标准分词（标准分词后，会自动生成词性）
2. 对关键词进行自定义词性标注的检查和修正
3. 使用PerceptronLexicalAnalyzer（感知机词法分析器）进行分词和实体识别
4. 规则审核

但是在训练分好词和标注好词性的语料数据时（按照199801的语料分词标准准备的数据），有些分词和词性识别并不是很准还有些词性会被覆盖。
1. 问题1：这个时候是不是需要使用PerceptronLexicalAnalyzer.learn方法来进行学习修正，还是我上述的操作本身是有问题的？
2. 问题2：在进行语料标注时，标准词性和自定义复合词性对于上下文是否有要求？如果有要求，如何界定上下文的边界？
3. 问题3：语料训练时候，是需要将整片文章进行训练，还是只对需要提取内容的部分段落进行训练？
4. 问题4：实体识别时，如果一个实体存在不同词性时，是否需要根据上下文分词来判断当前实体的具体词性？如何设置上下文的窗口大小？


## 复现问题


### 步骤

1. 我先训练自己的语料(一份进行标准分词的合同正文)，在进行标准分词后，我对语料进行了词性修改（标注为自定义词性），然后将整篇文章进行训练
```
客户信息/khxx ：/w 
客户方/khfbq ：/w 致富银行股份有限公司记账中心/khfname
[发/v 票/n 内容/n]/fpxx
名称/fpxxmcbq ：/w 致富科技有限公司记账中心122号/fpxxmc
[发票/n 抬头/vi]/fptt ：/w 致富科技有限公司记账中心/fpttname
.......
```

2. 然后调用HanLP的API训练上面的语料
```
               //1、分词
		CWSTrainer();
		//2、词性标注
		POSTrainer();
		//3、实体识别
		NERTrainer();
```
3. 训练完成后，生成自定义的cws、pos、ner.bin和bin.txt文件，然后使用感知机分词进行生产数据的分词
```
   PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer(自定义_CWS_MODEL_FILE, 
   自定义_POS_MODEL_FILE,自定义_NER_MODEL_FILE);
   String testInfoTxt = FileUtils.readTxt(""D:\\testInfo.txt"");
   List<List<Term>> list = analyzer.seg2sentence(testInfoTxt );
   
```

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
--->[客户信息/khxx, ：/w]
--->[客户方/khfbq, ：/w, 致富科技有限公司记账中心/khfname]
--->[发票内容/fpxx]
--->[名称/fpxxmcbq, ：/w, 致富科技有限公司记账中心122号/fpxxmc]
--->[发票抬头：/fptt, 致富科技有限公司记账中心/fpttname]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
--->[客户信息/khxx, ：/w]
--->[客户/n]
--->[方/q, ：/w, 致富科技有限公司记账中心/khfname]
--->[发票内容/fpxx]
--->[名称/n, ：/w, 致富科技有限公司记账中心122号/khfname]
--->[发票抬头：/fptt, 致富科技有限公司记账中心/khfname]
```


## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,355676695,List<Term> 怎么转换成List<String>,open,提问,https://github.com/hankcs/HanLP/issues/951,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.8  master text 里面的com

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我想做两个短文本的相似性判断，仅仅根据关键词或者分词的相似性，相似的单词越多则两文本相似性更好。
因为HanLP很多分词函数返回是List<Term>，而通过DemoWordDistance文件判断两个单词相似性是String类型，所以我需要把Term转变成String.

因为代码报错，无法输出结果。后面详细问题并未填写。

Update Comment:感谢yaleimeng,问题得到解决，因为我测试用的是github的test里面下载的com大文件夹，而test里面的com/hankcs/hanlp/seg/common/下面没有Term.java，但是import 这个位置的Term没问题，好奇怪啊
/
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
  
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,355480551,"Exception in thread ""main"" java.lang.IllegalArgumentException: Illegal Capacity: -1",open,提问,https://github.com/hankcs/HanLP/issues/948,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.8
我使用的版本是：1.6.7

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

对“龙子湖高校园区15号河南农业大学”分词时崩溃

## 复现问题
对“龙子湖高校园区15号河南农业大学”分词时崩溃

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public static void main(String[] args){
        Segment seg = new DijkstraSegment();
        seg.seg(""龙子湖高校园区河南农业大学"");
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
不崩溃
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
崩溃了
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,354992252,solr中使用hanlp自定义词典相关问题,open,重复,https://github.com/hankcs/HanLP/issues/944,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

solr中使用自定义同义词词库，多个同义词时返回不完整

## 复现问题

我的同义词词典synonyms_department.txt, 每一行的内容如下：

`肾内科,肾脏科,肾病内科,肾病科,肾脏内科,肾内`

定义一个hanlp分词的type:
```
     <fieldType name=""hanlp_syn_department"" class=""solr.TextField"" positionIncrementGap=""100"">
          <analyzer type=""index"">
               <tokenizer class=""com.hankcs.lucene.HanLPTokenizerFactory"" enableIndexMode=""true"" enableNameRecognize=""true"" enableOrganizationRecognize=""true""/>
               <filter class=""solr.SynonymFilterFactory"" synonyms=""synonyms_department.txt"" ignoreCase=""true"" expand=""false"" />
          </analyzer>
          <analyzer type=""query"">
              <!-- 切记不要在query中开启index模式 -->     
              <tokenizer class=""com.hankcs.lucene.HanLPTokenizerFactory"" enableIndexMode=""false"" enableNameRecognize=""true"" enableOrganizationRecognize=""true""/>
          </analyzer>              
     </fieldType>
```
使用hanlp_syn_department类型的字段做analysis：
![screen shot 2018-08-29 at 1 05 08 pm](https://user-images.githubusercontent.com/38783332/44766503-479c5900-ab8c-11e8-8d6c-bfc22b62e21a.png)


相同的同义词词典，在ik中的效果：
![screen shot 2018-08-29 at 12 25 31 pm](https://user-images.githubusercontent.com/38783332/44765325-c4c4cf80-ab86-11e8-853a-718b20f8bab1.png)


其他issues关于同义词的配置也翻了还是没有找到原因，请指教问题出在哪里. "
HanLP,hankcs,354971000,请教下NLPTokenizer切分短网址问题,open,提问,https://github.com/hankcs/HanLP/issues/943,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
1.6.8

当前最新版本号是：
我使用的版本是：1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
集卡你最棒！家人和朋友越多，集齐瓜分奖金越多，本周集卡火热进行中，一起来 t.cn/RdHCuCh 退订回T
短网址前后有空格，切分的词是：一起来 t.cn/rdhcuch 退订/d,。
这个切分是不是有问题？我把空格去掉后切分是这样的
一起/d, 来t/v, ./v, cn/rdhcuch/nx, 退订/v
来t和.号切分成动词，怎么会这样样切分呢？

有没有补救的办法

## 复现问题
System.out.println(NLPTokenizer
            .segment(""集卡你最棒！家人和朋友越多，集齐瓜分奖金越多，本周集卡火热进行中，一起来 t.cn/RdHCuCh 退订回T""));

System.out.println(NLPTokenizer
            .segment(""集卡你最棒！家人和朋友越多，集齐瓜分奖金越多，本周集卡火热进行中，一起来 t.cn/RdHCuCh退订回T""));

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,354650412,请教一下关于分词的问题,open,提问,https://github.com/hankcs/HanLP/issues/942,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

1.6.8

## 我的问题

我想取出如:""北京市人民医院"" 这样一个词,但是总是会分成 ""北京市"" + ""人民医院"" 两个词,请问有什么好的解决方式吗? (地名也需要取出来)

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,354157907,data-for-1.6.8.zip资源文件自定义词典中文乱码,open,提问,https://github.com/hankcs/HanLP/issues/936,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.6.8
我使用的版本是：portable-1.6.8

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在windows下，解压缩资源包，文件名都显示正常；在ubuntu系统解压缩，data/dictionary/custom/目录下，中文名字的字典文件出现乱码。



<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,353966896,fail to set relative path in hanlp.properties,open,提问,https://github.com/hankcs/HanLP/issues/935,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.6.7
我使用的版本是：1.6.7

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

设置hanlp.properties中root为相对路径无效
参考 https://github.com/hankcs/HanLP/pull/254
尝试不同的relative path设置方式都失败了，以下为其中一种举例

## 复现问题

### hanlp.properties:
#本配置文件中的路径的根目录，根目录+其他路径=完整路径（支持相对路径，请参考：https://github.com/hankcs/HanLP/pull/254）
#Windows用户请注意，路径分隔符统一使用/
root=../HanLP/

### path
resources
\- HanLP
    \- data
        \- ...
   hanlp.properties

### report:
首次编译运行时，HanLP会自动构建词典缓存，请稍候……
Aug 24, 2018 5:37:50 PM com.hankcs.hanlp.corpus.io.IOUtil readBytes
WARNING: 读取../HanLP/data/dictionary/CoreNatureDictionary.txt.bin时发生异常java.io.FileNotFoundException: ../HanLP/data/dictionary/CoreNatureDictionary.txt.bin (No such file or directory)
Aug 24, 2018 5:37:50 PM com.hankcs.hanlp.dictionary.CoreDictionary load
WARNING: 核心词典../HanLP/data/dictionary/CoreNatureDictionary.txt不存在！java.io.FileNotFoundException: ../HanLP/data/dictionary/CoreNatureDictionary.txt (No such file or directory)

java.lang.ExceptionInInitializerError
	at com.hankcs.hanlp.seg.common.Vertex.newB(Vertex.java:455)
	at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:73)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:573)
	at com.hankcs.hanlp.tokenizer.StandardTokenizer.segment(StandardTokenizer.java:50)
	at com.hankcs.hanlp.HanLP.segment(HanLP.java:626)
	at HanLPMain.test(HanLPMain.java:37)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:124)
	at org.testng.internal.Invoker.invokeMethod(Invoker.java:580)
	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:716)
	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:988)
	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:125)
	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109)
	at org.testng.TestRunner.privateRun(TestRunner.java:648)
	at org.testng.TestRunner.run(TestRunner.java:505)
	at org.testng.SuiteRunner.runTest(SuiteRunner.java:455)
	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:450)
	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:415)
	at org.testng.SuiteRunner.run(SuiteRunner.java:364)
	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52)
	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:84)
	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1208)
	at org.testng.TestNG.runSuitesLocally(TestNG.java:1137)
	at org.testng.TestNG.runSuites(TestNG.java:1049)
	at org.testng.TestNG.run(TestNG.java:1017)
	at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72)
	at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:123)
Caused by: java.lang.IllegalArgumentException: 核心词典../HanLP/data/dictionary/CoreNatureDictionary.txt加载失败
	at com.hankcs.hanlp.dictionary.CoreDictionary.<clinit>(CoreDictionary.java:44)
	... 31 more"
HanLP,hankcs,351500412,CRFNERecognizer用pku1998年1-6月份数据训练，java.lang.OutOfMemoryError：GC overhead limit exceeded,open,讨论,https://github.com/hankcs/HanLP/issues/925,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v1.6.7
我使用的版本是：v1.6.7

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
CRFNERecognizer用pku1998年1-6月份数据(64.6M)训练内存溢出:java.lang.OutOfMemoryError：GC overhead limit exceeded
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public static void main(String[] args) throws IOException {
	String CORPUS = ""data/test/pku98/1998-2.txt"";
	String NER_MODEL_PATH = ""data/model/crf/pku1998-2/ner.txt"";
	CRFTagger tagger = new CRFNERecognizer(null);
        tagger.train(CORPUS, NER_MODEL_PATH);
	}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,351474215,同义词的词典是怎么训练的？,open,重复,https://github.com/hankcs/HanLP/issues/924,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：V1.6.7
我使用的版本是：V1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
同义词的词典是怎么训练的？
dictionary\synonym\CoreSynonym.txt怎么添加新的行？

"
HanLP,hankcs,351433517,去除停用词，提取有用的关键词,open,提问,https://github.com/hankcs/HanLP/issues/923,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.6.7.jar
我使用的版本是：hanlp-1.6.7.jar

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我的问题在于提取的关键词会有一些不需要的词
比如：我根据一个电器公司的简介和经营产品，那么提取的关键词个人感觉应该都跟电器相关的
但是会有几个不重要的词会提取出来，请问怎么解决，有什么意见，请告诉我，谢谢大佬。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码
![image](https://user-images.githubusercontent.com/32606897/44244108-d7ddb400-a204-11e8-8ebd-f6893840cd47.png)
```
  private static List getKeyWordRank(String path) throws IOException {
        String t1 = new String();
        BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(path), ""utf-8""));
        String str;
        while ((str = reader.readLine()) != null) {
            t1 += str;
        }
        List<String> list = HanLP.extractKeyword(t1, 10);
        for (String item:list) {
            System.out.println(item);
        }
        return list;
    }


```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出  [电视, 电脑, 电器, 空调, 电冰箱, 电子, 微波炉, 吸尘器, 消毒柜, 厨房]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出 [电视, 电脑, 电器, 空调, 海尔, 电子, 网点, 营业额, 白电, 厨房]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,350365016,请问怎么训练依存句法模型,open,提问,https://github.com/hankcs/HanLP/issues/918,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：V1.6.7
我使用的版本是：V1.6.6

<!--以上属于必填项，以下可自由发挥-->

请问怎么训练依存句法模型？
有这样的示例吗？
"
HanLP,hankcs,348205035,solr分词与在线分词有差别的原因？,open,重复,https://github.com/hankcs/HanLP/issues/912,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-portable-1.5.3
我使用的版本是：hanlp-portable-1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在solr配置了最新的jar包进行分词与http://hanlp.hankcs.com上的在线演示结果不同，主要原因是什么？
例如：夜圣乔治一级葡萄园
在线分词“夜圣乔治”，“一级”，“葡萄园”
solr分词“夜”，”圣”，”乔”，”治”，“一级”，“葡萄园”

"
HanLP,hankcs,347841768,按照词性优先级配置,open,提问,https://github.com/hankcs/HanLP/issues/911,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：master
我使用的版本是：master


## 我的问题
目前分词的词优先级好像是根据配置的顺序的
请问是否支持按照词性优先级输出分词结果？比如说我这边定义了两个词：
和服 n1
服装 n2
然后分词 日本和服装
但需要根据不同场景我可能需要分别输出和服或者服装，这时候我就需要配置一个优先级来控制输出（n1和n2的输出优先级）"
HanLP,hankcs,346516300,pyhanlp如何重写Filter过滤器,open,提问,https://github.com/hankcs/HanLP/issues/908,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.6
我使用的版本是：1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
pyhanlp如何重写Filter过滤器

<!-- 请详细描述问题，越详细越可能得到解决 -->
正在使用pyhanlp，有一个需求是需要在使用corestopword后，分词结果保留数字。按照现在java版本的逻辑是需要重写Filter，自己不太知道如何在pyhanlp里面重写过滤器

"
HanLP,hankcs,346104402,提取新词的几点疑问,open,讨论,https://github.com/hankcs/HanLP/issues/905,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

1.在com.hankcs.hanlp.mining.word.WordInfo.computeProbabilityEntropy()计算左右熵的时候,如果该词位于首或者尾部时取最小值 这样取出的熵值为0,当设置默认熵值,开头结尾的词将无法取出
2.互信息
代码中给出的算法
![1](https://user-images.githubusercontent.com/36916036/43451100-c14b4b42-94e6-11e8-923a-912d6e534bd0.png)

![12379570-7c7355505813d673](https://user-images.githubusercontent.com/36916036/43620279-765a49ce-9704-11e8-80fa-7515e1a37ab4.png)
不是完全一致




"
HanLP,hankcs,344737458,在一个工程里面同时使用多份自定义词典,open,求助,https://github.com/hankcs/HanLP/issues/898,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：4.0.0
我使用的版本是：4.0.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
能不能在一个工程里面同时使用多份自定义词典，例如我有两个业务场景，场景一需要自定义词典1，场景二需要自定义词典2

"
HanLP,hankcs,341956020,短语提取为什么要去停用词,open,提问,https://github.com/hankcs/HanLP/issues/890,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
当前最新版本号是：1.6.6
我使用的版本是：1.6.6

短语提取为什么要去停用词，如果不去停用词

这是一篇文章：

欢迎投稿 文章来源 作者 对新手来说 学习机器学习和深度学习是比较困难的 各种深度学习库也是比较难理解 所以 我创建了这个机器学习和深度学习速查表 希望对多家有帮助 热门文章推荐 马化腾 马云和李彦宏都错了 场景比数据和技术都重要 最新 谷歌董事长 我可以直接告诉你 互联网很快消失 斯坦福 卷积神经网络视觉识别课程讲义 独家 面对人工智能 下个像柯洁一样哭泣的可能就是你 最新 扎克伯格 如何在被抛弃和被否定中成就自己 震惊 禁止中国人参加 的第二轮比赛 重磅 揭秘 版本的技术设计和棋艺水平 重磅 谁让英伟达一夜损失 亿还留下一道思考题

短语提取后：
[机器学习深度, 深度学习, 卷积神经网络, 学习速查表, 李彦宏错, 柯洁哭泣, 神经网络视觉, 马云李彦宏, 一道思考题, 英伟达损失]

我不知道短语提取用的分词时哪种方法，但是我用segement分词后结果为

[欢迎/v, 投稿/vi, /w, 文章/n, 来源/n, /w, 作者/nnt, /w, 对/p, 新手/n, 来说/uls, /w, 学习/v, 机器学习/gi, 和/cc, 深度/n, 学习/v, 是/vshi, 比较/d, 困难/an, 的/ude1, /w, 各种/rz, 深度/n, 学习/v, 库/n, 也/d, 是/vshi, 比较/d, 难/a, 理解/v, /w, 所以/c, /w, 我/rr, 创建/v, 了/ule, 这个/rz, 机器学习/gi, 和/cc, 深度/n, 学习/v, 速查表/n, /w, 希望/v, 对/p, 多/a, 家/q, 有/vyou, 帮助/v, /w, 热门/a, 文章/n, 推荐/v, /w, 马化腾/nr, /w, 马云/nr, 和/cc, 李彦宏/nr, 都/d, 错/v, 了/ule, /w, 场景/n, 比/p, 数据/n, 和/cc, 技术/n, 都/d, 重要/a, /w, 最新/a, /w, 谷歌/ntc, 董事长/nnt, /w, 我/rr, 可以/v, 直接/ad, 告诉/v, 你/rr, /w, 互联网/n, 很快/d, 消失/vi, /w, 斯坦福/nrf, /w, 卷积/gm, 神经网络/nz, 视觉/n, 识别/vn, 课程/n, 讲义/n, /w, 独家/d, /w, 面对/v, 人工智能/n, /w, 下/f, 个/q, 像/v, 柯洁/nr, 一样/uyy, 哭泣/vi, 的/ude1, 可能/v, 就是/v, 你/rr, /w, 最新/a, /w, 扎克伯格/nrf, /w, 如何/ryv, 在/p, 被/pbei, 抛弃/v, 和/cc, 被/pbei, 否定/v, 中/f, 成就/n, 自己/rr, /w, 震惊/v, /w, 禁止/v, 中国/ns, 人/n, 参加/v, /w, 的/ude1, 第二/mq, 轮/qv, 比赛/vn, /w, 重磅/n, /w, 揭秘/v, /w, 版本/n, 的/ude1, 技术/n, 设计/vn, 和/cc, 棋艺/n, 水平/n, /w, 重磅/n, /w, 谁/ry, 让/v, 英伟达/nz, 一/m, 夜/t, 损失/n, /w, 亿/m, 还/d, 留下/v, 一道/d, 思考题/n]

“英伟达一夜损失”，”一”是去停用词去掉了，“夜”不知道怎么去掉的(这不是重点)，然后短语就是英伟达损失，如果说不去停用词的话，假设短语提取后不会出现“英伟达一夜”或者“一夜损失”或者“英伟达一夜损失"",因为我是希望用提取出的短语当做文章的关键词，所以后三个短语正好也是我不想要的，（之后我也想用短语当做字典重新对文章分词，然后进行后面的实验）
"
HanLP,hankcs,340126344,识别类似“王寒炮轰张冰冰”结构的句子时结果与预期不同。,open,讨论,https://github.com/hankcs/HanLP/issues/884,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.6
我使用的版本是：1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在对“王寒炮轰张冰冰”的类似的句子分词时，nlp，感知机和crf都会出现不同程度的误差现象。
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 分别对“王寒炮轰张冰冰”进行nlp,感知机分词和crf分词
2.查看三种方式的分词结果


### 触发代码

```
    public void testIssue1234() throws Exception
    {
       String testContent = ""王寒炮轰张冰冰"";
       //三种分词的方法为常规代码，暂省略
        nlpTokenizerTest(testContent);
        perceptronTest(testContent);
        crfTest(testContent);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
王寒/nr，炮轰/v，张冰冰/nr
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
nlp分词结果：[王寒炮轰/nr, 张冰冰/nr]
感知机分词结果：王寒炮轰张冰冰/nr
crf分词结果：[王寒炮/nr, 轰张冰冰/nr]
## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
![qq 20180711153707](https://user-images.githubusercontent.com/18030444/42557212-724bfa4e-8520-11e8-955d-8bd29747e7e1.png)
"
HanLP,hankcs,339704057,分号词性标注不准确，期待推出语义角色分析和指代消岐、初步实体和关系抽取等功能,open,重复,https://github.com/hankcs/HanLP/issues/882,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：发行版 1.6.6
我使用的版本是：发行版 1.6.6

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

分号词性标注不准确，期待推出语义角色和指代消岐等功能

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

输入以下句子进行词性标注：
“而变革者则提出解释理解世界的新方法：凯恩斯在对斯密进行修补；弗洛伊德另辟蹊径；毕加索挑战马蒂斯；爱因斯坦修订牛顿为大自然的立法；德鲁克对组织的研究，和他提出的“知识工人”与受雇阶层。”

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
       CoNLLSentence sentence =  HanLP.parseDependency(sentence0);

        CoNLLWord[] wordArray = sentence.getWordArray();
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

正确的词性标注

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
1	而	而	c	c	_	4	状中结构	_	_
2	变革者	变革者	n	n	_	4	主谓关系	_	_
3	则	则	d	d	_	4	状中结构	_	_
4	提出	提出	v	v	_	0	核心关系	_	_
5	解释	解释	v	v	_	10	定中关系	_	_
6	理解	理解	v	v	_	10	定中关系	_	_
7	世界	世界	n	n	_	6	动宾关系	_	_
8	的	的	u	u	_	6	右附加关系	_	_
9	新	新	a	a	_	10	定中关系	_	_
10	方法	方法	n	n	_	11	定中关系	_	_
11	：凯	：凯	nh	nr	_	12	定中关系	_	_
12	恩斯	恩斯	nh	nr	_	21	主谓关系	_	_
13	在	在	p	p	_	21	状中结构	_	_
14	对	对	p	p	_	16	状中结构	_	_
15	斯密	斯密	j	j	_	14	介宾关系	_	_
16	进行	进行	v	v	_	18	定中关系	_	_
17	修补	修补	v	vn	_	16	动宾关系	_	_
18	；	；	n	n	_	19	定中关系	_	_
19	弗洛伊德	弗洛伊德	nh	nr	_	13	介宾关系	_	_
20	另	另	d	d	_	21	状中结构	_	_
21	辟	辟	v	v	_	23	定中关系	_	_
22	蹊径	蹊径	n	n	_	21	动宾关系	_	_
23	；	；	v	v	_	25	主谓关系	_	_
24	毕加索	毕加索	Vg	Vg	_	25	主谓关系	_	_
25	挑战	挑战	v	v	_	4	动宾关系	_	_
26	马蒂斯	马蒂斯	nh	nr	_	27	定中关系	_	_
27	；爱因斯坦	；爱因斯坦	ns	ns	_	28	主谓关系	_	_
28	修订	修订	v	v	_	25	动宾关系	_	_
29	牛顿	牛顿	nh	nr	_	30	主谓关系	_	_
30	为	为	v	v	_	34	定中关系	_	_
31	大自然	大自然	n	n	_	30	动宾关系	_	_
32	的	的	u	u	_	30	右附加关系	_	_
33	立法	立法	v	vn	_	34	定中关系	_	_
34	；德鲁克	；德鲁克	n	n	_	38	定中关系	_	_
35	对	对	p	p	_	38	定中关系	_	_
36	组织	组织	v	v	_	38	定中关系	_	_
37	的	的	u	u	_	36	右附加关系	_	_
38	研究	研究	v	vn	_	28	动宾关系	_	_
39	，	，	wp	w	_	28	标点符号	_	_
40	和	和	c	c	_	42	左附加关系	_	_
41	他	他	r	r	_	42	主谓关系	_	_
42	提出	提出	v	v	_	46	定中关系	_	_
43	的	的	u	u	_	42	右附加关系	_	_
44	“	“	wp	w	_	46	标点符号	_	_
45	知识	知识	n	n	_	46	定中关系	_	_
46	工人	工人	n	n	_	28	并列关系	_	_
47	”	”	wp	w	_	46	标点符号	_	_
48	与	与	c	c	_	49	左附加关系	_	_
49	受雇	受雇	v	v	_	46	并列关系	_	_
50	阶层	阶层	n	n	_	49	动宾关系	_	_
51	。	。	wp	w	_	4	标点符号	_	_

而 --(状中结构)--> 提出
变革者 --(主谓关系)--> 提出
则 --(状中结构)--> 提出
提出 --(核心关系)--> ##核心##
解释 --(定中关系)--> 方法
理解 --(定中关系)--> 方法
世界 --(动宾关系)--> 理解
的 --(右附加关系)--> 理解
新 --(定中关系)--> 方法
方法 --(定中关系)--> ：凯
：凯 --(定中关系)--> 恩斯
恩斯 --(主谓关系)--> 辟
在 --(状中结构)--> 辟
对 --(状中结构)--> 进行
斯密 --(介宾关系)--> 对
进行 --(定中关系)--> ；
修补 --(动宾关系)--> 进行
； --(定中关系)--> 弗洛伊德
弗洛伊德 --(介宾关系)--> 在
另 --(状中结构)--> 辟
辟 --(定中关系)--> ；
蹊径 --(动宾关系)--> 辟
； --(主谓关系)--> 挑战
毕加索 --(主谓关系)--> 挑战
挑战 --(动宾关系)--> 提出
马蒂斯 --(定中关系)--> ；爱因斯坦
；爱因斯坦 --(主谓关系)--> 修订
修订 --(动宾关系)--> 挑战
牛顿 --(主谓关系)--> 为
为 --(定中关系)--> ；德鲁克
大自然 --(动宾关系)--> 为
的 --(右附加关系)--> 为
立法 --(定中关系)--> ；德鲁克
；德鲁克 --(定中关系)--> 研究
对 --(定中关系)--> 研究
组织 --(定中关系)--> 研究
的 --(右附加关系)--> 组织
研究 --(动宾关系)--> 修订
， --(标点符号)--> 修订
和 --(左附加关系)--> 提出
他 --(主谓关系)--> 提出
提出 --(定中关系)--> 工人
的 --(右附加关系)--> 提出
“ --(标点符号)--> 工人
知识 --(定中关系)--> 工人
工人 --(并列关系)--> 修订
” --(标点符号)--> 工人
与 --(左附加关系)--> 受雇
受雇 --(并列关系)--> 工人
阶层 --(动宾关系)--> 受雇
。 --(标点符号)--> 提出

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

谢谢！！"
HanLP,hankcs,339293865,自定义词典的最长匹配,open,无效,https://github.com/hankcs/HanLP/issues/879,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.6
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
Hanlp的自定义词典功能，当自定义词典中的词较多时，可能会出现较长字符包含较短字符的情况，比如自定义词典中可能同时含有“增”以及“急增""两个词条，那么在对“盈利急增25亿元”进行分词时，会分出“盈利/vn，急/v，增/v，25/m，亿元/q”的结果，我想这可能和Trie树以及自动机的算法有关，现在我想实现自定义词条的最大匹配，我看到CustomDictionary类中有parseLongestText的方法，不过不知道其参数的含义和用法，不知道这个方法能否实现自定义词典最大匹配的功能。或者Hanlp的内部有没有实现类似功能的接口呢？如果没有的话能否指点下实现分词功能的相关代码的位置？谢谢
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
### 步骤

1. 首先在自定义词典中添加“增”和“急增”两个词条
2. 然后删除原来的bin文件，重新训练缓存文件
3. 接着利用N最短路分词，开启了自定义分词，地名识别，机构识别，并开启了自定义词典强制匹配
4.对“盈利急增25亿元”进行分词，会分出“盈利/vn，急/v，增/v，25/m，亿元/q”的结果

### 触发代码

```
    public void testIssue1234() throws Exception
    {
           String str = ""盈利急增25亿元"";          
           Segment nShortSegment = new 
           NShortSegment().enableCustomDictionary(true).enablePlaceRecognize(true)
				.enableOrganizationRecognize(true).enableOffset(true);
           nShortSegment.enableCustomDictionaryForcing(true);
           List<Term> termList = nShortSegment.seg(str);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
盈利 急增 25 亿元
### 实际输出
<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
盈利 急 增 25 亿元
没有实现自定义词典的最大匹配

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,335511512,希望線上演示的浮動說明能夠增加顯示該詞性的中文說明,open,提问,https://github.com/hankcs/HanLP/issues/869,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

http://hanlp.hankcs.com/

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

希望線上演示的浮動說明能夠增加顯示該詞性的中文說明(記性太差容易忘記英文詞性代表意思)

順便問下 線上演示的倉庫是哪一個 lol

似乎不在此倉庫內

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
""謎样""
n
名词
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

![2018-06-26-02-01-59-2](https://user-images.githubusercontent.com/167966/41867392-9b8e543e-78e5-11e8-99ad-dcc47b7edfb0.png)


"
HanLP,hankcs,334314614,JDK9 支持,open,讨论,https://github.com/hankcs/HanLP/issues/866,"在 `EnumBuster` 中用到了包 `sun.reflect`，在 JDK9 及 以后的 JDK 版本中，已经变成了 `jdk.internal.reflect`  。可以考虑另外实现，避免内部包的引入

<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

"
HanLP,hankcs,329270428,使用感知机识别机构实体，在线学习有的时候无效,open,重复,https://github.com/hankcs/HanLP/issues/856,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

使用感知机识别机构实体，在线学习有的时候无效，不知道什么原因，标注不对吗？

### 触发代码

```
        String s = ""就读于家乡江苏省镇江市省立师范学校初中二年级"";
        PerceptronLexicalAnalyzer analyzer = new PerceptronLexicalAnalyzer();
        System.out.println(analyzer.learn(""[江苏省/ns 镇江市/ns 省立师范学校/n]/nt""));
        System.out.println(analyzer.seg(s));
```
### 期望输出
```
true
[就读/v, 于/p, 家乡/n, 江苏省镇江市省立师范学校/nt, 初中/n, 二/m, 年级/n]
```

### 实际输出
```
true
[就读/v, 于/p, 家乡/n, 江苏省/ns, 镇江市/ns, 省立/v, 师范学校/l, 初中/n, 二/m, 年级/n]
```

## 其他信息


"
HanLP,hankcs,328947267,感知机机构识别，如何对错误识别进行干预？,open,提问,https://github.com/hankcs/HanLP/issues/854,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

使用感知机进行机构识别，总体比较准确，但是很多识别到的机构是错误的，需要排除，如何利用在线学习进行人工干预呢？
"
HanLP,hankcs,328754496,一个代码的小问题,open,改进,https://github.com/hankcs/HanLP/issues/853,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

在阅读 `DoubleArrayTrie` 源码的时候在第393行（`build` 方法)内发现一个很小的问题：

```
    if (_keySize > _key.size() || _key == null)
            return 0;
```
 这里的 `_key` 是先被使用然后再判断是否为空，当 `_key` 为空时，是没法执行到 `_key == null` 这一条件的吧


## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤



### 触发代码


### 期望输出



### 实际输出



## 其他信息

"
HanLP,hankcs,327600448,CRF分词，数词和标点符号都被识别成nz,open,提问,https://github.com/hankcs/HanLP/issues/849,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4 
我使用的版本是：1.6.4 jar版本 master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
```
        String name1 = ""您好,帮我看一下电影院几点开门"";
        System.out.println(""标准分词：""+ HanLP.segment(name1));
        final Segment segment1 = new CRFSegment();
        List<Term> termList1 = segment1.seg(name1);
        System.out.println(""CRFSegment:""+termList1);
```
log:
标准分词：[您好/n, ,/w, 帮/v, 我/rr, 看/v, 一下/m, 电影院/nis, 几/d, 点/qt, 开门/vi]
CRFSegment:[您好/n, ,/nz, 帮/v, 我/rr, 看一下/nz, 电影院/nis, 几点/nz, 开门/vi]

-- “看一下”和“几点”，以及标点符号都被识别成nz，有办法解决这个问题吗？


"
HanLP,hankcs,327311595,使用自定义的标签为每个字进行标注，除了重新定义模型文件，还需要其他什么工作？,open,提问,https://github.com/hankcs/HanLP/issues/848,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

## 我的问题

使用自定义的标签为每个字进行标注，除了重新定义模型文件，还需要其他什么工作？比如我想得到如下的结果：
患    Bx
者    Ix
，	D
56	Ba
岁	Ia
，    D
血	Bi
压	Ii
不	Bis
稳	Iis
定	Iis

这个时候除要定义新的模型文件之外，代码部分还需要修改哪些地方？期待回复，谢谢！

"
HanLP,hankcs,326803382,感知机分词器对分词结果的字符正则化问题,open,改进,https://github.com/hankcs/HanLP/issues/844,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
`AbstractLexicalAnalyzer.analyze()`方法，当没有加载ner.bin模型（`neRecognizer == null`）时，返回的分词结果是字符正则化后的形式，与输入原文不一致。

从代码中的行为来看，`AbstractLexicalAnalyzer.analyze()`中的分词过程会首先进行字符正则化，`segmenter`、`posTagger`和`neRecognizer`直接处理的都是经字符正则化的数据。

但是在输出分词结果的时候，只有执行了`neRecognizer`的逻辑分支里将未正则化的词写回分词结果数组：

```
// AbstractLexicalAnalyzer.java Line 190
// if (neRecognizer != null) 分支
String[] nerArray = neRecognizer.recognize(wordArray, posArray);
wordList.toArray(wordArray);
```

其他情况的返回值（Line 219 - Line 233）下都没有执行这个操作，返回的wordArray中的词是被字符正则化处理后的形式。


## 复现问题

### 触发代码

```
    public static void main(String[] args) throws Exception {
        // 加载 nerModel 的实例
        PerceptronLexicalAnalyzer analyzer1 = new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                HanLP.Config.PerceptronPOSModelPath, HanLP.Config.PerceptronNERModelPath);
        // 未加载 nerModel 的实例
        PerceptronLexicalAnalyzer  analyzer2 = new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                HanLP.Config.PerceptronPOSModelPath);

        // 输出分词结果
        System.out.println(analyzer1.analyze(""上海自来水来自海上，黄山落叶松叶落山黄。""));
        System.out.println(analyzer2.analyze(""上海自来水来自海上，黄山落叶松叶落山黄。""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
上海/ns 自来水/n 来自/v 海上/s ，/w 黄山/ns 落叶/n 松叶/n 落/v 山黄/n 。/w
上海/ns 自来水/n 来自/v 海上/s ，/w 黄山/ns 落叶/n 松叶/n 落/v 山黄/n 。/w
```

### 实际输出

第二行的逗号是正则化后的，没有转换回来。

```
上海/ns 自来水/n 来自/v 海上/s ，/w 黄山/ns 落叶/n 松叶/n 落/v 山黄/n 。/w
上海/ns 自来水/n 来自/v 海上/s ,/w 黄山/ns 落叶/n 松叶/n 落/v 山黄/n 。/w
```
"
HanLP,hankcs,325615755,"分词结果中名词性语素的词性标识“Ng”同Nature枚举类中的标识“ng""不一致",open,改进,https://github.com/hankcs/HanLP/issues/838,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

使用感知机分词器（`PerceptronLexicalAnalyzer`）的分词结果中，名词性语素的词性标识输出是""Ng""，但是Nature枚举类中对应的是Nature.ng，对应字符串""ng""。这会输出一行警告：
```
五月 23, 2018 5:06:34 下午 com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
警告: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!
如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类
```

## 复现问题
```
String s = ""个人公积金如何办理？"";
Segment segment = new PerceptronLexicalAnalyzer();
segment.seg(s);
```

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
个人/n 公积/n 金/ng 如何/r 办理/v ？/w
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
个人/n 公积/n 金/Ng 如何/r 办理/v ？/w
```

## 其他信息

Nature.ng -> 名词性语素

"
HanLP,hankcs,325141422,繁体简体转换词典问题,open,改进,https://github.com/hankcs/HanLP/issues/835,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.4
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
dictionary/other/CharTable.txt 中有一行
橙=橘

由此产生的问题就是如果开启了Normalization=true （我使用的场景中开启这个参数的主要目的是大小写统一）
“橙子”分词以后会产生的结果是“橘子”，但是这两种水果并不是相同的水果。

另外 dictionary/tc/t2s.txt 中有一个词
橙=橘子
不太确定这个词典的用途是不是繁体到简体的转换对应，如果是的话，感觉不太对，“橙”并不等于简体的“橘子”

## 复现问题


### 触发代码
配置文件中加入：Normalization=true

HanLP.segment(""橙子"")

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

橙子


### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

橘子


## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,324280184,word2vec 训练工具参数文档描述不准确,open,改进,https://github.com/hankcs/HanLP/issues/833,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

项目中提供的word2vec 训练工具，文档中关于-hs 和-cbow 的参数实际上必须要有数值参数（1或其它），否则运行时会报异常：
Exception in thread ""main"" java.lang.IllegalArgumentException: Argument missing for -cbow
wiki 中关于训练工具的参数Examples上也有同样的问题。
看了一下代码AbstractTrainer.java  中的setConfig函数中解析参数，-cbow 后面必须跟上1才会使用 cbow 模型。
if ((i = argPos(""-cbow"", args)) >= 0) config.setUseContinuousBagOfWords(Integer.parseInt(args[i + 1]) == 1);


## 复现问题
java -cp  hanlp-1.6.3.jar com.hankcs.hanlp.mining.word2vec.Train -input input.txt -output output.txt  -cbow

### 期望输出
开始训练

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
Exception in thread ""main"" java.lang.IllegalArgumentException: Argument missing for -cbow
	at com.hankcs.hanlp.mining.word2vec.AbstractTrainer.argPos(AbstractTrainer.java:50)
	at com.hankcs.hanlp.mining.word2vec.AbstractTrainer.argPos(AbstractTrainer.java:40)
	at com.hankcs.hanlp.mining.word2vec.AbstractTrainer.setConfig(AbstractTrainer.java:62)
	at com.hankcs.hanlp.mining.word2vec.Train.execute(Train.java:24)
	at com.hankcs.hanlp.mining.word2vec.Train.main(Train.java:38)
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,323571925,wiki 中word2vec章节提供的维基百科的预训练数据的训练参数,open,提问,https://github.com/hankcs/HanLP/issues/829,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

请问wiki 中word2vec章节提供的 维基百科的[预训练数据](https://pan.baidu.com/s/1qYFozrY) 的训练参数是什么？

"
HanLP,hankcs,323111087,新词发现里空字符串的问题,open,改进,https://github.com/hankcs/HanLP/issues/826,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

新词发现里这个[地方](https://github.com/hankcs/HanLP/blob/master/src/main/java/com/hankcs/hanlp/mining/word/NewWordDiscover.java#L64)附近有替换分割符为空串的步骤，在pyhanlp里得到的结果为 `哈哈\x00` 之类，这里是否应该是`""""`（空串，长度为0）而非`""\0""`（0号字符，长度为1）?

"
HanLP,hankcs,323090694,运行demo时发现提供的词典不完整,open,提问,https://github.com/hankcs/HanLP/issues/825,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：HanLP-1.6.3
我使用的版本是：HanLP-1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

在运行源码中的test类时发现有些词典是在data/test/目录下的，但是我下载下来的data包中不包含有这部分词典，请问在哪里可以获取到这些词典？



"
HanLP,hankcs,322576747,关于感知机在线学习的问题,open,提问,https://github.com/hankcs/HanLP/issues/822,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我使用了感知机的在线学习功能，但是没有达到我要的效果。请看一下下面代码
### 触发代码

                  PerceptronSegmenter segmenter = new
		  PerceptronSegmenter(Config.CRFCWSModelPath);
		  segmenter.learn(""下雨天 地面 积水"");
		  System.out.println(segmenter.segment(""下雨天地面积水""));
                  这样可以正常输出：[下雨天, 地面, 积水]
                 但是如果这样：
                 PerceptronLexicalAnalyzer segmenter1 = new
		  PerceptronLexicalAnalyzer(Config.CRFCWSModelPath,
		  Config.CRFPOSModelPath, Config.CRFNERModelPath);
		  Sentence sentence = segmenter1.analyze(""下雨天地面积水"");
		  System.out.println(sentence);
                  输出就错了：[下雨，天,地，面,积，水]
上面代码不是已经在线学习过了，理论上应该可以正常输出了，为什么输出还是错的？我看到你回复过有序列化到模型？请问这个应该怎么做？

"
HanLP,hankcs,319817378,依存句法分析：使用NeuralNetworkDependencyParser对相同词性结构句子得到不同依存句法分析结果,open,提问,https://github.com/hankcs/HanLP/issues/815,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
当前最新版本号是：portable-1.6.3
我使用的版本是：portable-1.6.3（通过 maven 配置的版本）

## 我的问题
对于相同词性结构的句子<机构>的<修饰性名词><人名>，依存句法分析结果不一致。

## 复现问题
直接调用parser NeuralNetworkDependencyParser（并未进行任何修改），对以下句子进行依存句法分析：
1.上海华安工业公司的员工韩梅梅
2.上海华安工业公司的董事韩梅梅
得到的依存分析结果不一致。不知道是否有改进方法？

### 触发代码
```
    public void testIssue1234() throws Exception
    {
        IDependencyParser parser = new NeuralNetworkDependencyParser().enableDeprelTranslator(false);
        CoNLLSentence sentenceParsed1 = parser.parse(""上海华安工业公司的员工韩梅梅"");
        CoNLLWord[] arcs1 = sentenceParsed1.word;
        for (CoNLLWord word : arcs1) {
            System.out.printf(""%s(%s) --(%s)--> %s(%s)\n"", word.LEMMA, word.ID, word.DEPREL, word.HEAD.LEMMA, word.HEAD.ID);
        }
        System.out.println();
        CoNLLSentence sentenceParsed2 = parser.parse(""上海华安工业公司的董事韩梅梅"");
        CoNLLWord[] arcs2 = sentenceParsed2.word;
        for (CoNLLWord word : arcs2) {
            System.out.printf(""%s(%s) --(%s)--> %s(%s)\n"", word.LEMMA, word.ID, word.DEPREL, word.HEAD.LEMMA, word.HEAD.ID);
        }
    }
```
### 期望输出
```
上海(1) --(ATT)--> 华安(2)
华安(2) --(ATT)--> 公司(4)
工业(3) --(ATT)--> 公司(4)
公司(4) --(ATT)--> 员工(6)
的(5) --(RAD)--> 公司(4)
员工(6) --(ATT)--> 韩梅梅(7)
韩梅梅(7) --(HED)--> ##核心##(0)

上海(1) --(ATT)--> 华安(2)
华安(2) --(ATT)--> 公司(4)
工业(3) --(ATT)--> 公司(4)
公司(4) --(ATT)--> 董事(6) //对应""员工""
的(5) --(RAD)--> 公司(4)
董事(6) --(ATT)--> 韩梅梅(7)
韩梅梅(7) --(HED)--> ##核心##(0)
```

### 实际输出
但实际输出的依存分析，上海华安工业公司的ATT路径不一致，1.指向“员工”修饰性名词，2.指向“韩梅梅”实体。

```
上海(1) --(ATT)--> 华安(2)
华安(2) --(ATT)--> 公司(4)
工业(3) --(ATT)--> 公司(4)
公司(4) --(ATT)--> 员工(6)
的(5) --(RAD)--> 公司(4)
员工(6) --(ATT)--> 韩梅梅(7)
韩梅梅(7) --(HED)--> ##核心##(0)

上海(1) --(ATT)--> 华安(2)
华安(2) --(ATT)--> 公司(4)
工业(3) --(ATT)--> 公司(4)
公司(4) --(ATT)--> 韩梅梅(7) //不对应""员工""
的(5) --(RAD)--> 公司(4)
董事(6) --(ATT)--> 韩梅梅(7)
韩梅梅(7) --(HED)--> ##核心##(0)
```"
HanLP,hankcs,318690595,NLPTokenizer.segment 线程安全问题,open,提问,https://github.com/hankcs/HanLP/issues/809,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
`NLPTokenizer.segment` 在多线程运行情况下，概率性地出现如下问题：

```
SEVERE: Exception while executing runnable io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed@721b9773
java.lang.ArrayIndexOutOfBoundsException
        at java.lang.System.arraycopy(Native Method)
        at com.hankcs.hanlp.dictionary.TransformMatrixDictionary.extendSize(TransformMatrixDictionary.java:221)
        at com.hankcs.hanlp.corpus.util.CustomNatureUtility.addNature(CustomNatureUtility.java:62)
        at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:838)
        at com.hankcs.hanlp.tokenizer.lexical.AbstractLexicalAnalyzer.segSentence(AbstractLexicalAnalyzer.java:321)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:557)
        at com.hankcs.hanlp.tokenizer.NLPTokenizer.segment(NLPTokenizer.java:49)
        at com.company.xushen.Server$NLPImpl.segment(Server.java:71)
        at com.company.xushen.NLPGrpc$MethodHandlers.invoke(NLPGrpc.java:230)
        at io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:171)
        at io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)
        at io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:698)
        at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
        at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
```

我怀疑，虽然 `NLPTokenizer.segment` 声称是线程安全的，但是在调用栈的 `CustomNatureUtility.addNature` 并不是线程安全的，才导致这样的错误。
"
HanLP,hankcs,317467719,语义查询 添加类似“哈哈哈哈哈”的重复叠词文档时 无法返回正确结果,open,提问,https://github.com/hankcs/HanLP/issues/807,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 描述
在进行语义查询时, 插入的文档如果是类似“哈哈哈哈哈哈”这种叠词重复的组合，无法正确返回结果，有BUG。

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现
使用doc2vector进行语义查询，调用addDocument接口添加“哈哈哈哈哈哈”这种文档
使用的词向量模型是官方提供的HANLP预训练向量。
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤
无

### 触发代码
无
### 期望输出
无


### 实际输出
无

## 其他信息
无
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,316098855,MathTools.java 代碼疑問,open,重复,https://github.com/hankcs/HanLP/issues/802,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是： 1.6.4
我使用的版本是： 1.5.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
您好，我想請教 MathTools.java 裡的 calculateWeight
   
` double value = -Math.log(dSmoothingPara * frequency / (MAX_FREQUENCY) + (1 - dSmoothingPara) * ((1 - dTemp) * nTwoWordsFreq / frequency + dTemp));`    

這行代碼是參考什麼算法或公式所寫的?

<!-- 请详细描述问题，越详细越可能得到解决 -->

"
HanLP,hankcs,315702527,关于感知机与CRF对空格与标点的词性识别问题,open,讨论,https://github.com/hankcs/HanLP/issues/797,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：portable-1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
综合比较其他几种分词器，感知机对词性的标注相对更准确，但是对于空格和标点符号(尤其是英文标点)的标注存在许多问题。

例如对以下这句话的标注：
`""你好， 我想知道： 风是从哪里来; 雷是从哪里来； 雨是从哪里来？""`

perceptron:
```
(你/r) (好/a) (，/w) ( /v) (我/r) (想/v) (知道/v) (：/w) ( 风/n) (是/v) (从/p) (哪里/r) (来/v) (; 雷/d) (是/v) (从/p) (哪里/r) (来/v) (；/w) ( 雨/n) (是/v) (从/p) (哪里/r) (来/v) (？/w) 
```
crf:
```
(你好/d) (，/v) ( /v) (我/r) (想/v) (知道/v) (：/w) ( 风/n) (是/v) (从/p) (哪里/r) (来;/v) ( 雷/n) (是/v) (从/p) (哪里/r) (来/v) (；/w) ( 雨/n) (是/v) (从/p) (哪里/r) (来/v) (？/v) 
```
viterbi:
```
(你好/l) (，/w) ( /w) (我/r) (想/v) (知道/v) (：/w) ( /w) (风/n) (是从/v) (哪里/r) (来/v) (;/w) ( /w) (雷/n) (是从/v) (哪里/r) (来/v) (；/w) ( /w) (雨/n) (是从/v) (哪里/r) (来/v) (？/w) 
```

重点关注空格与标点的分词，结果发现：感知机与CRF对有时空格与标点识别为其他词性，甚至会与前后的词成为组合，反而默认的viterbi对于标点的处理更好。

最近开始接触这方面，尚未仔细阅读源码，请问对于空格与标点是如何处理的，能否改进？请指导。

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->"
HanLP,hankcs,315535064,优化CRF分词效率的方法,open,讨论,https://github.com/hankcs/HanLP/issues/796,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.3
我使用的版本是：1.6.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

[CRF分词](https://github.com/hankcs/HanLP#6-crf%E5%88%86%E8%AF%8D)和[感知机分词](https://github.com/hankcs/HanLP/wiki/%E7%BB%93%E6%9E%84%E5%8C%96%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A0%87%E6%B3%A8%E6%A1%86%E6%9E%B6)的流程相差不大(都是提取特征->查概率/权重->累加->Viterbi), 
但Wiki上面的测试结果差距却很大。而且HanLP早期的CRF模型特征模板数量少于当前感知机的七个模板。
因此查看了一下HanLP构造CRF模型的逻辑，我发现了一个问题：
CRF++生成的特征都是以“U[0-9]+:”开头的，而模型使用BinTrie索引特征概率，这就导致BinTrie加速的第一层只有一个字符“U”，，所有的特征都都走了二分查找，难怪速度会慢。

## 解决思路

需要解决的是如何把汉字索引到第一级同时又不影响效率，我觉得可以考虑拆解重组特征模板和特征Key，或者直接reverse字符串。
"
HanLP,hankcs,313957780,关于自定义Recognition和日期或者时间的分词上的问题,open,提问,https://github.com/hankcs/HanLP/issues/791,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.2
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
请问在对""管理员于2017-10-12对该设备信息进行了修改操作""进行拆分时，“2017-10-12”这个日期会被单独拆开。有没有好的方式解决这点，找了找类似的issue,貌似没找到合适的方式。另外现在有提供Recognition自定义么。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->


### 触发代码

```
     public void testIssue1234() throws Exception
    {
        PerceptronLexicalAnalyzer analyzer =  new PerceptronLexicalAnalyzer();
        System.out.println(analyzer.seg(""管理员于2017-10-12对该设备信息进行了修改操作""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
管理员/n, 于/p, 2017-10-12, 对/p, 该/r, 设备/n, 信息/n, 进行/v, 了/u, 修改/v, 操作/v
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
管理员/n, 于/p, 2017-/m, 10/m, -/q, 12/m, 对/p, 该/r, 设备/n, 信息/n, 进行/v, 了/u, 修改/v, 操作/v


```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
运行截图：
![38286722-5e5c1772-37f9-11e8-83b0-eb77566f9c82](https://user-images.githubusercontent.com/18030444/38715484-f8034542-3f0d-11e8-8b38-3fa6236a8e5b.png)
"
HanLP,hankcs,313675528,一个自定义词库生成bin文件的问题,open,无效,https://github.com/hankcs/HanLP/issues/789,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
当前最新版本号是：1.6.2
我使用的版本是：1.6.2




## 我的问题

自定义词库生成bin文件，通过配置hanlp.properties文件中CustomDictionaryPath，配置同一目录下多个自定义字典，Hanlp不能一次生成所有自定义字典的bin文件，每次只能生成CustomDictionaryPath配置项的第一个自定义字典的bin文件，导致剩下的自定义字典中的词不能识别拆分。


## 复现问题
Hadoop集群中添加自定义词典

### 步骤

1. 首先，在自定义目录custom下面添加自定义词典(比如，字典名为军事.txt; 动植物.txt; 历史.txt; 娱乐.txt; 旅行.txt)
2. 然后，在hanlp.properties中添加CustomDictionaryPath配置(比如，CustomDictionaryPath=data/dictionary/custom/军事.txt; 动植物.txt; 历史.txt; 娱乐.txt; 旅行.txt)
3. 接着，执行Hanlp拆分，执行完后，只能生成""军事.txt.bin""的bin文件，动植物、历史、娱乐、旅行对应的bin文件不能生成

### 触发代码


### 期望输出

通过配置文件可以生成所有bin文件



### 实际输出

只有第一个配置词典的bin文件，导致后续分词，除第一个以外的字典都不能用来自定义分词



<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,313543284,textrank算法公式没有注释,open,提问,https://github.com/hankcs/HanLP/issues/787,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.2
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在阅读这个博文的时候，发现textrank算法的公式没注释说明，希望博主发一份公式说明，我最近在搞关键词提取，希望再textrank的基础上改进算法
博文url：
http://www.hankcs.com/nlp/textrank-algorithm-to-extract-the-keywords-java-implementation.html

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

![image](https://user-images.githubusercontent.com/23119744/38651352-5cafd126-3e33-11e8-8b29-f866538276cb.png)


"
HanLP,hankcs,312264067,关于感知机分词的一点疑问,open,提问,https://github.com/hankcs/HanLP/issues/783,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.2
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
仿照文档中的示例可以正确识别“下雨天地面积水”，但是对“中国联通是公司”的学习不如预期，不知道是不是我在使用上有不当导致。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->



### 触发代码

```
    public void testIssue1234() throws Exception
    {
        PerceptronSegmenter segmenter = new PerceptronSegmenter(HanLP.Config.PerceptronCWSModelPath);
            System.out.println(segmenter.segment(""下雨天地面积水""));
            segmenter.learn(""下雨天 地面 积水"");
            System.out.println(segmenter.segment(""下雨天地面积水""));
            System.out.println(segmenter.segment(""中国联通是公司""));
            segmenter.learn(""中国联通 是 公司"");
            System.out.println(segmenter.segment(""中国联通是公司""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
            [下雨, 天地, 面积, 水]
            [下雨天, 地面, 积水]
            [中国, 联通, 是, 公司]
            [中国联通, 是, 公司]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
            [下雨, 天地, 面积, 水]
            [下雨天, 地面, 积水]
            [中国, 联通, 是, 公司]
            [中国, 联通, 是, 公司]
```



"
HanLP,hankcs,310731182,com.hankcs.hanlp.summary.KeywordExtractor类的shouldInclude方法，不识别一个字的词,open,改进,https://github.com/hankcs/HanLP/issues/780,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.6.2.jar
我使用的版本是：hanlp-1.5.4.jar

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

com.hankcs.hanlp.summary.KeywordExtractor类的shouldInclude方法，为什么判断一个字的词不在计算范围内，像  霾  这个字本身就是一个词，是不是太武断了

## 复现问题
### 触发代码

public boolean shouldInclude(Term term)
    {
        // 除掉停用词
        if (term.nature == null) return false;
        String nature = term.nature.toString();
        char firstChar = nature.charAt(0);
        switch (firstChar)
        {
            case 'm':
            case 'b':
            case 'c':
            case 'e':
            case 'o':
            case 'p':
            case 'q':
            case 'u':
            case 'y':
            case 'z':
            case 'r':
            case 'w':
            {
                return false;
            }
            default:
            {
                if (_**term.word.trim().length() > 1**_ && !CoreStopWordDictionary.contains(term.word))
                {
                    return true;
                }
            }
            break;
        }

        return false;
    }


"
HanLP,hankcs,310670495,对部分人名中有数词（如张三丰），在拆分时会出现不如预期情况。,open,提问,https://github.com/hankcs/HanLP/issues/779,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.2
我使用的版本是：1.6.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
存在某些人名中有数词出现的人名时，会出现拆分不如预期的问题，如“张三丰，黄三元，刘五郎”会出现问题，而“王三强，闻一多，李四光”等经测试没有问题，不知道是不是因为有问题的属于常用词或商标名等原因导致。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->



### 触发代码

```
    public static void main(String[] args) {
        String content1 = ""张三丰，刘五郎，黄三元，张一楠，王三强，丁一楠，李四光，闻一多，赵一楠，李四"";
        System.out.println(NLPTokenizer.segment(content1));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出：张三丰，刘五郎，黄三元，张一楠，王三强，丁一楠，李四光，闻一多，赵一楠，李四
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出：张/q, 三丰/nz, ，/w, 刘/nr, 五郎/nz, ，/w, 黄/a, 三元/nz, ，/w, 张一楠/nr, ，/w, 王三强/nr, ，/w, 丁一楠/nr, ，/w, 李四光/nr, ，/w, 闻一多/nr, ，/w, 赵一楠/nr, ，/w, 李四/nr]

```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
测试结果
![jg](https://user-images.githubusercontent.com/18030444/38225667-8e0e25c2-3728-11e8-98cd-4d2070edddd1.png)
"
HanLP,hankcs,310015142,命名实体识别后分词不理想,open,提问,https://github.com/hankcs/HanLP/issues/777,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是： master主分支

我新建了实体识别楼盘名，ViterbiSegment 分词 “地址是星海城三期”，
识别出楼名 
[星海城/nbd] 和 [星海城三期/nbd]，但分词结果却是
[地址/n, 是/vshi, 星海/n, 城/n, 三期/nbdp]

请问如何使结果为
[地址/n, 是/vshi, 星海城三期/nbd]"
HanLP,hankcs,309970071,依存分析设置使用感知机分词之后，标点符号的依存关系错误,open,改进,https://github.com/hankcs/HanLP/issues/776,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.1
我使用的版本是：maven  上的 portable 1.6.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

       依存句法分析可以设置分词器，在设置为感知机分词器之后，标点符号的依存关系不再是之前的 **标点符号** 关系，而是 **动宾关系** 之类。


<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 触发代码

```java
        String test = ""什么是自然语言处理?"";
        IDependencyParser parser = new NeuralNetworkDependencyParser();
        PerceptronLexicalAnalyzer segmenter = new PerceptronLexicalAnalyzer(HanLP.Config.PerceptronCWSModelPath,
                HanLP.Config.PerceptronPOSModelPath, HanLP.Config.PerceptronNERModelPath);
        parser.setSegment(segmenter);
        CoNLLSentence sentence = parser.parse(test);
        System.out.println(sentence);
        // 可以方便地遍历它
        for (CoNLLWord word : sentence) {
            System.out.printf(""%s --(%s)--> %s\n"", word.LEMMA, word.DEPREL, word.HEAD.LEMMA);
        }
        CoNLLWord[] wordArray = sentence.getWordArray();
        System.out.println(""-------0000----"");
```

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
1	什么	什么	r	ry	_	2	主谓关系	_	_
2	是	是	v	vshi	_	0	核心关系	_	_
3	自然语言处理	自然语言处理	nz	nz	_	2	动宾关系	_	_
4	?	?	wp	w	_	2	标点符号	_	_

什么 --(主谓关系)--> 是
是 --(核心关系)--> ##核心##
自然语言处理 --(动宾关系)--> 是
? --(标点符号)--> 是
-------0000----

```


### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
1	什么	什么	r	r	_	2	主谓关系	_	_
2	是	是	v	v	_	0	核心关系	_	_
3	自然语言	自然语言	n	n	_	4	主谓关系	_	_
4	处理	处理	v	vn	_	2	动宾关系	_	_
5	?	?	n	n	_	4	动宾关系	_	_

什么 --(主谓关系)--> 是
是 --(核心关系)--> ##核心##
自然语言 --(主谓关系)--> 处理
处理 --(动宾关系)--> 是
? --(动宾关系)--> 处理
-------0000----
```
## 其他信息

依存句法分析的默认分词器对标点符号的依存分析是正确的。

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,309647968,关于感知机分词offset为0的问题,open,改进,https://github.com/hankcs/HanLP/issues/775,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：1.6.1

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1.6.0后HanLP推出了感知机分词，非常感谢作者的辛苦付出。我的问题：
1、感知机分词没有对Term做偏移标记？Term中的offset现在都是0，是实现起来很困难吗？
2、如果感知机分词能实现offset，是否有计划在下一版增加这一功能，大致是什么时间？
3、有无可能通过HanLP.newSegment(int segType)提供一个工厂方法，来创建感知机分词？

### 触发代码

```
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,309191692,单个词的词性识别，是否可标注多个词性,open,提问,https://github.com/hankcs/HanLP/issues/774,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

关于词性标注，是否一个词可标注多个词性，例如：汪洋，可以是人名也可以是汪洋\n 大海\n

"
HanLP,hankcs,308347513,我自己也在做词典的命名实体，想知道哪里有命名实体的标注规则文档，还是说这些是根据自己的需求来定,open,提问,https://github.com/hankcs/HanLP/issues/773,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,307942813,关于hanlp分词中/++/.!=\\等符号以及符号组~@#$%^&的符号合成新词问题,open,提问,https://github.com/hankcs/HanLP/issues/771,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.6.0
我使用的版本是：1.5.2 / 1.6.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

利用hanlp分词的时候，英文半角符号组/,?,!,+,|,\,=,. 、符号组~,@,#,$,%,^&，都会自动拼接在一起，组成新词


### 触发代码

startJVM(getDefaultJVMPath(),""-Djava.class.path=C:/Users/reading-magic/Desktop/Work/Cut/hanlp-1.6.0.jar;\
C:/Users/reading-magic/Desktop/Work/Cut"",""-Xms1g"", ""-Xmx1g"")
HanLP = JClass(""com.hankcs.hanlp.tokenizer.NLPTokenizer"")
hanlp_words=HanLP.segment(""c#/.net,|?.*/\\=!,~@#$%^&"")
for i in hanlp_words:
    print(i)

### 期望输出
期望输出的符号能够分开
### 期望输出

c#    /nx
/    /w
.    /w
net    /w
,    /w
|    /w
.    /w
”*“   /w
?   /w
/    /w
\    /w
\    /w
=   /w
!    /w
,    /w
~     /nx
@   /nx
“#”   /nx
$   /nx
%   /nx
^   /nx
&   /nx

### 实际输出

实际上把对应词性为w和nx的词分别合成在一起，
```
实际输出
```
c#    /nx,
 /.    /w,
 net   /nx,
 ,|?.*/\=!,    /w
  ~@#$%^&     /nx
## 其他信息
”*“跟”#“会与html或者css冲突
利用了python调用了java 程序，同时在词典里找不到相应的信息
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
![image](https://user-images.githubusercontent.com/22814402/37819529-baf80648-2eb8-11e8-8e33-3519e5ac91f9.png)

![image](https://user-images.githubusercontent.com/22814402/37819522-b5b7bf20-2eb8-11e8-9969-4b8adf8b1043.png)
"
HanLP,hankcs,301955370,索引模式下全切分不准确,open,提问,https://github.com/hankcs/HanLP/issues/765,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

IndexTokenizer索引模式全切分的问题，
如“我爱中华人民共和国”，
**实际输出**
“我”，“爱”，“中华人民共和国”，”中华人民 ”，”中华”，”华人” ，”人民共和国 ”，”人民”，”共和国 ”，”共和” 。
”人民政府”,”人民”,”政府”,”民政”
**期望输出**
“我”，“爱”，“中华人民共和国”，”中华人民 ”，”中华”，”人民共和国 ”，”人民”，”共和国 ”，”共和” 。
”人民政府”,”人民”,”政府”

viterbi取得最优解“中华人民共和国”后，然后**完全根据词库进行细切**的，故分出来“华人”，”民政”，等明显为bad case。
关于fix这个问题，您有什么建议吗？



"
HanLP,hankcs,294985719,常见词重叠，被误识别为机构的,open,提问,https://github.com/hankcs/HanLP/issues/759,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        HanLP.Config.enableDebug(true);
        Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(false).enableOffset(true)
            .enablePartOfSpeechTagging(true).enableCustomDictionary(true);
        System.out.println(StandardTokenizer.segment(""辖区有中学和小学小学各1所。""));
    }
```
### 期望输出

[辖区/n, 有/vyou, 中学/nt, 和/cc, 小学/nt, 小学/nt, 各/rz, 1所/nt, 。/w]

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[辖区/n, 有中学/nt, 和/cc, 小学小学/nt, 各/rz, 1所/nt, 。/w]
```

## 其他信息

机构识别出差的问题，类似的很多，比如句子“战争下会无缘无故打起来”，“不”写错为“下”，把“战争下会”拼为nt了

"
HanLP,hankcs,293773466,词网与词图的区别,open,,https://github.com/hankcs/HanLP/issues/757,"请教一下词网和词图这两种数据结构分别适用于怎样的算法。
在ViterbiSegment中使用了词网结构，每一行都是前缀词链。我觉得使用像DijkstraSegment中一样的词图，然后依次对节点选择最优解应该也可以实现Viterbi最短路切分。
那么，另外又定义的词网结构的意义是什么，它在效率上是更优吗？"
HanLP,hankcs,293565987,请问怎么自定义简繁转换的词典,open,提问,https://github.com/hankcs/HanLP/issues/756,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.5.3
我使用的版本是：portable-1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
你好，请教个问题，因为我们公司的商标台陆通，台不转繁体，只有陆转繁体，所以想知道是否能通过自定义的方式解决这种特定情况下的简繁转换问题，默认情况下台也会被转成繁体，但这不是我们要的效果，该如何解决
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,293082418,你好，请问这可以做三元组抽取吗?,open,,https://github.com/hankcs/HanLP/issues/755,"你好，请问这可以做三元组抽取吗?
 比如：
张三在湖南长大，出生于1999年1月一日。
我想提出：张三出生于1999年1月一日

这个能抽取吗？或者作者可否指点一下，给一下思路，谢谢。"
HanLP,hankcs,293045179,最新版本（1.5.3）问题:同样调用HanLP.segment()方式一和方式二分词结果不一致,open,提问,https://github.com/hankcs/HanLP/issues/754,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
最新版本（1.5.3），按照说明分别配置，分词结果不一样
System.out.println(HanLP.segment(""把两张车票换成三张""));
方式一(maven)：[把/p, 两/m, 张/q, 车票/n, 换/v, 成三张/nr]
方式二(自定义)：[把/pba, 两/m, 张/q, 车票/n, 换成/v, 三/m, 张/q]

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤
1.方式一：直接用maven 引，即：
<dependency>
    <groupId>com.hankcs</groupId>
    <artifactId>hanlp</artifactId>
    <version>portable-1.5.3</version>
</dependency>
2.方式二：下载源码和data.zip数据配置
3.测试分词：System.out.println(HanLP.segment(""把两张车票换成三张""));

### 触发代码：System.out.println(HanLP.segment(""把两张车票换成三张""));

### 期望输出：[把/pba, 两/m, 张/q, 车票/n, 换成/v, 三/m, 张/q]

### 实际输出：[把/p, 两/m, 张/q, 车票/n, 换/v, 成三张/nr]
"
HanLP,hankcs,292640320,自定义词典 持久化问题,open,,https://github.com/hankcs/HanLP/issues/753,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.3

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
1、自定义词典持久化好像官方没有给出最优做法，参考 问题  [#182#](https://github.com/hankcs/HanLP/issues/182)  需要维护 CustomDictionary.txt 文件，是需要自己通过IO操作文件 ； 

2、在配置文件中增加 CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; **# mydict.txt;** 自定义词典文件，没有效果，生成的缓存文件也没有，是否还需要有其他操作，或者需要单独生生成 

 
"
HanLP,hankcs,291065235,您好，请问有没有非全局的用户词典，即分词时可动态指定是用哪个用户词典,open,,https://github.com/hankcs/HanLP/issues/752,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.53
我使用的版本是：1.53

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

您好，请问有没有非全局的用户词典，即分词时可动态指定使用哪个用户词典，类似ANSJ分词时可指定
存储着自定义词集合的Forset参数
"
HanLP,hankcs,289143519,同义词词典第一句开头包含\U+FEFF,open,,https://github.com/hankcs/HanLP/issues/749,"1. 我使用的mave依赖包版本号
        <dependency>
            <groupId>com.hankcs</groupId>
            <artifactId>hanlp</artifactId>
            <version>portable-1.5.3</version>
        </dependency>

2. 发现有个问题是同义词词典开头出现一个非占位空格\U+FEFF
![image](https://user-images.githubusercontent.com/5870269/35026499-59e0c7c4-fb86-11e7-90ef-d2862f224090.png)
【【 疑问 】】
UTF-8 with BOM这种格式的同义词词典在加载的时候，会将第一个字符当做args[0]来计算id值，那么这样计算出来的ID值是不是存在问题？
![image](https://user-images.githubusercontent.com/5870269/35042223-b80fc476-fbc2-11e7-8030-3dedacaf369e.png)

"
HanLP,hankcs,288858081,HMM-NGram分词模型等效词替换后如何保留的原词？,open,提问,https://github.com/hankcs/HanLP/issues/748,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->
master
当前最新版本号是：
1.5.3
我使用的版本是：
1.5.3
<!--以上属于必填项，以下可自由发挥-->

## 我的问题
二元接续词典地名人名等被替换成了“等效词”，这样在训练1998年语料的时候像“中国”，“北京”等地名都成了等效词。如下面：
`未##串	x	130296

未##人	nr	607718	nrf	113445

未##团	nt	112253	ntc	25517	nto	18894	ntu	5426	nth	2556	ntcb	1846	nts	677	ntch	568	ntcf	118

未##地	ns	595380	nsf	124178

未##它	xx	1000

未##数	mq	753456	m	733982

未##时	t	757118`
我训练后的结果也同上面一样，被替换后已有的词“中国”就没有了呀，但是你的二元接续词典里还有原词，比如：
`中国	ns	39573`

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
   /**
 * 等效词编译器
 * @author hankcs
 */
public class PosTagCompiler
{
    /**
     * 编译，比如将词性为数词的转为##数##
     * @param tag 标签
     * @param name 原词
     * @return 编译后的等效词
     */
    public static String compile(String tag, String name)
    {
        if (tag.startsWith(""m"")) return Predefine.TAG_NUMBER;
        else if (tag.startsWith(""nr"")) return Predefine.TAG_PEOPLE;
        else if (tag.startsWith(""ns"")) return Predefine.TAG_PLACE;
        else if (tag.startsWith(""nt"")) return Predefine.TAG_GROUP;
        else if (tag.startsWith(""t"")) return Predefine.TAG_TIME;
        else if (tag.equals(""x"")) return Predefine.TAG_CLUSTER;
        else if (tag.equals(""nx"")) return Predefine.TAG_PROPER;
        else if (tag.equals(""xx"")) return Predefine.TAG_OTHER;
        return name;
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```
期望既有等效词，也有地名等原词的条件转移词典，我看hankcs的CoreNatureDictionary.txt也是有的
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```
CoreNatureDictionary.txt未输出：
`中国	ns	39573`
**想问下hankcs的训练的词典为什么既有等效词，也有地名等原来的词，是一个综合的结果呢?
我训练数据少了哪一步呢，跟你的结果不一样**
## 其他信息
我指的词典主要指的是：CoreNatureDictionary.txt，CoreNatureDictionary.ngram.txt，CoreNatureDictionary.tr.txt
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,288058814,如何使用CRF解码用CRF++训练生成的模型,open,改进,https://github.com/hankcs/HanLP/issues/745,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
使用CRF++解码用CRF++训练生成的模型进行实体识别时，如果开启多线程运行，则会报错，猜测是其c++底层有错误。
HanLP使用了CRF解码，但是只用于分词。
请问如果要用CRF解码用CRF++训练生成的模型进行实体识别，应该参考哪些部分，或者可以复用CRF解码用于分词的哪些思路吗？
<!-- 请详细描述问题，越详细越可能得到解决 -->"
HanLP,hankcs,288050880,依存方法分析中时间词的词性未能正确标注,open,,https://github.com/hankcs/HanLP/issues/744,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

在对以下句子进行分析时：

我的太太小张今天感冒了

通过segment得到词性如下：
 ['我/rr', '的/ude1', '太太/n', '小张/n', '今天/t', '感冒/vi', '了/ule']
这里的今天被解析成为时间词，正确。

在依存分析中：
   1     我/rr 	 3	定中关系 
   2     的/ude1	 1	右附加关系
   3    太太/n  	 6	主谓关系 
   4    小张/n  	 6	主谓关系 
   5    今天/n  	 6	主谓关系 
   6    感冒/vi 	 0	核心关系 
   7     了/ule	 6	右附加关系

这里今天被解析成名词，虽未出错，但不够精细，导致两者不一致的原因是？

## 复现问题
我使用python，通过jpype来调用 Hanlp

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,288049936,依存方法分析的初始化问题,open,,https://github.com/hankcs/HanLP/issues/743,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.3
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我通过HanLP.parseDependency调用依存方法分析，经查看，这是一个静态方法，估计每次调用时都要加载模型。有没有办法可以将parser的实例保存起来以避免下次调用时再加载模型呢？

注：我通过Jpype (python)来使用HanLP

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,288036615,Viterbi分词器模型异常,open,,https://github.com/hankcs/HanLP/issues/742,"
1、待分词文本：陶某、陶某某路经该处时
    public static void main(String args[]) {
        HanLP.Config.DEBUG = true;
        Segment segment = new ViterbiSegment();
        List<Term> termList = segment.seg(""陶某、陶某某路经该处时"");
        System.out.println(termList);
    }
经查看日志，陶某人名已经识别出，可识别结果依然为：
[陶/ag, 某/rz, 、/w, 陶/ag, 某/rz, 某/rz, 路经/v, 该/rz, 处/n, 时/qt]

程序运行日志为：
粗分结果[陶/ag, 某/rz, 、/w, 陶/ag, 某/rz, 某/rz, 路经/v, 该/rz, 处/n, 时/qt]
人名角色观察：[  K 1 A 1 ][陶 B 771 D 30 C 15 E 9 ][某 G 2055 C 1082 D 420 L 34 K 7 ][、 M 19857 L 5234 K 4094 ][陶 B 771 D 30 C 15 E 9 ][某 G 2055 C 1082 D 420 L 34 K 7 ][某 G 2055 C 1082 D 420 L 34 K 7 ][路经 A 20843310 ][该 K 18 L 13 ][处 L 52 K 10 D 9 ][时 L 228 C 137 D 134 K 106 B 88 ][  K 1 A 1 ]
人名角色标注：[ /K ,陶/B ,某/G ,、/M ,陶/B ,某/C ,某/L ,路经/A ,该/K ,处/D ,时/L , /K]
识别出人名：陶某 BG
识别出人名：陶某 BC

和预期结果不符合。


2、运用公式可能有误
经检查代码片段中，计算转移概率公式（见MathTools.calculateWeight方法）：
 double value = -Math.log(dSmoothingPara * frequency / (MAX_FREQUENCY) + (1 - dSmoothingPara) * ((1 - dTemp) * nTwoWordsFreq / frequency + dTemp));

该公式经推敲，多乘了一个frequency ，由于是底层算法，不知理解是否有误。

我将公式修改为：
 double value = -Math.log(dSmoothingPara * frequency / (MAX_FREQUENCY) + (1 - dSmoothingPara) * ((1 - dTemp) * nTwoWordsFreq / frequency + dTemp));

重新运行代码得到如下结果：
[陶某/nr, 、/w, 陶某某/nr, 路经/v, 该/rz, 处/n, 时/qt]
满足预期结果

3、由于是底层算法代码，不知是否修改有误，请指导！
"
HanLP,hankcs,287986817,机构识别角色标注是否不全。比如K、P没找到对应的意义,open,,https://github.com/hankcs/HanLP/issues/741,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息
![image](https://user-images.githubusercontent.com/30866940/34856575-ca7ea6ae-f780-11e7-9d4d-c9afbfaa567a.png

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,287334574,分词问题：自定义词典有时会失效,open,提问,https://github.com/hankcs/HanLP/issues/739,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-portable-1.5.3
我使用的版本是：hanlp-portable-1.3.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

自定义词典在某种情况下会失效，对“海南省海口市龙华区春来早市场”进行分词，结果正常，但是增加一个方位词内字之后会出现分词异常的情况

## 复现问题
我自定义的词典：
```
海南省 dictrict1 1000
海口市 dictrict2 1000
龙华区 dictrict3 1000
春来早市场 resrge 1000
```
```
   List<Term> termList = HanLP.segment(""海南省海口市龙华区春来早市场"");
    //分词结果：海南省/dictrict1, 海口市/dictrict2, 龙华区/dictrict3, 春来早市场/resrge
```
此时说明 “春来早市场”是正确加载的了

### 触发代码

```
   List<Term> termList = HanLP.segment(""海南省海口市龙华区春来早市场内"");
```
### 期望输出

```
海南省/dictrict1, 海口市/dictrict2, 龙华区/dictrict3, 春来早市场/resrge, 内/s
```

### 实际输出

```
海南省/dictrict1, 海口市/dictrict2, 龙华区/dictrict3, 春/tg, 来/v, 早市/n, 场内/s
```
增加了“内”字，重新分词，发现分词效果与预期的差别比较大，这种情况应该怎么处理呢





  
  "
HanLP,hankcs,286561222,"姓名前面有句号，导致姓名识别错误：""。杨瑞云告诉大家，慢慢的，""",open,提问,https://github.com/hankcs/HanLP/issues/738,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

对句子前面有句号，分词识别人名错误。删除句号，识别正确。句子如下：
。杨瑞云告诉大家，慢慢的，

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
        HanLP.Config.enableDebug(true);
        Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(true).enableOffset(true)
            .enablePartOfSpeechTagging(true).enableCustomDictionary(true);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[。/w, 杨瑞云/nr, 告诉/v, 大家/rr, ，/w, 慢慢/d, 的/ude1, ，/w]
```

### 实际输出

```
[。/w, 杨瑞/nr, 云/vg, 告诉/v, 大家/rr, ，/w, 慢慢/d, 的/ude1, ，/w]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,286238268,句法依存分析错误一例,open,提问,https://github.com/hankcs/HanLP/issues/737,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
1.5.2

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
对下列句子进行依存关系分析：
他那样有身价的人能干出这样的事？
得到如下结果：
1	他	他	r	rr	_	3	主谓关系	_	_
2	那样	那样	r	rzv	_	3	状中结构	_	_
3	有	有	v	vyou	_	6	定中关系	_	_
4	身价	身价	n	n	_	3	动宾关系	_	_
5	的	的	u	ude1	_	3	右附加关系	_	_
6	人	人	n	n	_	11	定中关系	_	_
7	能干	能干	a	a	_	11	定中关系	_	_
8	出	出	v	vf	_	7	动补结构	_	_
9	这样	这样	r	rzv	_	11	定中关系	_	_
10	的	的	u	ude1	_	9	右附加关系	_	_
11	事	事	n	n	_	0	核心关系	_	_
12	？	？	wp	w	_	11	标点符号	_	_
这个结果应该是错的吧？核心关系应该是”干出“吧？


"
HanLP,hankcs,286204478,在Android上使用外挂字典遇到sun.reflect.ReflectionFactory类找不到的问题,open,提问,https://github.com/hankcs/HanLP/issues/736,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.5.3
我使用的版本是：portable-1.3.5

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
在Android使用hanlp.properties配置外挂字典时，遇到问题：
01-04 21:49:47.031 1196-2513/com.nio.nlu W/HanLP: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!
                                                  如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类
                                                  
                                                  --------- beginning of crash
01-04 21:49:47.033 1196-2513/com.nio.nlu E/AndroidRuntime: FATAL EXCEPTION: Thread-66
                                                           Process: com.nio.nlu, PID: 1196
                                                           Theme: themes:{}
                                                           java.lang.NoClassDefFoundError: Failed resolution of: Lsun/reflect/ReflectionFactory;
                                                               at com.hankcs.hanlp.corpus.util.EnumBuster.<init>(EnumBuster.java:34)
                                                               at com.hankcs.hanlp.corpus.util.CustomNatureUtility.<clinit>(CustomNatureUtility.java:43)
                                                               at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:838)
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:306)
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:63)
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:50)
                                                               at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:202)
                                                               at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
                                                               at com.hankcs.hanlp.seg.Segment.seg(Segment.java:505)
                                                               at com.nextev.nlu.segment.Segmentation.tokenizationPropertyList(Segmentation.java:561)
                                                               at com.nio.nlu.service.NluService.initSegmentTool(NluService.java:105)
                                                               at com.nio.nlu.service.NluService.-wrap0(NluService.java)
                                                               at com.nio.nlu.service.NluService$1.run(NluService.java:95)
                                                               at java.lang.Thread.run(Thread.java:818)
                                                            Caused by: java.lang.ClassNotFoundException: Didn't find class ""sun.reflect.ReflectionFactory"" on path: DexPathList[[zip file ""/system/app/NluService/NluService.apk""],nativeLibraryDirectories=[/system/app/NluService/lib/arm, /system/app/NluService/NluService.apk!/lib/armeabi-v7a, /vendor/lib, /system/lib]]
                                                               at dalvik.system.BaseDexClassLoader.findClass(BaseDexClassLoader.java:56)
                                                               at java.lang.ClassLoader.loadClass(ClassLoader.java:511)
                                                               at java.lang.ClassLoader.loadClass(ClassLoader.java:469)
                                                               at com.hankcs.hanlp.corpus.util.EnumBuster.<init>(EnumBuster.java:34) 
                                                               at com.hankcs.hanlp.corpus.util.CustomNatureUtility.<clinit>(CustomNatureUtility.java:43) 
                                                               at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:838) 
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:306) 
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:63) 
                                                               at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:50) 
                                                               at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:202) 
                                                               at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57) 
                                                               at com.hankcs.hanlp.seg.Segment.seg(Segment.java:505) 
                                                               at com.nextev.nlu.segment.Segmentation.tokenizationPropertyList(Segmentation.java:561) 
                                                               at com.nio.nlu.service.NluService.initSegmentTool(NluService.java:105) 
                                                               at com.nio.nlu.service.NluService.-wrap0(NluService.java) 
                                                               at com.nio.nlu.service.NluService$1.run(NluService.java:95) 
                                                               at java.lang.Thread.run(Thread.java:818) 
                                                           	Suppressed: java.lang.ClassNotFoundException: sun.reflect.ReflectionFactory
                                                               at java.lang.Class.classForName(Native Method)
                                                               at java.lang.BootClassLoader.findClass(ClassLoader.java:781)
                                                               at java.lang.BootClassLoader.loadClass(ClassLoader.java:841)
                                                               at java.lang.ClassLoader.loadClass(ClassLoader.java:504)
                                                               		... 15 more
                                                            Caused by: java.lang.NoClassDefFoundError: Class not found using the boot class loader; no stack trace available



## 其他信息
是否是Android的java平台平台本身就不包含这个类，是否可以用java.lang.reflect替代sun.reflect？
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,286194624,"建议核心词典添加""地 n""这个词条",open,提问,https://github.com/hankcs/HanLP/issues/735,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：mater
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

广州等地政府发行的债券大面积出现了问题。
""地""会识别为ude2词性，明显不对

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
        HanLP.Config.enableDebug(true);
        Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(true).enableOffset(true)
            .enablePartOfSpeechTagging(true).enableCustomDictionary(true);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->


  "
HanLP,hankcs,285629287,依存分析是否线程安全,open,提问,https://github.com/hankcs/HanLP/issues/732,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ &radic;] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2 离线

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

问题很简单，文档说了HanLp的分词是线程安全的，那依存分析是不是线程安全的呢？使用在多线程中使用依存分析需要注意什么？



  "
HanLP,hankcs,285594592,关于商品型号（字母，分隔符，数据的组合）的分词效果探讨,open,讨论,https://github.com/hankcs/HanLP/issues/731,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2 离线

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

采用标准分词，进行一些包含商品描述的语句进行切分。商品型号通常是任意的字母，分隔符，数据的组合。不是人名、地名、机构等。
进行分词时，可能最终结果粒度过细。而希望是一个整体。
由于型号在不断增加，比较笨的做法是时候添加词库，但那样工作量比较大。较为繁琐。


## 复现问题

Sample: SAMSUNG 三星 **RS542NCAEWW/SC** 545L 风冷变频对开门冰箱，还行还行，可以看看;
分词结果： [SAMSUNG][ ][三星][ ][RS][542][NCAEWW][/][SC][ ][545][L][ ][风冷][变频][对开门冰箱][\][n][还行][还行][，][可以][看看][;]

### 期望输出

[SAMSUNG][ ][三星][ ][RS542NCAEWW/]SC[ ][545][L][ ][风冷][变频][对开门冰箱][\][n][还行][还行][，][可以][看看][;]

即。希望是一个整体 “**RS542NCAEWW/]SC**”


## 其他信息

这种例子有很多，如 
WD 西部数据 **WD20EZRZ** 台式机硬盘 蓝盘 2TB

希望 “WD20EZRZ ” 整体切分

NORITZ 能率 **JSQ25-E4/GQ-13E4AFEX** 13升燃气热水器防冻型

希望 ‘’JSQ25-E4“ 整体切分

  "
HanLP,hankcs,285200186,用于姓名识别改进的讨论Issue,open,讨论,https://github.com/hankcs/HanLP/issues/729,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

之前由于工作上需要，研究了一下HanLP的实体识别存在的不足。主要是误召回比较高。也尝试了一些方法修改，能解决部分问题。准备在我的分支上面修改再PR。所以这里开一个Issue进行讨论。本Issue不太涉及具体识别错误的情况，主要用于收集反馈和讨论。

我在我的Fork新建了一个[Project](https://github.com/TylunasLi/HanLP/projects)：

### 触发代码

```
    public void testIssue729() throws Exception
    {
        String[] sentences = new String[]{
                ""贷款的钱打给谁"",
                ""今天钱扣了有逾期吗""
        };
        HanLP.Config.enableDebug(true);
        Segment segment = HanLP.newSegment().enableNameRecognize(true).enablePlaceRecognize(true).enableOffset(true);
        for (String sentence : sentences)
                systme.out.println(segment.seg(sentence));
    }
```

### 期望输出


```
期望输出
```

### 实际输出


```
实际输出
```

## 其他信息


"
HanLP,hankcs,284844761,人名识别出错：党员来到村民焦玉莲家中,open,改进,https://github.com/hankcs/HanLP/issues/726,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
人名识别出错：党员来到村民焦玉莲家中
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
HanLP.Config.enableDebug(true);
Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true).enablePlaceRecognize(true).enableOffset(true).enablePartOfSpeechTagging(true).enableCustomDictionary(true);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
党员/nnt, 来到/v, 村民/n, 焦玉莲/nr, 家中/s
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
党员/nnt, 来到/v, 村民/n, 焦/ng, 玉莲/nz, 家中/s
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,284689650,如何在python中识别日本人名的译名,open,,https://github.com/hankcs/HanLP/issues/725,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<1.5.2；master>

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<如何在python中实现对日本人名译名的分词>

本人使用了http://www.hankcs.com/nlp/python-calls-hanlp.html中在python中使用HanLP的方法，成功复现所有文中提到的功能，如何实现日本人名识别。
我目前在JClass中调用了com.hankcs.hanlp.recognition.nr.JapanesePersonRecognition包，但是不知道使用哪一个方法。

"
HanLP,hankcs,284530565,依存句法分析相关问题,open,提问,https://github.com/hankcs/HanLP/issues/723,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
您好，我想根据自己的领域语料训练一个神经网络句法分析模型，但是没有看到相关资料，麻烦问下您，hanlp中有根据自己的语料生成神经网络句法分析模型的模块么？打扰了，谢谢！
主要是有两个问题不太明确：
（1）句法分析的语料应该如何标注呢，我利用gate可以进行普通的词性和命名实体标注，但是不晓得句法分析的标注方式；
（2）标注后的语料如何转换成hanlp所支持的神经网络句法分析模型。
万分感谢！
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,283090218,请问下hanlp模型自带的custom dictionary是根据什么来制定的？,open,重复,https://github.com/hankcs/HanLP/issues/720,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v1.5.2, data-for-1.3.3
我使用的版本是：v1.5.2, data-for-1.3.3

<!--以上属于必填项，以下可自由发挥-->
我在使用standardtokenizer 的时候有遇到一个问题， 像“大众”， “现代”， “光明“，不管任何状态下都会识别成ntc， 后来我们有进去看到custom dictionary有对这个调整一些frequency,想请教下这个frequency是怎么设定的。谢谢～～～



"
HanLP,hankcs,283086716,word2vector功能中，加载词向量模型128维度超出限制,open,,https://github.com/hankcs/HanLP/issues/719,"
## 版本号
当前最新版本号是：portable-1.5.2
我使用的版本是：portable-1.5.2



## 我的问题
加载一个128维的词向量模型，抛出异常
java.lang.ArrayIndexOutOfBoundsException: 128


"
HanLP,hankcs,282774559,custom文件夹下的词典问题？比如上海、全国地名等等。,open,提问,https://github.com/hankcs/HanLP/issues/717,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

您好！不好意思打扰了。我现在正在使用hanlp，想请问下custom文件夹下的词典都是怎么来的？比如上海、全国地名等等。谢谢~


"
HanLP,hankcs,282305948,基于CRF的依存句法分析报OutOfMemoryError异常,open,,https://github.com/hankcs/HanLP/issues/716,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
我想自己训练一个依存句法分析器，看到你博客上面CRF提供提供了通过CRF++的训练方式，
我使用CRF++训练的一个150M左右的依存句法分析model，替换掉hanLP中的CRFDependencyModelMini.txt.bin，发现报如下错误：
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
	at com.hankcs.hanlp.model.crf.CRFModel.load(CRFModel.java:334)
	at com.hankcs.hanlp.dependency.CRFDependencyParser$CRFModelForDependency.load(CRFDependencyParser.java:234)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.loadDat(CRFDependencyParser.java:104)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.load(CRFDependencyParser.java:94)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.<init>(CRFDependencyParser.java:54)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.<init>(CRFDependencyParser.java:67)
	at com.hankcs.hanlp.dependency.CRFDependencyParser.compute(CRFDependencyParser.java:89)
	at dependency.HanLPDependency.main(HanLPDependency.java:10)
"
HanLP,hankcs,281612433,MutualInformationEntropyPhraseExtractor 实现建议,open,,https://github.com/hankcs/HanLP/issues/715,"我觉得短语提取增加训练语料更合适。如果仅凭输入的文档进行计算，互信息结果肯定不准（尤其是输入比较短的情况）。
实现起来也简单。先通过训练语料计算出有互信息的Occurrence对象。
在提取阶段使用训练好的Occurrence对象。如果没有训练，就使用原算法。"
HanLP,hankcs,281346204,时间识别不准确，CheckDateElements中的规则存在问题,open,,https://github.com/hankcs/HanLP/issues/714,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
使用NShortSegment分词器进行分词，时间识别不准确，存在以下问题：

1. 会将中英文逗号识别为/m类型
2. 如果中英文逗号后紧跟数字，分词时会将逗号和数字合并识别为/m类型
3. 九点这种可以识别为/t类型，但9点误识别为/m类型
4. 18:00这种格式的时间表述，会被误识别为/m类型

WordBasedGenerativeModelSegment中的CheckDateElements方法，相关规则建议修改
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        Segment segment = new NShortSegment();
        List<Term> list1 = segment.seg(""想出去玩，9点要上班，18:00下班,9成的人会加班。"");
        System.out.println(list1.toString());
        List<Term> list2 = segment.seg(""想出去玩但9点要上班，然后18:00下班,9成的人会加班。"");
        System.out.println(list2.toString());
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[想/v, 出去/vf, 玩/v, ，/w, 9点/t, 要/v, 上班/vi, ，/w, 18:00/t, 下班/vi, ,/w, 9成/m, 的/ude1, 人/n, 会/v, 加班/vi, 。/w]

[想/v, 出去/vf, 玩/v, 但/c, 9点/t, 要/v, 上班/vi, ，/w, 然后/c, 18:00/t, 下班/vi, ,/w, 9成/m, 的/ude1, 人/n, 会/v, 加班/vi, 。/w]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[想/v, 出去/vf, 玩/v, ，9点/m, 要/v, 上班/vi, ，18:00/m, 下班/vi, ,9成/m, 的/ude1, 人/n, 会/v, 加班/vi, 。/w]

[想/v, 出去/vf, 玩/v, 但/c, 9点/m, 要/v, 上班/vi, ，/m, 然后/c, 18:00/m, 下班/vi, ,9成/m, 的/ude1, 人/n, 会/v, 加班/vi, 。/w]

```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,281255288,1.51版本分词问题,open,提问,https://github.com/hankcs/HanLP/issues/713,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
1.51portable

当前最新版本号是：1.52
我使用的版本是：1.51-portable

## 我的问题
对于“无法”这个词，如果单独分词会分成“无”“法”，但是放到句子“当前订单无法派送”里面会分成“无法”，有没有方法让这两种情况得到同一个分词结果
"
HanLP,hankcs,280496621,项目打包data目录没打进去,open,提问,https://github.com/hankcs/HanLP/issues/712,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
1.mvn clean package -DskipTests 打包后data目录并没有被打进去，请问作者你们是怎么打包的？
2.我通过修改pom.xml文件的resources配置这种方式将data目录打进去了，但是在使用的时候依然报文件找不到的错误。

请指点下，非常感谢！


## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先执行 mvn clean package -DskipTests 
2. 然后查看target下的jar包内容 没有将 data打进去

"
HanLP,hankcs,280070508,nr.txt 中的数据是怎么来的,open,,https://github.com/hankcs/HanLP/issues/709,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,279612467,拼音切词,open,提问,https://github.com/hankcs/HanLP/issues/706,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.5.2
我使用的版本是：1.5.2



## 我的问题

你好，HanLP支持自定义拼音词库吗？因为我想使用拼音来进行切词，我们是通过语音识别转文字，识别的时候回有同音词，所以想通过拼音来切词


   CustomDictionary.add(""dakai"");
		CustomDictionary.add(""yiloumenkou"");
		CustomDictionary.add(""deng"");
		List<Term> list = StandardTokenizer.segment(""dakaiyiloumenkoudeng"");
		System.out.println(list);
### 期望输出

dakai yiloumenkou  deng

### 实际输出 dakaiyiloumenkoudeng

"
HanLP,hankcs,279404445,lucene-core-7.0.1与hanlp1.52 hanlp-index存在分词兼容问题,open,提问,https://github.com/hankcs/HanLP/issues/705,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.52
我使用的版本是：1.52

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
通过hanlp-ext 编译整合进elasticsearch6.0 做文档类型的分词映射，映射的分词是hanlp-index
elasticsearch6.0 文档映射部分代码如下
```
PUT test_2017_11_12 
{
    ""mappings"":
    {
        ""meal"":
        {
            ""properties"":
            {
                ""city"":
                {
                    ""type"": ""text"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 32
                        }
                    }
                },
                ""name"":
                {
                    ""type"": ""text"",
                    ""analyzer"": """"hanlp-index"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 256
                        }
                    }
                },
                ""address"":
                {
                    ""type"": ""text"",
                    ""analyzer"": ""hanlp-index"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 512
                        }
                    }
                },
                ""tags"":
                {
                    ""type"": ""text"",
                    ""analyzer"": """"hanlp-index"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 256
                        }
                    }
                },
                ""price"":
                {
                    ""type"": ""float""
                },
                ""phone"":
                {
                    ""type"": ""text"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 32
                        }
                    }

                },
                ""images"":
                {
                    ""type"": ""text"",
                    ""index"": false
                },
                ""recommendations"":
                {
                    ""type"": ""text"",
                    ""analyzer"": """"hanlp-index""
                },
                ""facilities"":
                {
                    ""type"": ""text"",
                    ""fields"":
                    {
                        ""keyword"":
                        {
                            ""type"": ""keyword"",
                            ""ignore_above"": 128
                        }
                    }
                },
                ""uriSign"":
                {
                    ""type"": ""text"",
                    ""index"": false
                },
                ""uri"":
                {
                    ""type"": ""text"",
                    ""index"": false
                },
                ""createTime"":
                {
                    ""type"": ""date"",
                    ""format"": ""epoch_millis""
                },
                ""updateTime"":
                {
                    ""type"": ""date"",
                    ""format"": ""epoch_millis""
                },
                ""syncTime"": 
                {
                    ""type"": ""date"",
                    ""format"": ""epoch_millis""
                }
            }
        }
    }
}
```
通过google搜索相关资料显示是hanlp-index 分词的方式与lucene-core-7.0.1存在兼容问题
DefaultIndexingChain.java:767  lucene 默认的索处理链文件
java.lang.IllegalArgumentException: startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards startOffset=1,endOffset=3,lastStartOffset=9 for field 'recommendations'
这是针对elasticsearch文档映射的字段

报错的Java错误堆栈信息如下
```
java.lang.IllegalArgumentException: startOffset must be non-negative, and endOffset must be >= startOffset, and offsets must not go backwards startOffset=1,endOffset=3,lastStartOffset=9 for field 'recommendations'
	at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:767) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:430) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:392) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:239) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:481) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1717) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1462) ~[lucene-core-7.0.1.jar:7.0.1 8d6c3889aa543954424d8ac1dbb3f03bf207140b - sarowe - 2017-10-02 14:36:35]
```
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先建立elasticsearch 文档映射
2. 然后通过elasticsearch存储文reset文档接口存储文档
3. 接着elasticsearch日志报错，错误堆栈已经写在文档上，见上文

### 触发代码

```
 PUT test_2017_11_12/meal/5a098a920f3f766cdebf4767
{ 
    ""city"" : ""长沙"", 
    ""name"" : ""盟重烧烤"", 
    ""address"" : ""冬瓜山裕南街85号"", 
    ""tags"" : [
        ""书院路"", 
        ""其它""
    ], 
    ""priceText"" : ""68/人"", 
    ""price"" : 68, 
    ""phone"" : ""15116157770"", 
    ""images"" : [
        ""http://test.com/host/2017/07/30/1501392633001415.jpg-ssq75""
    ], 
    ""recommendations"" : [
        ""星城最正宗的湘西味烤串"", 
        ""主打牛油由湘西直接供货"", 
        ""时尚潮人聚集的深夜食堂""
    ], 
    ""facilities"" : [
        ""Wi-Fi"", 
        ""适合小聚""
    ], 
    ""uriSign"" : ""cd5fb379260756ae50ac9ceb1c2e2331"", 
    ""uri"" : ""http://test/hotel/203381"", 
    ""syncTime"" : 1510669453830, 
    ""createTime"" :1510584054604, 
    ""updateTime"" : 1510584054604
}

```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,279287549,语料有了，如何重新训练CrfSegment  model,open,提问,https://github.com/hankcs/HanLP/issues/704,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
想重新训练crf分词模型，相关wiki里提示用 
..\..\crf_learn  -f 3 -c 4.0 template pku_training.bmes.txt model -t
但是不会用，上面这个语句在哪里执行，什么环境？多谢！！

另外，看了一些issue也有同学提出比如将信息熵、互信息或tfidf加入到特征中，也有论文介绍将embedding的特征融入到crf模型中 ，如 Revisiting Embedding Features for Simple Semi-supervised Learning ，如果要加入这些特征，请问修改哪些类及方法可以实现，多谢指导 ！！
"
HanLP,hankcs,278903867,基于神经网络的句法分析器问题,open,,https://github.com/hankcs/HanLP/issues/702,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：
最新版本
<!--以上属于必填项，以下可自由发挥-->

你好，nndepparser的代码是不是不包括训练部分的代码呢？我没找到相应的函数，谢谢了



"
HanLP,hankcs,277613914,Python下测试NLP分词功能时出现异常“java.lang.ExceptionInInitializerError”,open,提问,https://github.com/hankcs/HanLP/issues/698,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在Python下测试NLP分词时出现异常“java.lang.ExceptionInInitializerError”，其他方式测试正常
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
代码如下：
NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer')
print(NLPTokenizer.segment('中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程'))




"
HanLP,hankcs,277279994,portable版本比完整版好 ,open,提问,https://github.com/hankcs/HanLP/issues/697,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

使用默认标准分词, 发现maven引用portable版本的分词效果比下载下jar并挂载了数据包的好

## 复现问题

在同样的语料上进行分词测试(sighan bakeoff05的msr和pku)一下就行

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

起码分词结果一致吧? 或者更新数据包

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,277244571,标点符号,open,,https://github.com/hankcs/HanLP/issues/695,"请问如何关闭 标点符号变更？
输入的是中文标点符号，变成了英文标点，而且有一些特殊的也变了，比如（变成了《，并且英文标点的词性都是m"
HanLP,hankcs,276994572,Word2Vec训练1G的语料出现java.lang.OutOfMemoryError: Java heap space,open,,https://github.com/hankcs/HanLP/issues/694,训练过程中没有出现问题，但训练到100%后会发生该错误，不晓得能不能优化一下，使得Word2Vec能支持更大的语料的训练（其实感觉1G应该不算很大）
HanLP,hankcs,276672369,人名识别错误,open,改进,https://github.com/hankcs/HanLP/issues/692,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
人名识别错误

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
未修改任何源代码

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
//        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""韩国呢""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
韩国/nsf 呢/y
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
粗分词网：
0:[ ]
1:[韩, 韩国]
2:[国]
3:[呢]
4:[ ]

粗分结果[韩国/nsf, 呢/y]
人名角色观察：[  K 1 A 1 ][韩国 K 55 X 28 L 4 ][呢 L 29 D 1 ][  K 1 A 1 ]
人名角色标注：[ /K ,韩国/X ,呢/D , /A]
识别出人名：韩国呢 XD
细分词网：
0:[ ]
1:[韩国, 韩国呢]
2:[]
3:[呢]
4:[ ]

韩国呢/nr
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,276547977,N-最短路分词 分词结果词性标注错误,open,,https://github.com/hankcs/HanLP/issues/691,"## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：portable-1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
待分词的文本内容：
""今天，刘志军案的关键人物,山西女商人丁书苗在市二中院出庭受审。""
分词结果：
[今天/t, ，/m, 刘志军/nr, 案/ng, 的/ude1, 关键人物/nz, ,/m, 山西/ns, 女/b, 商人/nnt, 丁书苗/nr, 在/p, 市二中/n, 院/n, 出庭/vi, 受审/vi, 。/w]
问题：
中文逗号和英文逗号的词性标注错误，应该为w词性

## 复现问题
未做其它改动，直接使用项目README.md中的N-最短路分词的demo测试


### 触发代码

```
	public static void main(String[] args) {
		Segment nShortSegment = new NShortSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
		System.out.println(nShortSegment.seg(""今天，刘志军案的关键人物,山西女商人丁书苗在市二中院出庭受审。""));
	}
```


"
HanLP,hankcs,276259554,1.3.4版本运行大批量语料分词吃内存严重,open,,https://github.com/hankcs/HanLP/issues/690,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.3.4

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
首先感觉hanlp很靠谱，觉得是一个非常好的NLP开源项目，我拿来对语料库（大约8KW的数据集）做分词，发现非常吃内存，这块我自定义的词典里面加入了大约120W左右的用户自定义词典。随着程序的运行，内存使用量攀高，但是CPU使用率基本没有什么变化。而且随着时间的增长，内存使用量越来越大，逼近整个电脑的峰值，最后不得不终止。请问楼主在测试大语料分词的时候，是否有这种情况发生？谢谢

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先我把自己自定义词典加入到myDictionary.txt中，大约120W左右
2. 然后我写了一个thrift服务，服务端采用CRFSegment与Hanlp.newSegment()两种方式测试，都发现吃内存比较严重。随着时间的增长，内存使用越来越多。当程序处理了大约1000万的数据的时候，发现我的电脑扛不住了，我的电脑是8G的内存，发现内存基本上被占满了。

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,276257416,如何在程序中重新加载自定义的词典,open,提问,https://github.com/hankcs/HanLP/issues/689,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：
我使用的版本是：hanlp-1.3.2-portable.jar  、 hanlp-lucene-plugin-1.1.2.jar


## 实际场景
一个web容器中有两个webapp，一个是webappA，另一个是solr，solr使用hanlp为中文分词器，并配置了用户词典（即customDictionaryPath属性），webappA有一个在线编辑词典的功能，希望编辑完字典，solr能够看到效果而不需要重启tomcat容器。

## 解决思路
在hanlp solr插件中的HanLPTokenizerFactory开启一个守护线程，每隔一段时间去检查字典的校检码，如果发生变化就删掉.bin缓存文件，并重新加载字典。

## 我的问题
1、我目前在CustomDictionary添加了如下一个静态方法，但是这样会把所有的自定义词典重新加载一遍，有没有只加载某个文件的方法呢
```
  public static void reloadDic(){
    	trie = null;
    	dat = new DoubleArrayTrie<CoreDictionary.Attribute>();
    	loadMainDictionary(path[0]);
    }
```
2、执行CustomDictionary.insert()方法后，为什么新词典已经产生效果，但dat.size()没有发生变化
"
HanLP,hankcs,276251268,P2P和C2C这种词没有分出来，希望加到主词库,open,提问,https://github.com/hankcs/HanLP/issues/688,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

我希望“P2P”和“C2C”这些词能够分成一个词，但是现在是分成了多个。

## 复现问题


### 步骤


### 触发代码

```
    public void testIssue1234() throws Exception
    {
       Segment segment=HanLP.newSegment().enableCustomDictionary(true);
        List<Term> terms=segment.seg(""P2P C2C"");
        System.out.println(terms);
    }
```
### 期望输出
P2P/n  C2C/n


### 实际输出
[P/nx, 2/m, P/nx,  /w, C/nx, 2/m, C/nx]

## 其他信息


"
HanLP,hankcs,276001956,HanLP的二元文法词典如何使用?,open,,https://github.com/hankcs/HanLP/issues/687,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.2
我使用的版本是：1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
我在data\dictionary\CoreNatureDictionary.ngram.txt文件中发现了""琐碎@事情 2"",  '琐碎'和'事情' 是合理的接续.我不知道运行之后是出现什么样的效果
### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
       String rawText = ""琐碎的事情"";
	List<Term> termList = StandardTokenizer.segment(rawText);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

[琐碎事情]
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

[琐碎, 的, 事情]

## 其他信息
不知道我这样的想法是不是错误的.还是我哪里操作有问题
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,275940835,请问分词的时候能识别时间吗？如11月22日,open,,https://github.com/hankcs/HanLP/issues/686,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,275733475,自定义词性 + 自定义词条，与 portable 自带词典冲突,open,提问,https://github.com/hankcs/HanLP/issues/685,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号 
当前最新版本号是：portable-1.5.2
我使用的版本是：portable-1.5.2
备注：web项目，基于 springboot 1.5.8.RELEASE，docker 部署

## 代码复现
我想用 HanLP 匹配语句中的国家城市信息，国家城市名字是规定好的，在库里有一套，在 app 启动时，将库里的国家城市数据刷入 CustomDictionary 中，代码如下：
```java
/**
   * 将国家城市名添加到用户词典
   */
  @Override
  public void addCountryAndCityNameIntoDict() {
    //添加国家
    List<Country> countries = countryRepository.findAll();
    for (Country country : countries) {
      boolean countryInvalid = country.getId() == null || StringUtils.isEmpty(country.getCnName())
              || StringUtils.isEmpty(country.getEnName());
      if (countryInvalid) {
        continue;
      }
      CustomDictionary.insert(country.getCnName(), ""country 1000000"");
      CustomDictionary.insert(country.getEnName().toLowerCase(), ""country 1000000"");
      //根据国家，添加城市
      List<City> cities = cityRepository.findByCountryId(country.getId());
      for (City city : cities) {
        boolean cityInvalid = city.getId() == null || StringUtils.isEmpty(city.getCnName())
                || StringUtils.isEmpty(city.getEnName());
        if (cityInvalid) {
          continue;
        }
        CustomDictionary.insert(city.getCnName(), ""city 1000000"");
        CustomDictionary.insert(city.getEnName().toLowerCase(), ""city 1000000"");
      }
    }
  }
```
城市名称示例：
```
---------------------
  cn_name   en_name  
---------------------
  上海       Shanghai 
  北京       Beijing     
  杭州       Hangzhou 
  台北       Taibei    
  海南       Kainan    
  重庆       Zhongqing
---------------------
```
然后我试了几个分词器，分词结果都是以 portable 版本自带词典的词语为主：
```java
@GetMapping(""/test"")
  public Result testHanLP(@RequestParam String query) {
    //转换格式
    query = HanLP.convertToSimplifiedChinese(query.toLowerCase());

    Map<String, String> data = new LinkedHashMap<>(7);

    Segment standardTokenizer = StandardTokenizer.SEGMENT.enableCustomDictionaryForcing(true).enablePlaceRecognize(false);
    data.put(""标准分词"", standardTokenizer.seg(query).toString());

    data.put(""NLP分词"", NLPTokenizer.segment(query).toString());

    Segment indexTokenizer = IndexTokenizer.SEGMENT.enableCustomDictionaryForcing(true).enablePlaceRecognize(false);
    data.put(""索引分词"", indexTokenizer.seg(query).toString());

    Segment nShortSegment = new NShortSegment().enableCustomDictionaryForcing(true).enablePlaceRecognize(false);
    data.put(""N-最短路径分词"", nShortSegment.seg(query).toString());

    Segment shortSegment = new DijkstraSegment().enableCustomDictionaryForcing(true).enablePlaceRecognize(false);
    data.put(""最短路径分词"", shortSegment.seg(query).toString());

    return Results.SUCC.data(data);
  }
```
测试语句：海南省北京市上海市重庆市台湾岛台湾省台北市北京是首都海南是旅游胜地
分词效果：
```json
{
  ""code"": 0,
  ""msg"": ""成功"",
  ""data"": {
    ""标准分词"": ""[海南省/ns, 北京市/ns, 上海市/ns, 重庆市/ns, 台湾岛/nz, 台湾省/ns, 台北市/ns, 北京/city, 是/v, 首都/n, 海南/city, 是/v, 旅游/vn, 胜地/n]"",
    ""NLP分词"": ""[海南省/ns, 北京市/ns, 上海市/ns, 重庆市/ns, 台湾岛/nz, 台湾省/ns, 台北市/ns, 北京/city, 是/v, 首都/n, 海南/city, 是/v, 旅游/vn, 胜地/n]"",
    ""索引分词"": ""[海南省/ns, 海南/ns, 北京市/ns, 北京/ns, 上海市/ns, 上海/ns, 重庆市/ns, 重庆/ns, 台湾岛/nz, 台湾/ns, 台湾省/ns, 台湾/ns, 台北市/ns, 台北/ns, 北京/city, 是/v, 首都/n, 海南/city, 是/v, 旅游/vn, 胜地/n]"",
    ""N-最短路径分词"": ""[海南省/ns, 北京市/ns, 上海市/ns, 重庆市/ns, 台湾岛/nz, 台湾省/ns, 台北市/ns, 北京/city, 是/v, 首都/n, 海南/city, 是/v, 旅游/vn, 胜地/n]"",
    ""最短路径分词"": ""[海南省/ns, 北京市/ns, 上海市/ns, 重庆市/ns, 台湾岛/nz, 台湾省/ns, 台北市/ns, 北京/city, 是/v, 首都/n, 海南/city, 是/v, 旅游/vn, 胜地/n]""
  }
}
```

### 期望输出
目前来看，索引分词还算凑合，我期望的是：
```
[海南省/ns, 海南/city, 北京市/ns, 北京/city, 上海市/ns, 上海/city, 重庆市/ns, 重庆/city, 台湾岛/nz, 台湾/city, 台湾省/ns, 台湾/city, 台北市/ns, 台北/city, 北京/city, 是/v, 首都/n, 海南/city, 是/v, 旅游/vn, 胜地/n]
```

### 提问
- 我该如何在用官方 portable jar 包的前提下，得到预期的输出？毕竟自己打包，然后放在项目下，不太优雅（我现在没有自己的 maven 仓库，如果自己打包的话，只能放在项目下了）。
- 在使用官方 portable jar 包时，能否提供一个禁用其自带的某些词典的方法？感觉这是一种解决思路。
- 我有考虑借助 hanlp.properties 和 自定义 data 目录，但是这样对于一个 web 项目来说，部署时就不太方便。
"
HanLP,hankcs,275337272,常见官职识别出错,open,改进,https://github.com/hankcs/HanLP/issues/683,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：master
我使用的版本是：master

<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
常见官职如“外交部长”识别出错
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
Segment segment = HanLP.newSegment().enableNameRecognize(true).enableOrganizationRecognize(true);
List<Term> termList = segment.seg(""国防部长王毅向记者介绍此访情况。"");
System.out.println(termList);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[国防部长/n, 王毅/nr, 向/p, 记者/nnt, 介绍/v, 此/rzs, 访/v, 情况/n, 。/w]
```
[国防部长/n, 王毅/nr, 向/p, 记者/nnt, 介绍/v, 此/rzs, 访/v, 情况/n, 。/w]
### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[国防部/nt, 长/a, 王毅/nr, 向/p, 记者/nnt, 介绍/v, 此/rzs, 访/v, 情况/n, 。/w]
```
## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->
总体感觉master不够稳定
"
HanLP,hankcs,275273955,自定义词的优先级以及自定义词性的问题,open,提问,https://github.com/hankcs/HanLP/issues/682,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：portable-1.5.0
我使用的版本是：portable-1.5.0

## 我的问题
1. 自定义词典中，词性可以任意定吗。比如我新创造一种 ""ssr"" 的词性，特指我自己的词
2. 使用了自定义词典，并且打开了自定义词典优先的开关。但是仍然无法匹配出想要的结果。

## 复现问题

### 触发代码

```
		String rawText = ""攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰"";
		// 原始分词效果： [攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走/v,
		// 上/f, 人生/n, 巅峰/n]

		// 测试1:
		CustomDictionary.insert(""狮逆"", ""ssr 20000"");
		CustomDictionary.insert(""狗，迎"", ""ssr 2000"");
		CustomDictionary.insert(""走上"", ""ssr 2000"");
		StandardTokenizer.SEGMENT.enableCustomDictionaryForcing(true);
		System.out.println(HanLP.segment(rawText));

		// 效果：
		// [攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗，迎/ssr, 娶/v, 白富美/nr, ，/w, 走上/ssr, 人生/n, 巅峰/n]
```
### 期望输出

[攻城/vi, 狮逆/ssr, 袭/vi, 单身/n, 狗，迎/ssr, 娶/v, 白富美/nr, ，/w, 走上/ssr, 人生/n, 巅峰/n]

可以看出， ""狗，迎"" 以及 ""走上"" 这种硬生生添加的词被识别出来了。
但是 “ 狮逆” 没有匹配出来。
请问是词性上有限制？ 还是说需要修改核心字典什么的？ 
https://github.com/hankcs/HanLP/issues/393 跟这个issue有关吗。portable版本不是很想单独修改文件
谢谢！



"
HanLP,hankcs,275232680,NShortSegment分词算法实现问题,open,提问,https://github.com/hankcs/HanLP/issues/681,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v1.5.2
我使用的版本是：v1.5.2

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
1） 为什么NShortSegment分词默认只选择第1条路径进行后续分析和处理，那它前面求出至少2条最短路径的意义何在？（文件NShortSegment.java第101行：List vertexList = coarseResult.get(0)）
2） 为什么NShortSegment分词定死nKind = 2？是为了什么考虑的？


## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
例如：我使用NShortSegment进行分词“他说的确实在理。”

虽然中间得到两条分词结果：
coarseResult = [[ , 他, 说, 的, 确实, 在, 理, 。, ], [ , 他, 说, 的, 确实, 在理, 。, ]]

但是后面默认使用第一个结果进行后续分析（第101行：List vertexList = coarseResult.get(0)），导致实际输出就是第一个分词结果。

那么这样的话，NShortSegment分词出两种分词结果，有什么意义呢？第二个分词结果后面根本就没用？而且个人认为第二个分词结果稍微好一些（路径更短）。

另外，为什么NSegment.java中定死nKind=2，即变成2-最短路径。这个nKind定死为2是出于分词效率考虑吗？是不是可以让用户来确定nKind的具体取值？
### 步骤

程序运行如下。

### 触发代码

```
   	public void testSegment() {
		Segment nShortSegment = new NShortSegment().enableCustomDictionary(true).enablePlaceRecognize(true).enableOrganizationRecognize(true);
		List<Term> term1 = nShortSegment.seg(""他说的确实在理。"");
		System.out.println(term1);
	}
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->
```
[ , 他, 说, 的, 确实, 在理, 。, ]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[ , 他, 说, 的, 确实, 在, 理, 。, ]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,272538358,请问，为什么不支持分词结果最小颗粒度？,open,提问,https://github.com/hankcs/HanLP/issues/670,"
## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.0
我使用的版本是：1.3.5
<!--以上属于必填项，以下可自由发挥-->

## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
请问，为什么不支持分词结果最小颗粒度。
在lucene场景中创建更丰富的索引，以便支持更丰富的检索场景。
我翻阅了部分代码和文档发现，以及使用作为
com.hankcs.lucene.HanLPTokenizerFactory
索引分词器。需要分词的短语为:“弹簧床”，得到的分词结果为：“弹簧”+“床”。
那么检索器无论怎样设置，是无法通过：“弹”，或者相关检索字词查询到结果？

刚接触搜索相关知识，如有不认识不周的，请包涵。


"
HanLP,hankcs,271271890,1.5.0 新词发现特性OutOfMemory，discovery函数读取语料字符串，是否可以提供流式管道作为参数的discovery函数,open,改进,https://github.com/hankcs/HanLP/issues/667,"<!--
注意事项和版本号必填，否则不回复。若希望尽快得到回复，请按模板认真填写，谢谢合作。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.5.0
我使用的版本是：1.5.0

<!--以上属于必填项，以下可自由发挥-->

## 我的问题
使用1.5.0版本新增加的特性：新词发现。输入语料文件大小90M，出现OutOfMemory

## 复现问题
		NewWordDiscover nd = new NewWordDiscover(4, 0.00005f, .6f, 0.2f, true);
		try {
			List<com.hankcs.hanlp.mining.word.WordInfo> newWords =nd.discovery(this.read(""/content/discuss_content.txt""), 100);
			
			System.out.println(newWords);
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

### 步骤

1. 首先，构造NewWordDiscover对象，参数：4, 0.00005f, .6f, 0.2f, true
2. 然后，从文件中读取语料，语料文件约90M
```
	private String read(String fileName) throws IOException {
		StringBuffer result = new StringBuffer();
		BufferedReader  br = new BufferedReader(new FileReader(new File(fileName)));
		
		String s = null;
		while((s = br.readLine())!=null){
            result.append(System.lineSeparator()+s);
        }
		br.close(); 
		return result.toString();
	}
```
3. 接着，运行10分钟左右后报OutOfMemory

### 触发代码
```
    private static void increaseFrequency(char c, Map<Character, int[]> storage)
    {
        int[] freq = storage.get(c);
        if (freq == null)
        {
            freq = new int[]{1};
            storage.put(c, freq);  // <-- 触发OutOfMemory
        }
        else
        {
            ++freq[0];
        }
    }
```
### 期望输出

正常输出新词结果，不报错

### 实际输出
抛出OutOfMemory异常

## 其他信息
JVM内存大小设置：-Xmx4096M -Xms4096M

## 建议
期望在语料较大时，能通过流读取语料文件，防止一次性加载撑爆内存

"
HanLP,hankcs,270926134,解析txt格式的个人求职简历,open,,https://github.com/hankcs/HanLP/issues/666,"楼主好，有个问题想请教下，我现在有这么个需求：
需要**解析txt格式的个人求职简历**
首先我通过分词来获取求职者的姓名、性别、住址、邮箱等基本信息没有什么问题，
**但是现在需要提取出简历中的求职者的项目工作经验，这个好像很不好搞，没找到好的办法能够完整的提取出项目经验的信息来，楼主有啥办法帮忙指导下么？**
感谢！！！"
HanLP,hankcs,270912665,把代码中的System.exit替换成RuntimeException异常,open,,https://github.com/hankcs/HanLP/issues/665,"在使用过程中我发现，在遇到异常情况的时候代码基本都采用System.exit来强制退出JVM，这样给HanLP使用带来很多不方便的地方。

分词出错仅仅是HanLP库的错误，不能让整个应用退出，应用的退出应该交给应用程序的开发者来决定。

建议定义一个或多个RuntimeException的子类，然后在HanLP遇到致命错误的时候抛出这个异常，而不是直接退出JVM给应用开发造成不便。"
HanLP,hankcs,270176763,能否支持观点抽取,open,提问,https://github.com/hankcs/HanLP/issues/663,"@hankcs    你好   
我想了解一下HanLP能否支持观点抽取
比如:＂服务态度很好，环境也不错，就是点歌系统不太好用。＂
正向观点 : 服务态度很好 环境不错
负向观点 : 点歌系统落后
"
HanLP,hankcs,269864446,在spark中使用分词器有些问题,open,提问,https://github.com/hankcs/HanLP/issues/662,"当前最新版本号是：1.3.5
我使用的版本是：1.3.4


## 我的问题
在spark中使用分词器时，报找不到词典！
请问怎样让程序加载放在hdfs上的data目录下的文件，或者说您有没有分词器在分布式计算框架中的一些好的实践？谢谢
"
HanLP,hankcs,269861128,CQueue.java 两个double型数据直接比较大小？,open,提问,https://github.com/hankcs/HanLP/issues/661,"原代码：

第32行：
```java
 while (pCur != null && pCur.weight < newElement.weight)
{
            pPre = pCur;
            pCur = pCur.next;
}
```

下面这一句：
```
 pCur.weight < newElement.weight
```
是否应该改成：```Double.compare(pCur.weight, newElement.weight) < 0```

"
HanLP,hankcs,269411690,可以先分词，后翻译吗?,open,改进,https://github.com/hankcs/HanLP/issues/660,"## 版本号

当前最新版本号是：1.3.5
我使用的版本是：1.3.5

## 我的问题
HanLP 分词正确，可是繁簡转换结果错误，查找一下，发现 HanLP 好像其中用了 OpenCC 同样词库，所以两边有共同错误，不知是否可以以分词来对比词库?

比如转换 ""计算发现"" 到繁体:
https://github.com/BYVoid/OpenCC/issues/272

https://github.com/BYVoid/OpenCC/issues/224"
HanLP,hankcs,269017275,如何让书名号内的词语不进行切分？用CustomDictionary.insert() ，不一定能起作用，麻烦。有没有有一个方法，能强制性给予不切分该词？,open,提问,https://github.com/hankcs/HanLP/issues/659,如何让书名号内的词语不进行切分？用CustomDictionary.insert() ，不一定能起作用，麻烦。有没有有一个方法，能强制性给予不切分该词？求指教。
HanLP,hankcs,267432566,对于提取不同的词，选取哪种分词方法回更好？,open,提问,https://github.com/hankcs/HanLP/issues/657,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [√ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：jar包-1.3.4
我使用的版本是：jar包-1.3.4


## 我的问题

您好！
我现在需要对一篇300字左右的新闻进行关键词提取，关键词的分别为：
```
时间
社会地点（eg：医院、学校）
自然地点（eg：江苏、杭州）
社会地位（eg：省长、总书记）
身份（eg：孕妇、医生）
人名
行为动词（主要是v和vi，eg：报告、安慰）
```
经过尝试以后发现不同的分词方法对于相同的文本会有不同的分词结果，例如有些就会识别更精准的行政机构，而有些则相比较差，因为对于NLP接触时间并不是很长，所以对于各种分词原理了解并不是很清晰。
请问您能否对于这些不同的关键词提取，提供在HanLP现存分词方法中，较好的分词方式，即对于不同的词的提取，采取最好的分词方式？
谢谢！"
HanLP,hankcs,267432115,运行Demo中的DemoCustomNature.java出现NoSuchMethodError错误,open,求助,https://github.com/hankcs/HanLP/issues/656,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ √] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：jar包-1.3.4
我使用的版本是：jar包-1.3.4


## 我的问题
您好！
我在运行demo中的DemoCustomNature.java时，出现了在下面“实际输出”中的报错。
操作系统是archlinux，java版本1.9和1.8都尝试过，均出现报错。
参考过[#279](!https://github.com/hankcs/HanLP/issues/279#ref-issue-170997234)的问题，但是无法解决。
请问如何才能正常运行代码？谢谢！

## 复现问题
直接运行了demo

### 触发代码
demo代码中的DemoCustomNature.java

### 实际输出

```
n
null
10月 22, 2017 11:08:47 上午 com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
警告: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!
如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类
Exception in thread ""main"" java.lang.NoSuchMethodError: sun.reflect.ReflectionFactory.newConstructorAccessor(Ljava/lang/reflect/Constructor;)Lsun/reflect/ConstructorAccessor;
	at com.hankcs.hanlp.corpus.util.EnumBuster.findConstructorAccessor(EnumBuster.java:255)
	at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:92)
	at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:68)
	at com.hankcs.hanlp.corpus.util.CustomNatureUtility.addNature(CustomNatureUtility.java:58)
	at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:829)
	at demo.DemoCustomNature.main(DemoCustomNature.java:41)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at com.intellij.rt.execution.application.AppMainV2.main(AppMainV2.java:131)

```"
HanLP,hankcs,267135284,像“挺好”，“没想到”等在字典文件CoreNatureDictionary.txt文件中都被标注成了nz,open,重复,https://github.com/hankcs/HanLP/issues/655,"## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题

像“挺好”，“没想到”等在字典文件CoreNatureDictionary.txt文件中都被标注成了nz，但是实际上它们通常都不是专有名词。对词性分析时造成影响。

## 复现问题

例如，“真是没想到呀。”就会得出“没想到/nz”，“这个东西挺好”就会得出“挺好/nz”。

"
HanLP,hankcs,266005853,ViterbiSegment 中粗切算法有些疑问,open,,https://github.com/hankcs/HanLP/issues/652,"ViterbiSegment.viterbi(WordNet wordNet) 
这个算法我大致看了， 但感觉不像维特比算法， 我看算法中只是从后向前取了每个词的到前面那个词的最距离最短的那个词。

比如  ""我爱汉语处理""  ，先取得处理， 接着从处理中取得
‘’汉语‘’ 而不是 ‘语’ ， 因为他们俩的距离最短。  接着再从 “汉语” 取到 '爱' 而不是“我爱” 因为他们俩的距离最短。 

但是维特比算法不是要考虑到所有前面的最短路径， 而我看这个算法只是取了当前词的到前面那个词最短路径所对应词。
"
HanLP,hankcs,265669396,可以有自定义停词库吗,open,,https://github.com/hankcs/HanLP/issues/651,最新版本可以使用自定义词库，不知道是否支持自定义停词库。试了一下直接在配置文件中停词path后面加路径不支持
HanLP,hankcs,265228735,CRFDependencyParser.compute出现bug，有两个词互为对方的HEAD,open,提问,https://github.com/hankcs/HanLP/issues/650,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.3.4
我使用的版本是：hanlp-1.2.8-sources


## 我的问题

在使用CRFDependencyParser.compute(sentence)时，依存关系出现了环的问题，比如“不少iPhone铁杆粉丝选择跳过iPhone8系列，等待iPhoneX的上市。”，结果如下（[word.ID, word.NAME, word.HEAD.ID, word.DEPREL]）：
1	未##数	2	数量
2	iphone	3	限定
3	铁杆	4	限定
4	粉丝	6	受事
5	选择	6	并列
6	跳过	7	限定
7	iphone8	8	限定
8	系列	10	受事
9	未##串	10	受事
10	等待	13	限定
11	iphonex	10	内容
12	的	11	“的”字依存
13	上市	10	内容
      结果中""等待""和""上市""互为HEAD，这应该是不正常的，这是咋回事儿捏？
    （我只是在自定义词库中加了iphone, iphone8, iphonex这些词）

"
HanLP,hankcs,264517592,自定义词典读取失败,open,,https://github.com/hankcs/HanLP/issues/648,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.2.8


## 我的问题
我的配置文件里只改变了root路径，经测试无误
但是自定义分词时添加了自己的词典my.txt后总是出现警告读取失败不能显示分词结果，当在自定义词典路径里不加入my.txt时能显示出默认分词结果

## 复现问题
未改变任何已有词典和模型，只添加了自己的词典（UTF-8）编码
且每次都删掉了缓存文件
### 步骤
首先按照步骤把配置文件内容复制好，更改root路径，在自定义词典路径处添加了my.txt(已注意了前面的空格和后面的分号)

### 触发代码
import java.util.List;
import com.hankcs.hanlp.HanLP;
import com.hankcs.hanlp.seg.Segment;
import com.hankcs.hanlp.seg.common.Term;

public class hanlp
{  public static void main (String[] args)
	{
       String  testline=""测试自定义分词词分义定自试测"";
       Segment segment=HanLP.newSegment().enableCustomDictionary(true);
       
       List<Term>termlist =segment.seg(testline);
       for(Term term:termlist)
       {
    	   System.out.println(term.toString());
       }
	}
}

### 期望输出


### 实际输出

无分词结果
 
提示：十月 11, 2017 5:28:24 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadDat
警告: 读取失败，问题发生在java.lang.ArrayIndexOutOfBoundsException: 1056291
	at com.hankcs.hanlp.utility.ByteUtil.bytesHighFirstToInt(ByteUtil.java:239)
	at com.hankcs.hanlp.corpus.io.ByteArray.nextInt(ByteArray.java:68)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadDat(CustomDictionary.java:323)
	at com.hankcs.hanlp.dictionary.CustomDictionary.loadMainDictionary(CustomDictionary.java:66)
	at com.hankcs.hanlp.dictionary.CustomDictionary.<clinit>(CustomDictionary.java:53)
	at com.hankcs.hanlp.seg.Segment.combineByCustomDictionary(Segment.java:199)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:57)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:498)
	at hanlp.main(hanlp.java:12)


当删去自定义词典路径里my.txt 时输出
测试/vn
自定义/nz
分词/n
词/n
分/qt
义定/nr
自/p
试/v
测/v

自定义词典my.txt的内容：
试测"
HanLP,hankcs,264151935,你好，我将我自己训练好的文件放到替换了原有模型文件，我的模型在crf++中有97的正确率，但是在这里只有30，也是4tag，我不知道是模型加载出现了问题，还是解码过程中出现了问题,open,,https://github.com/hankcs/HanLP/issues/646,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,263830574,咨询HanLP与LTP的关系，及License相关问题,open,提问,https://github.com/hankcs/HanLP/issues/644,"Hi，
  请问该项目和LTP是什么关系，哪些是移植，哪些是重新编写呢？如果商用，是否需要联系LTP取得授权呢？
  因为看到其他issue中提到依存句法分析模型训练是通过LTP训练出的。

THX

"
HanLP,hankcs,263123555,人民日报的标注预料库是人工的还是机器做的呢？,open,,https://github.com/hankcs/HanLP/issues/643,"
## 我的问题

人民日报的标注预料库是人工的还是机器做的呢？

"
HanLP,hankcs,263032148,python调用HanLP.parseDependency的时候报错,open,,https://github.com/hankcs/HanLP/issues/642,"
报错信息：jpype._jexception.LinkageErrorPyRaisable: java.lang.ExceptionInInitializerError，求助怎么解决
"
HanLP,hankcs,261810482,java web项目中data文件夹和hanlp.properties应该放在哪里,open,,https://github.com/hankcs/HanLP/issues/640,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题
请问如何在Java web项目中放置data文件夹和hanlp.properties呢？应该放在哪里才能加载到
"
HanLP,hankcs,261663365,可以增加针对elasticsearch的支持吗,open,,https://github.com/hankcs/HanLP/issues/639,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

能针对elasticsearch 5.x做支持吗

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,261162467,“中国移动通信集团” 和 “中国移动” 2个词的分词词序不一样,open,,https://github.com/hankcs/HanLP/issues/638,"当前最新版本号是：c57895f14801df54312e8b656ea8ac2eeef72f99
我使用的版本是：c57895f14801df54312e8b656ea8ac2eeef72f99

## 我的问题
中国移动通信集团 和 中国移动 2个词的分词词序不一样，导致solr完全匹配搜索的时候搜不到数据

## 复现问题

### 触发代码

```
        List<Term> termList = IndexTokenizer.segment(""中国移动通信集团"");
        for (Term term : termList)
        {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }
        List<Term> termList2 = IndexTokenizer.segment(""中国移动"");
        for (Term term : termList2) {
            System.out.println(term + "" ["" + term.offset + "":"" + (term.offset + term.word.length()) + ""]"");
        }

```
### 实际输出

```
中国移动通信集团/nt [0:8]
中国移动通信/nz [0:6]
中国移动/nz [0:4]
中国/ns [0:2]
移动通信/nz [2:6]
移动/vn [2:4]
通信/vn [4:6]
集团/nis [6:8]

中国移动/nz [0:4]
中国/ns [0:2]
移动/vn [2:4]
```

”中国移动通信集团“的分词结果中 “中国”和“移动”之间多了一个“移动通信”，导致和“中国移动”的分词结果顺序不匹配。这样在solr中使用text:""中国移动""这种全词匹配模式搜索的话，是搜不到数据的

之前因为词序的事情已经麻烦过你了，经过上次的修改，分词的词序已经趋于稳定，但还有少数有问题，这个就是一个特例，希望能够得到修正，再次感谢！
"
HanLP,hankcs,259991278,DoubleArrayTrie中的一个bug,open,提问,https://github.com/hankcs/HanLP/issues/636,"/**
     * 精确查询
     *
     * @param keyChars 键的char数组
     * @param pos      char数组的起始位置
     * @param len      键的长度
     * @param nodePos  开始查找的位置（本参数允许从非根节点查询）
     * @return 查到的节点代表的value ID，负数表示不存在
     */
    public int exactMatchSearch(char[] keyChars, int pos, int len, int nodePos)
    {
        int result = -1;

        int b = base[nodePos];
        int p;

#如果len是键的长度那么，循环应该是  for (int i = pos; i < pos+len; i++)
        for (int i = pos; i < len; i++)   

        {
            p = b + (int) (keyChars[i]) + 1;
            if (b == check[p])
                b = base[p];
            else
                return result;
        }"
HanLP,hankcs,259423720,分词疑问：“钱管家中怎么绑定网银”,open,改进,https://github.com/hankcs/HanLP/issues/633,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题
分词疑问，添加了自定义词，但是没有分词出来，而是人名识别依然起效果，但感觉人名识别也不对。

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""钱管家"");
        System.out.println(HanLP.segment(""钱管家中怎么绑定网银""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[钱管家/n,  中/, 怎么/ryv, 绑定/gi, 网银/n]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[钱管/nr, 家中/s, 怎么/ryv, 绑定/gi, 网银/n]
```

## 其他信息

从分词结果看，`钱管`分词为一个人名了，添加自定义分词后，没有任何影响。
然后，我关闭人名识别功能，发现还是没有作用，必须要向CoreNatureDictionary.ngram.txt中
添加
```
钱管家@中 10
```
并且，将`钱管家`添加到CoreNatureDictionary.txt中才行。



"
HanLP,hankcs,259019157,Hanlp使用,open,提问,https://github.com/hankcs/HanLP/issues/630,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.3.4
我使用的版本是：portable-1.3.4


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
在使用Hanlp分词的时候发现效果不错，所以想应用到某一个特定领域（例如新闻）。苦于对整个工程的原理和工程实现不是很清楚。作者能否出个专题（或者书籍）对广大READER介绍一下语料库的收集，使用，处理；模型的训练，调优测试；以及后续的维护等主题

@hankcs 
"
HanLP,hankcs,258754100,分词过程结果的不理解,open,,https://github.com/hankcs/HanLP/issues/628,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题
我在调试标准分词和N-short最短路分词的过程中遇到如下问题：
String testSentence = ""重庆邮电大学是一所工科大学，坐落在美丽的南山上。这里是数字通信发源地。"";
        //标准分词
        List<Term> standList = HanLP.segment(testSentence);
        System.out.println(""标准分词："" + standList);
输出结果是：
标准分词：[重庆邮电大学/ntu, 是/vshi, 一所/n, 工科大学/l, ，/w, 坐落/vi, 在/p, 美丽/a, 的/ude1, 南山/ns, 上/f, 。/w, 这里/rzs, 是/vshi, 数字通信/nz, 发源地/n, 。/w]
“重庆邮电大学”这个专有名词是在机构名词典里面的，标准分词并没有开启机构名词典这个模型，输出结果应该是“重庆”+“邮电大学”。当在机构名词典去掉“重庆邮电大学”这个词语后，分词结果和预想的一样，是“重庆”+“邮电大学”。说明调用标准分词的时候用到了机构名词典里面的数据，这和代码逻辑不一样，麻烦帮忙解答一下"
HanLP,hankcs,258407089,读取crf模型的时候得到的权重值不要人工去设定；具体位置在第一个字不可能M或者E,open,提问,https://github.com/hankcs/HanLP/issues/627,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

事例：U00:%x[-2,0]
U01:%x[-1,0]
U02:%x[0,0]
U03:%x[1,0]
U04:%x[2,0]
U05:%x[-1,0]/%x[0,0]
U06:%x[0,0]/%x[1,0]
"
HanLP,hankcs,256171240,句子”仍有很长的路要走“分词错误,open,改进,https://github.com/hankcs/HanLP/issues/622,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 句子”仍有很长的路要走“分词错误

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
没有修改

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        System.out.println(HanLP.segment(""仍有很长的路要走""));
    }
```
### 期望输出

```
[仍/d, 有/vyou, 很长/d, 的/ude1, 路/n, 要/v, 走/v]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[仍/d, 有/vyou, 很长/d, 的/ude1, 路要/nr, 走/v]

```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

## 分析
从结果看，好像是人名识别影响了。
但这个句子，不应该有人名的，这里错误将 路要 分成了人名了。

"
HanLP,hankcs,256121555,时间能否作为一个整体识别出来呢？,open,,https://github.com/hankcs/HanLP/issues/621,"在分词的时候我发现时间都是分开了，比如说2017年9月8日，分词结果为：
2017 年 9 月 8 日 
能不能把他们作为一个整体识别出来呢？而不是分开的，可以在用户词典里面加入正则表达式来把时间作为一个整体进行分割吗？"
HanLP,hankcs,255892340,机构名识别出后添加到词网中， 未##团 词性读取错乱。,open,,https://github.com/hankcs/HanLP/issues/620,"<!-- 请详细描述问题，越详细越可能得到解决 -->
机构名识别，
已经识别出的机构名创建新的节点并 inset 到词网中，
根据 未##团 获得的 ATTRIBUTE读取错乱，
以下是 debug 截图，和我根据词频搜索出的唯一结果，

![31caf6d28e7006c994a7a6fc554953c](https://user-images.githubusercontent.com/26315728/30159424-19cfb804-93fb-11e7-9e55-aa2e3809785e.png)

可以看到，词频顺序是对的，词性错乱了。
删除缓存文件后问题得到解决。
这种情况偶尔发生。
当然，我修改了代码，而且有点多。
能想到的就是在读取核心词典的时候错乱了，
希望给出建议。
谢谢。

"
HanLP,hankcs,255347167,pku_training.bmes.txt这个文件在哪里？,open,,https://github.com/hankcs/HanLP/issues/619,"使用这个命令时，`crf_learn  -f 3 -c 4.0 template pku_training.bmes.txt model -t`
需要 pku_training.bmes.txt文件，但是项目里没有，是从哪里可以找到么"
HanLP,hankcs,254879491,算法人名识别有点奇怪？,open,,https://github.com/hankcs/HanLP/issues/618,"他叫小明却能切出小明。
![default](https://user-images.githubusercontent.com/17618881/30004018-4e074046-90fa-11e7-8a8e-993775e333c0.png)

小萍就不对，切成小 萍。
![default](https://user-images.githubusercontent.com/17618881/30004013-286dc738-90fa-11e7-8107-67c5aeb6b0e2.png)

小花却又错。

我如何自定义一下，但我不可能所有的名字有小 #什么的人名都定义的，我如何让所有叫@人## 就能匹配到小 后面的全部人名都切对？    

"
HanLP,hankcs,254323145,增加核心数据，到40多M后，大概是100万条词条后，出现 java.lang.OutOfMemoryError: Java heap space,open,,https://github.com/hankcs/HanLP/issues/617,"你好，本人来自北京航空航天大学一名学生，发现程序可能存在的一个BUG，想请教一下。为何增加核心数据，到40多M后，大概是100万条词条后，出现 java.lang.OutOfMemoryError: Java heap space
![default](https://user-images.githubusercontent.com/17618881/29923940-c1204f9c-8e8d-11e7-887f-0422be6bcfe3.png)

请问你是否那里写错？JAVA应该不会这么烂的，渴望能得到你的回答。谢谢
"
HanLP,hankcs,254191561,hanlp计算同义词相似度中，同一类别的词相似度为什么不高,open,,https://github.com/hankcs/HanLP/issues/616,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题
我在计算同一类中出现的两个词之间的相似度时，本以为相似度会很高，而实际计算结果却不理想。
<!-- 请详细描述问题，越详细越可能得到解决 -->
比如说，我计算“孤”和“寡人”两个词的相似度时，发现相似度不够，所以我有点疑问，当一个词存在多个类别时，对于该类别选择的机理是什么?
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
String[] wordArray = new String[]
            {
                ""孤"", ""寡人""
            };
        System.out.printf(""%-5s\t%-5s\t%-10s\t%-5s\n"", ""词A"", ""词B"", ""语义距离"", ""语义相似度"");
        for (String a : wordArray) {
            for (String b : wordArray) {
                System.out.printf(""%-5s\t%-5s\t%-15d\t%-5.10f\n"", a, b, CoreSynonymDictionary.distance(a, b), CoreSynonymDictionary.similarity(a, b));
            }
        }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
词A   	词B   	语义距离      	语义相似度
孤    	孤    	0              	1.0000000000
孤    	寡人   	0              	1.0000000000
寡人   	孤    	0              	1.0000000000
寡人   	寡人   	0              	1.0000000000
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
词A   	词B   	语义距离      	语义相似度
孤    	孤    	0              	1.0000000000
孤    	寡人   	28319444208    	0.6188445149
寡人   	孤    	28319444208    	0.6188445149
寡人   	寡人   	0              	1.0000000000
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,253925853,你好，请问这个项目有没有聊天机器人的子项目或者改为聊天机器人的计划和项目推荐吗？,open,,https://github.com/hankcs/HanLP/issues/615,你好，请问这个项目有没有聊天机器人的子项目或者改为聊天机器人的计划和项目推荐吗
HanLP,hankcs,253861370,能够将识别出来的机构名称进行提取吗？,open,提问,https://github.com/hankcs/HanLP/issues/614,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 我的问题
您好！我在使用您的HanLP的时候发现机构命名识别率很高。
但是有什么好的方法将识别出来的机构名称提取出来？





"
HanLP,hankcs,253100018,核心词典CoreNatureDictionary.txt加载失败,open,,https://github.com/hankcs/HanLP/issues/612,"

在使用hanlp分词时，一直显示核心词典CoreNatureDictionary.txt加载失败，仔细确认过hanlp.properties中的路径，加载了相关jar文件，仍然无法执行程序。 
"
HanLP,hankcs,252799433,关于license问题，hankcs 你好，我有一个分词项目，使用了hanLP分词算法和代码,open,提问,https://github.com/hankcs/HanLP/issues/611,"hankcs 你好，我有一个分词项目，使用了hanLP分词算法和代码。

主要进行了大量的重构和结构的调整，修改面积过大没法提交pull 

如果也在github上开源，是否可以

如果可以，在license怎么声明，因为我看之前您的文档写明了版权和上海林源公司有关，是不是影响license选择"
HanLP,hankcs,252150529,添加了类序列化接口，可支持spark集群调用,open,,https://github.com/hankcs/HanLP/pull/608,"<!--
感谢你对开源事业的贡献！这是一份模板，方便记录你做出的功绩，谢谢！
-->

## 注意事项

* 这次修改没有引入第三方类库。
* 也没有修改JDK版本号
* 所有文本都是UTF-8编码
* 代码风格一致
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 解决了什么问题？带来了什么好处？

<!-- 你的补丁解决了什么问题，给大家带来了什么好处？ -->
添加了类序列化接口，可以在Spark集群上并行化使用HanLP；
使用方法:
1. 在com.hankcs.hanlp.utility.Config中修改放置词典的hdfs路径
2. 重新打jar包
3. 调用方法
```
import com.hankcs.hanlp.Config;
import com.hankcs.hanlp.seg.Viterbi.ViterbiSegment;
import com.hankcs.hanlp.corpus.io.IIOAdapter
import java.io._;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import java.net.URI;
 
class HadoopFileIoAdapter extends IIOAdapter {
    @Override
    def open(path: String): java.io.InputStream = {
        val conf: Configuration = new Configuration();
        val fs: FileSystem = FileSystem.get(URI.create(path), conf);
        fs.open(new Path(path));
    }   
 
    @Override
    def create(path: String): java.io.OutputStream = {
        val conf: Configuration = new Configuration();
        val fs: FileSystem = FileSystem.get(URI.create(path), conf);
        fs.create(new Path(path));
    }
}

val a = sc.parallelize(Seq(""互联网搜索算法也成为当今的热门方向。"",""算法工程师逐渐往人工智能方向发展。""))
val test = a.map(e=> {
    Config.IOAdapter = new HadoopFileIoAdapter(); 
    val vs = new ViterbiSegment();
    vs.seg(e)
    })

test take 10
```


## 相关issue

<!-- 如果跟已有issue相关的话，麻烦列一下 -->
#588 "
HanLP,hankcs,251541149,怎样拿取匹配到的所有词性？,open,,https://github.com/hankcs/HanLP/issues/605,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：v1.3.4
我使用的版本是：v1.3.4


## 我的问题
我自定义了一个词性“深圳”为 shenzhen，而深圳的基础词库里的词性是 ns 地名词性，我代码里拿取深圳的词性只能拿到 shenzhen 词性了，有办法能拿到深圳这个词所匹配的所有词性吗？

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
       System.out.println(HanLP.segment(""深圳""));
        CustomDictionary.insert(""深圳"",""shenzhen 1"");
        System.out.println(HanLP.segment(""深圳""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[深圳/ns,/shenzhen]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[深圳/ns]
[深圳/shenzhen]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,251220734,OrganizationRecognize,open,提问,https://github.com/hankcs/HanLP/issues/604,"请教一下，开启OrganizationRecognize后为什么要重新生成切分图？ 我使用自定义词典的词，在这次重新生成后就失效了。

"
HanLP,hankcs,251167258,root 写相对路径但是读取文件一直有异常,open,,https://github.com/hankcs/HanLP/issues/603,"我使用的版本是：hanlp-1.3.4    master

我的问题
我把写好的程序集成到web系统上，data放在了web/asset/HanLP-master下面，有看到一个issue里提到的，所以我写了System.out.println(new File(""."").getAbsolutePath());输出的是D:\jboss\jboss-4.2.2.GA - FJ\bin\.   我写成root=../assets/HanLP-master/是不行的，那该怎么写呢

 期望输出
读取data进行分词

实际输出
读取../assets/HanLP-master/data/dictionary/CoreNatureDictionary.txt.bin时发生异常java.io.FileNotFoundException:




"
HanLP,hankcs,249306822,关于唐诗人名的简繁、拼音体转换以及convertToPinyinString函数的建议,open,改进,https://github.com/hankcs/HanLP/issues/602,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.3.4
我使用的版本是：portable-1.3.4


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

1. 唐诗诗人人名繁体转简体出现乱码
2. 某些人名似乎可不转
3.多音字人名不正确
4.convertToPinyinString函数建议


 

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 下载jar，加入工程，运行代码
2. 然后从 https://github.com/chinese-poetry/chinese-poetry 获得json
3. 接着转换json，打印诗人人名、简体、拼音

### 触发代码

```
     Log.d(TAG, HanLP.convertToSimplifiedChinese(""裴諴""));
    Log.d(TAG, HanLP.convertToSimplifiedChinese(""拾得""));
    List<Pinyin> pinyinList = HanLP.convertToPinyinList(""曾扈"") ;
    String pyname="""";
    for (Pinyin pinyin : pinyinList)
    {
      pyname+= pinyin.getPinyinWithToneMark()+"" "";
    }
    Log.d(TAG, pyname);
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
裴諴 拾得 zeng hu
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
裴��   十得 céng hù 
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

另外不知道为什么convertToPinyinString(String text, String separator, boolean remainNone)这么设计，能不能加个convertToPinyinString(String text, int start,int len)? 谢谢。"
HanLP,hankcs,249303933,自定义词典没有起到作用？,open,,https://github.com/hankcs/HanLP/issues/601,"
## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
 我是通过maven直接引入的
   <dependency>
            <groupId>com.hankcs</groupId>
            <artifactId>hanlp</artifactId>
            <version>portable-1.3.4</version>
   </dependency>

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题

自定义词典不起作用

## 复现问题

自定义词典不起作用

### 步骤

```
我在配置文件hanlp.properties中加入了
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt;data/dictionary/custom/CustomDictionary/自定义词典.txt
```
自定义词典格式如下：

```
罗氏婴儿配方粉 n 1000
挂花大头菜 n 1000
黄毛籽 n 1000
青豆 n 1000
儿童营养饼干 n 1000
汤菜 n 1000
青萝卜 n 1000
```

分词结果：

### 触发代码

```
String str = ""罗氏婴儿配方粉是什么?"";
        CoNLLSentence sentence = new NeuralNetworkDependencyParser().enableDeprelTranslator(false).parse(str);
        // 可以方便地遍历它
        for (CoNLLWord word : sentence) {
            System.out.printf(""%s--%s --(%s)--> %s\n"",word.ID, word.LEMMA, word.DEPREL, word.CPOSTAG);
        }
```
### 期望输出

```
1--罗氏婴儿配方粉 --(SBV)-->n
2--是 --(HED)--> v
3--什么 --(VOB)--> r
4--? --(WP)--> wp
```

### 实际输出

```
1--罗氏 --(ATT)--> nz
2--婴儿 --(ATT)--> n
3--配方 --(ATT)--> n
4--粉 --(SBV)--> a
5--是 --(HED)--> v
6--什么 --(VOB)--> r
7--? --(WP)--> wp
```



"
HanLP,hankcs,248638155,1.3.4版本自定义词典发现问题,open,无效,https://github.com/hankcs/HanLP/issues/599,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.2.8


## 我的问题
当我使用1.3.4版本的时候，在hanlp.properties中添加自定义词典的路径（只写一个CustomDictionaryPath配置，其他都不写），就会报出找不到核心词典的错误，当换到1.2.8版本之后，同样的配置文件使用起来没有任何问题。不知道是我配置的原因，还是本身项目的bug
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先
我的配置文件内容为：
root=F:/hanlp/
CustomDictionaryPath=data/dictionary/custom/xxx.txt;
2. 然后
运行程序会报出
警告: 读取F:/hanlp/data/dictionary/CoreNatureDictionary.mini.txt.bin时发生异常java.io.FileNotFoundException: F:\hanlp\data\dictionary\CoreNatureDictionary.mini.txt.bin (系统找不到指定的文件。)
八月 08, 2017 4:20:46 下午 com.hankcs.hanlp.dictionary.CoreDictionary load
警告: 核心词典F:/hanlp/data/dictionary/CoreNatureDictionary.mini.txt不存在！java.io.FileNotFoundException: F:\hanlp\data\dictionary\CoreNatureDictionary.mini.txt (系统找不到指定的文件。)
八月 08, 2017 4:20:46 下午 com.hankcs.hanlp.dictionary.CoreDictionary <clinit>
严重: 核心词典F:/hanlp/data/dictionary/CoreNatureDictionary.mini.txt加载失败
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,248617262,能不能借阅一下你们生成CRFSegmentModel.txt.bin原文件CRFSegmentModel.txt,open,提问,https://github.com/hankcs/HanLP/issues/598,"请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ √] 我在此括号内输入x打钩，代表上述事项确认完毕。

 1、我现在试着想加入词性，重新构造特征模版，可不可以参考一下你们的
"
HanLP,hankcs,247268917,关键词提取可能会把自定义词性的词全部过滤掉,open,重复,https://github.com/hankcs/HanLP/issues/595,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
关键词提取时会去掉停用词，即通过 KeywordExtractor的shouldInclude()方法，
任何自定义词性如果首字母满足switch条件，都会被作为停用词去掉，无法提取任何自定义词性的词汇作为关键词。我在复现步骤提供一个简单的case。
shouldInclude 的代码如下:
```
public boolean shouldInclude(Term term)
    {
        // 除掉停用词
        if (term.nature == null) return false;
        String nature = term.nature.toString();
        char firstChar = nature.charAt(0);
        switch (firstChar)  
        {
            case 'm':
            case 'b':
            case 'c':
            case 'e':
            case 'o':
            case 'p':
            case 'q':
            case 'u':
            case 'y':
            case 'z':
            case 'r':
            case 'w':
            {
                return false;
            }
            default:
            {
                if (term.word.trim().length() > 1 && !CoreStopWordDictionary.contains(term.word))
                {
                    return true;
                }
            }
            break;
        }

        return false;
    }
```
## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->


### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""我是自定义的词"", ""mZiDingYi 1111"");
        String msg = ""测试一下,我是自定义的词我是自定义的词我是自定义的词我是自定义的词,测试一下"";
        System.out.println(HanLP.segment(msg));
        System.out.println(TextRankKeyword.getKeywordList(msg, 100));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[测试/vn, 一下/m, ,/w, 我是自定义的词/mZiDingYi, 我是自定义的词/mZiDingYi, 我是自定义的词/mZiDingYi, 我是自定义的词/mZiDingYi, ,/w, 测试/vn, 一下/m]
[测试, 我是自定义的词]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[测试/vn, 一下/m, ,/w, 我是自定义的词/mZiDingYi, 我是自定义的词/mZiDingYi, 我是自定义的词/mZiDingYi, 我是自定义的词/mZiDingYi, ,/w, 测试/vn, 一下/m]
[测试]
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,246712730,在提取关键词的过程中，根据词性过滤得出的关键词,open,,https://github.com/hankcs/HanLP/issues/594,"在提取关键词的过程中，我想根据词性过滤，只留下词性为名词的关键词。
我的想法是在分词的步骤中，就按照词性去过滤，只留下名词，但是在标准分词源码部分，并没有找到关于词性的代码，hankcs能否指点一下小弟呢？不胜感激
"
HanLP,hankcs,246636296,分词粒度是否可以配置,open,重复,https://github.com/hankcs/HanLP/issues/593,"hanlp的分词粒度可否配置？
比如""团购网站的本质是什么？""能否通过配置粒度，分词结果保存不同粒度的结果“团购网站/团购/团购网/网站/的/本质/是/什么” "
HanLP,hankcs,246257198,Wiki 项目介绍请帮忙修改一下,open,改进,https://github.com/hankcs/HanLP/issues/591,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

衍生项目中，之前我描述的不准确，**应该将 (HanLP 自然语言处理 for nodejs)  改成 (HanLP Docker Image: 自然语言处理)**。

https://github.com/hankcs/HanLP/wiki/%E8%A1%8D%E7%94%9F%E9%A1%B9%E7%9B%AE

![image](https://user-images.githubusercontent.com/3538629/28705638-52d03156-73a3-11e7-8111-83bdd253211a.png)

"
HanLP,hankcs,244601728,在Spark单机使用正常，map到节点上使用出现错误,open,求助,https://github.com/hankcs/HanLP/issues/588,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4

@hankcs 您好，按照您说的方法，修改配置
root=hdfs://localhost:9000/hanlpdata/
IOAdapter=com.xxx.HDFSIOAdapter
而且在scala重写了接口：
class HadoopFileIoAdapter extends IIOAdapter {
    @Override
    def open(path: String): java.io.InputStream = {
        val conf: Configuration = new Configuration();
        val fs: FileSystem = FileSystem.get(URI.create(path), conf);
        fs.open(new Path(path));
    }    

    @Override
    def create(path: String): java.io.OutputStream = {
        val conf: Configuration = new Configuration();
        val fs: FileSystem = FileSystem.get(URI.create(path), conf);
        fs.create(new Path(path));
    }
}
HanLP.Config.IOAdapter = new HadoopFileIoAdapter();

也只能在单机spark下运行分词
scala> System.out.println(HanLP.segment(""你好，欢迎使用HanLP汉语处理包！""));
[你好/vl, ，/w, 欢迎/v, 使用/v, HanLP/nx, 汉语/gi, 处理/vn, 包/v, ！/w]


分发到节点的时候就出错了，请问你们在集群上使用的时候是否遇到过这种问题？需要注意哪些坑呢？
val a = sc.parallelize(Seq(""中国的神威太湖之光计算机被用于天气预报"",""制药研究和工业设计等领域。""))
val res = a.map(e=>{
        HanLP.segment(e).toString
    })
res take 2 foreach println

Exit code: 255
Stack trace: ExitCodeException exitCode=255:
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:578)
	at org.apache.hadoop.util.Shell.run(Shell.java:489)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:755)
	at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:297)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)"
HanLP,hankcs,243681080,停止词中添加控制字符的问题,open,提问,https://github.com/hankcs/HanLP/issues/584,"
## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。


当前最新版本号是：1.3.4
我使用的版本是：1.3.4

我希望在停用词中添加 ""\t""  这样的控制字符 ,因为切分 ""运动用途	用法""(中间是""\t"")这个词的时候结果是:
运动/vn
用途/n
	/nx
法/n

不知道为什么把制表符认为成了字母专名.
然后我再stopword.txt中添加""	"" 当读取停止词词典就报下边的错:

严重: 载入停用词词典D:/Idea_workplace/HanLP/data/dictionary/stopwords.txt失败java.lang.StringIndexOutOfBoundsException: String index out of range: 0
	at java.lang.String.charAt(String.java:658)
	at com.hankcs.hanlp.collection.MDAG.MDAG.replaceOrRegister(MDAG.java:541)
	at com.hankcs.hanlp.collection.MDAG.MDAG.<init>(MDAG.java:220)
	at com.hankcs.hanlp.collection.MDAG.MDAG.<init>(MDAG.java:170)
	at com.hankcs.hanlp.collection.MDAG.MDAGSet.<init>(MDAGSet.java:42)
	at com.hankcs.hanlp.dictionary.stopword.StopWordDictionary.<init>(StopWordDictionary.java:44)
	at com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary.<clinit>(CoreStopWordDictionary.java:42)
	at com.hankcs.demo.DemoCustomDictionary.main(DemoCustomDictionary.java:79)

不知道是不是我的用法有问题?

感谢大神的开源,方便我很多  :)"
HanLP,hankcs,243290300,对“训练HMM-NGram分词模型”产生的问题和理解,open,提问,https://github.com/hankcs/HanLP/issues/582,"对你在 [训练分词模型](https://github.com/hankcs/HanLP/wiki/%E8%AE%AD%E7%BB%83%E5%88%86%E8%AF%8D%E6%A8%A1%E5%9E%8B) 中提到的HMM-NGram分词模型产生了几个问题

1、HMM是由输出量求输入量的模型，请问这里的分词输出量是什么，是指分词的序列吗？那输入量又是什么呢？

HanLP中使用了一阶隐马模型，在这个隐马尔可夫模型中，隐状态是词性，显状态是单词。见[词性标注](http://www.hankcs.com/nlp/part-of-speech-tagging.html)


2、训练后一共得出3个文件：

CoreNatureDictionary.txt：单词词性词典
CoreNatureDictionary.ngram.txt：二元接续词典
CoreNatureDictionary.tr.txt：词性转移矩阵

请问这几个字典的作用？你能简单地用最短分词结合上面的字典做一个梳理吗，即是如何分词，又是如何做词性标注的？

我的理解：

CoreNatureDictionary.txt和CoreNatureDictionary.ngram.txt是用来分词的，CoreNatureDictionary.tr.txt是用来做词性标注的。在S-segment中，先通过动态规划，即用字典CoreNatureDictionary.txt和CoreNatureDictionary.ngram.txt生成词图（动态规划路径图），然后选择最短的路径。这里只是用到了MM，并没有用到HMM，属于机械式规则+统计的分词方法。
而CoreNatureDictionary.tr.txt正如问题1的说的，根据输入的分词序列来判断词性的序列。

有资料中说到，分词方法的演进，可分为
1、机械式规则
2、规则+统计
3、MM+Viterbi
4、由字构词
5、神经网络

我认为3其实也是2，MM首先也要将句子进行动态规划进行分词（机械规则），也就是查字典，先将句子中所有可以查到的词（查核心字典），先划分出来，然后按时词到词出现的顺序（二元连续字典）生成路径的权重，也就是构造词图。"
HanLP,hankcs,242575042,英文数量词的识别问题,open,提问,https://github.com/hankcs/HanLP/issues/581,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.3


## 我的问题

在开启数量词识别后，英文的数量没有识别

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 触发代码

```
Segment segment = HanLP.newSegment()
              .enablePlaceRecognize(true)
              .enableIndexMode(false)
              .enableOrganizationRecognize(true)
              .enableNumberQuantifierRecognize(true);
System.out.println(segment.seg(""18k金项链""));
System.out.println(segment.seg(""18k""));
System.out.println(segment.seg(""18kg""));
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[18k/mq, 金项链/nz]
[18k/mq]
[18kg/mq]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[18/m, k/nx, 金项链/nz]
[18/m, k/nx]
[18/m, kg/nx]
```

## 其他信息

我在CoreNatureDictionary.txt中添加了下面记录后倒是得到了想要的结果
```
k q 1000
kg q 1000
```

问题是为什么核心词典里没有这些词的配置，是有什么特殊原因吗？？？

"
HanLP,hankcs,242273640,ngram词典重新加载,open,改进,https://github.com/hankcs/HanLP/issues/580,"## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是： 1.3.4
我使用的版本是： 1.3.3


## 我的问题

现在ngram词典很多标识的不是很准确，而且也有很多缺失的（当然，很大一部分的原因是因为我们业务需求的问题），所以现在打算人工修改词典，然后做一个ngram词典的热更新，问题是：

**如果要重新加载ngram词典，是否删除缓存然后调用com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary#load这个方法即可？**




"
HanLP,hankcs,240869766,请问一下，您这里的自动摘要模块支持英文吗,open,提问,https://github.com/hankcs/HanLP/issues/576,请问一下，您这里的自动摘要模块支持英文吗
HanLP,hankcs,238762763,[Not a bug] HanLP API Docker Image,open,改进,https://github.com/hankcs/HanLP/issues/571,"各位

我封装了一个docker images，可以用来快速开始使用HanLP API，尤其是在跨语言和平台上，集成HanLP服务。

http://nlp.chatbot.io/public/index.html

快速开始: https://hub.docker.com/r/samurais/hanlp-api/"
HanLP,hankcs,238052768,关于Android的使用,open,提问,https://github.com/hankcs/HanLP/issues/570,"你好，很感谢你们能提供如此优秀的开源库，我想询问一下关于安卓如何使用这个库， 我用的IDE是android studio,例如HanLP.properties应该放哪儿，感谢解答"
HanLP,hankcs,237745171,关于CharacterBasedGenerativeModel 原始语料无法找到的问题,open,讨论,https://github.com/hankcs/HanLP/issues/566,"基于2阶HMM 分词器 HMMSegment中使用到的模型，但是无法找到与之相对应的原始语料。还望大神能够提供与之相对应的原始语料方便我们学习和进行debug
"
HanLP,hankcs,237124062,hanlp支持从用户语料继续训练嘛？,open,提问,https://github.com/hankcs/HanLP/issues/564,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

我从文档上看到hanlp可以帮助用户训练自己的语料。那么假设我有自己的分词语料，我需要怎么去训练呢？
"
HanLP,hankcs,237113248,hanlp 能否加入一个远程词典更新的功能,open,求助,https://github.com/hankcs/HanLP/issues/563,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

我再使用 [elasticsearch-ik](https://github.com/medcl/elasticsearch-analysis-ik)的时候，发现这个远程更新的功能非常不错，也很简单，就是通过远程的一个 txt 文件，判断 ETag 是否更新，然后重新 reload 词典，我自己在使用的项目里面已经参考 ik 的机制，写了一个自动更新的功能。

BTW: CustomDictionary 里面能不能加入一个 reload 的功能，我用远程更新的时候需要重新加载这个词典。我本地测试了一个直接把 CustomDictionary.BinTrie = null，运行过了一段时间，好像没有对象过多的内存泄露，但是没有详细测试过。

"
HanLP,hankcs,236122585,神经网络句法分析的时候加载有问题，CoNLLWord类 toString 方法打印两次名称,open,提问,https://github.com/hankcs/HanLP/issues/561,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [√] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题

采用NeuralNetworkDependencyParser.compute的进行句法分析的时候，默认会调用 jar 包的路径，会报错。需要先将 HanLP.Config.IOAdapter = null; 这样会从文件读取。而且CoNLLWord类的 toString 方法把词写了两次。

## 复现问题
### 步骤

### 触发代码

```
     CoNLLSentence sentence = NeuralNetworkDependencyParser.compute(""徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"");
        System.out.println(sentence);
        // 可以方便地遍历它
        for (CoNLLWord word : sentence) {
            System.out.printf(""%s --(%s)--> %s\n"", word.LEMMA, word.DEPREL, word.HEAD.LEMMA);
        }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
1	徐先生	nh	nr	_	4	主谓关系	_	_
```

### 实际输出

```
1	徐先生	徐先生	nh	nr	_	4	主谓关系	_	_
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,234720231,关于solr中添加hanlp的分词插件中，配置hanlp.properties的root相对路径,open,提问,https://github.com/hankcs/HanLP/issues/558,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-portable.jar1.3.4和hanlp-solr-plugin.jar1.1.2
我使用的版本是：hanlp-portable.jar1.3.4和hanlp-solr-plugin.jar1.1.2


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->
我是想将hanlp的分词插件加载到solr中来，先已经把hanlp的配置文件hanlp.properties放进来了，data包放到solr项目工程的webContent下了，如果root写的绝对路径是可以正常加载的，如果写成项目的相对路径，就无法找到路径，想请教一下，root可以配置成相对路径吗？该如何配置？，还是说data包不能放到solr项目工程下的webContent下？我看了一下里面之前别人的提问 ，我看到说配置root路径里，是../的方式配置的，请大神解答，谢谢！
另，在solr中的schema文件里配置自定义词库时应该也可以写项目的相对路径吗？写法是不是和root配置是一致的？


## 触发代码
```
 直接启动solr项目
```


## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先将hanlp.properties文件放到了solr项目工程的src路径下，通过编译可生成classpath下
2. 然后在WebContent下新建了一个名为hanlp的文件夹
3. 接着将data包放到solr项目工程的WebContent中的hanlp下，
4. 最后修改hanlp.properties里的root路径，我想修改成相对路径，root=../项目名称/hanlp/

### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望可以通过相对路径找到data包
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出，无法载入data包
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,233406891,自定义的词性无法识别,open,提问,https://github.com/hankcs/HanLP/issues/553,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4

## 我的问题
在零配置的情况下，自定义字典的词性识别不正确

### 步骤
### 触发代码

```
    public static void main(String[] args) {
        CustomDictionary.insert(""别克"",""brand 1"");
        CustomDictionary.insert(""英朗"",""family 1"");

        String text1=""别克英朗"";
        String text2 = ""别克 英朗"";
        StandardTokenizer.SEGMENT.enableAllNamedEntityRecognize(false);

        List<Term> terms1 = StandardTokenizer.segment(text1);
        System.out.println(terms1);

        List<Term> terms2 = StandardTokenizer.segment(text2);
        System.out.println(terms2);
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
[别克/brand,  英朗/family]
[别克/brand,  /w, 英朗/family]
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
[别克英朗/nrf]
[别克/brand,  /w, 英朗/family]
```

## 其他信息



"
HanLP,hankcs,232436883,依存文法运行效率问题,open,提问,https://github.com/hankcs/HanLP/issues/551,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：1.3.4
我使用的版本是：1.3.4


## 我的问题
用scala做NLP的典型意见，期待效果如下：
![qq 20170531101801](https://cloud.githubusercontent.com/assets/10076707/26613211/3dd00be4-45ec-11e7-98f3-151ff9f90789.png)
我的解决方案：依存文法 + 提取核心、主谓、定中 + 形成意见
但是HanLP的依存文法运行 时间很长，spark上依存 500 条 记录需要40s，不能满足要求
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->
没有修改代码，调用：
``` scala
val sentenceWord = HanLP.parseDependency(sentence).word
```
只实现了HdfsIOAdapter：
``` scala
import java.io.{InputStream, OutputStream}
import com.hankcs.hanlp.corpus.io.IIOAdapter
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
/**
  * @author zzzhy
  *         2017-05-18
  */
object HdfsIOAdapter {
  private val conf = new Configuration()
}
class HdfsIOAdapter extends IIOAdapter {

  import HdfsIOAdapter._
  private val fs = FileSystem.newInstance(conf)

  override def create(path: String): OutputStream = {
    val fsDataOutputStream = fs.create(new Path(path))
    fsDataOutputStream.getWrappedStream
  }

  override def open(path: String): InputStream = {
    val fsDataInputStream = fs.open(new Path(path.replace(""\\"", ""/"")))
    fsDataInputStream.getWrappedStream
  }
}
```

### 触发代码

``` scala
val sentenceWord = HanLP.parseDependency(sentence).word
```

有何使用不当或者有何改进方式？
"
HanLP,hankcs,232211272,你训练人名识别是用了一年的人民日报的语料吗？2014年的吗？总共有多少篇文章？,open,,https://github.com/hankcs/HanLP/issues/550,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [ ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：
我使用的版本是：


## 我的问题

<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先……
2. 然后……
3. 接着……

### 触发代码

```
    public void testIssue1234() throws Exception
    {
        CustomDictionary.add(""用户词语"");
        System.out.println(StandardTokenizer.segment(""触发问题的句子""));
    }
```
### 期望输出

<!-- 你希望输出什么样的正确结果？-->

```
期望输出
```

### 实际输出

<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
实际输出
```

## 其他信息

<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,232153107,分词能否实现输出多个分词结果,open,提问,https://github.com/hankcs/HanLP/issues/549,"比如： 『购物就上天猫』  
这句话会用各种分词方法都是这样的结果：
[购物/vn, 就/d, 上天/vi, 猫/n]

天猫这个在自定义词典中，切词频设置很高也没有用。  
Hanlp能否实现把多种分词可能都输出， 比如上诉情况，返回两种结果：  
[[购物/vn, 就/d, 上天/vi, 猫/n],[购物/vn, 就/d, 上/v天猫/ntc]]"
HanLP,hankcs,230222308,HanLP.properties如何引用Java系统参数,open,提问,https://github.com/hankcs/HanLP/issues/539,"问题产生的情景是：我有一个使用到hanlp的webapp项目工程，需要部署到windows和linux等不同的系统环境中，但是HanLP配置文件中的路径每次都需要手动修改，我想可不可以使用系统参数来代替呢
例如：修改之前HanLP.properties中的配置是这样的
```
    root=D:/dic/
```

修改之后，期望可以引用jvm参数（-Dhanlp.dic=D:/dic/），
```
root=${hanlp.dic}
```
如果hanlp没有此功能，我应该怎么扩展呢，需要修改哪些代码？




"
HanLP,hankcs,229545358,生成CoreNatureDictionary.txt.bin的模块是哪个？,open,重复,https://github.com/hankcs/HanLP/issues/537,"   生成CoreNatureDictionary.txt.bin的模块是哪个？我用NatureDictionaryMaker来制作了自己的词典。但是用自己做的这个词典去进行分词是系统没有生成*.txt.bin 报错无法进行分词。
   我在做的不是中文，是小数民族文字。所以我想要是生成*.txt.bin 文件的模块有编码限制，对他进行修改。  
我在用的版本是1.2.8"
HanLP,hankcs,228592672,"""烤羊排""进行分词为""烤""和""羊排""",open,提问,https://github.com/hankcs/HanLP/issues/533,"问题
对词语""烤羊排""进行分词，
得到结果: 烤羊排 [nf]， 期望能把 烤 和 羊排 区分开

版本
程序版本号：hanlp-portable-1.2.7
数据版本号：data-for-1.3.2.zip

触发代码
`HanLP.segment(""烤羊排"");`

代码输出
`烤羊排 [nf]`

尝试
CoreNatureDictionary.txt中有 烤、羊排、烤羊排三个词语，我在词库CoreNatureDictionary.ngram.txt中增加 烤@羊排 2000，但是还是没有效果，麻烦帮忙看一下，非常感谢。"
HanLP,hankcs,228197403,整合LDA 主谓宾提取,open,,https://github.com/hankcs/HanLP/pull/532,"整合LDA 主谓宾提取 方便与其他分词一起使用
"
HanLP,hankcs,228175315,关于语义距离计算的问题,open,提问,https://github.com/hankcs/HanLP/issues/531,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：hanlp-1.3.3-release
我使用的版本是：hanlp-1.3.3-release


## 我的问题
在调用[语义距离](https://github.com/hankcs/HanLP#20-语义距离)和语义相似度方法时，出现语义相似度为0。
<!-- 请详细描述问题，越详细越可能得到解决 -->

## 复现问题
<!-- 你是如何操作导致产生问题的？比如修改了代码？修改了词典或模型？-->

### 步骤

1. 首先，我通过[关键词提取](https://github.com/hankcs/HanLP#14-关键词提取)获取了某个句子的关键词。例如，[程序员, 程序, 分为, 人员, 软件]。
2. 然后，我对获得的关键词进行语义距离和语义相似度的计算。计算结果中，与“程序员”一词相关的记录，语义距离为无穷，语义相似度为0。

新手，刚接触分词，以下是自己做出的一些尝试与猜想，如有描述不当，还请批评指正。
1.猜想需要添加“程序员”到自定义词典，尝试了动态增加词语“程序员”，结果无变化。
2.猜想在《同义词词林扩展版》中没有与“程序员”相关的记录，故无法计算语义距离和语义相似度。

请问，具体是哪一种问题导致一个词语对自己本身的语义相似度为0？对于相应的问题，我可以怎么解决，或参考什么样例去解决？

### 触发代码

```
String content = ""程序员(英文Programmer)是从事程序开发、维护的专业人员。一般将程序员分为程序设计人员和程序编码人员，但两者的界限并不非常清楚，特别是在中国。软件从业人员分为初级程序员、高级程序员、系统分析员和项目经理四大类。"";
List<String> keywordList = HanLP.extractKeyword(content, 5);
System.out.println(keywordList);
System.out.printf(""%-5s\t\t%-5s\t\t%-10s\t\t%-5s\n"", ""词A"", ""词B"", ""语义距离"", ""语义相似度"");
for (String a : keywordList)
{
	
    for (String b : keywordList)
    {
        System.out.printf(""%-5s\t\t%-5s\t%-15d\t%-5.10f\n"", a, b, CoreSynonymDictionary.distance(a, b), CoreSynonymDictionary.similarity(a, b));
    }
}
```
### 期望输出
(输出结果末尾加""*""为方便提示关键行)
<!-- 你希望输出什么样的正确结果？-->

```
词A   		词B   	语义距离      	语义相似度
程序员  		程序员  	0              	1.0000000000*
程序员  		程序   	(非无穷)		(非0)*
程序员  		分为   	(非无穷)		(非0)*
程序员  		人员   	(非无穷)		(非0)*
程序员  		软件   	(非无穷)		(非0)*
程序   		程序员  	(非无穷)		(非0)*
程序   		程序   	0              	1.0000000000
程序   		分为   	28631246094    	0.6146479284
程序   		人员   	21054522402    	0.7166241456
程序   		软件   	14295982706    	0.8075883064
分为   		程序员  	(非无穷)		(非0)*
分为   		程序   	28631246094    	0.6146479284
分为   		分为   	0              	1.0000000000
分为   		人员   	49685768496    	0.3312720740
分为   		软件   	42927228800    	0.4222362347
人员   		程序员  	(非无穷)		(非0)*
人员   		程序   	21054522402    	0.7166241456
人员   		分为   	49685768496    	0.3312720740
人员   		人员   	0              	1.0000000000
人员   		软件   	6758539696     	0.9090358392
软件   		程序员  	(非无穷)		(非0)*
软件   		程序   	14295982706    	0.8075883064
软件   		分为   	42927228800    	0.4222362347
软件   		人员   	6758539696     	0.9090358392
软件   		软件   	0              	1.0000000000
```

### 实际输出
(输出结果末尾加""*""为方便提示关键行)
<!-- HanLP实际输出了什么？产生了什么效果？错在哪里？-->

```
词A   		词B   		语义距离      	语义相似度
程序员  		程序员  	9223372036854775807	0.0000000000*
程序员  		程序   	9223372036854775807	0.0000000000*
程序员  		分为   	9223372036854775807	0.0000000000*
程序员  		人员   	9223372036854775807	0.0000000000*
程序员  		软件   	9223372036854775807	0.0000000000*
程序   		程序员  	9223372036854775807	0.0000000000*
程序   		程序   	0              		1.0000000000
程序   		分为   	28631246094    		0.6146479284
程序   		人员   	21054522402    		0.7166241456
程序   		软件   	14295982706    		0.8075883064
分为   		程序员  	9223372036854775807	0.0000000000*
分为   		程序   	28631246094    		0.6146479284
分为   		分为   	0              		1.0000000000
分为   		人员   	49685768496    		0.3312720740
分为   		软件   	42927228800    		0.4222362347
人员   		程序员  	9223372036854775807	0.0000000000*
人员   		程序   	21054522402    		0.7166241456
人员   		分为   	49685768496    		0.3312720740
人员   		人员   	0              		1.0000000000
人员   		软件   	6758539696     		0.9090358392
软件   		程序员  	9223372036854775807	0.0000000000*
软件   		程序   	14295982706    		0.8075883064
软件   		分为   	42927228800    		0.4222362347
软件   		人员   	6758539696     		0.9090358392
软件   		软件   	0              		1.0000000000
```

## 其他信息
补充一下自己同时结合调用两种方法（[语义距离](https://github.com/hankcs/HanLP#20-语义距离)和[关键词提取](https://github.com/hankcs/HanLP#14-关键词提取)）的初衷：
我期望通过输入关键词，计算关键词与句子的关联度（以下简称 关联度），以达到“关键词-句子-关联度”的结果。
而在计算关联度时，我计划：
先提取句子关键词，如，[程序员, 程序, 分为, 人员, 软件]；
再输入关键词，如，“程序员”，“编程”等，计算输入的关键词与句子关键词之间的相似度；
最后利用关键词的相似度，从一定的程度反映关联度。

以上是我目前期望的需求，如果作者有遇到或解决过类似问题，还请多多指教。
谢谢！
<!-- 任何可能有用的信息，包括截图、日志、配置文件、相关issue等等。-->

"
HanLP,hankcs,227904238,需要怎么处理不去加载内部的角色组和列表,open,提问,https://github.com/hankcs/HanLP/issues/529,"<!--
这是HanLP的issue模板，用于规范提问题的格式。本来并不打算用死板的格式限制大家，但issue区实在有点混乱。有时候说了半天才搞清楚原来对方用的是旧版、自己改了代码之类，浪费双方宝贵时间。所以这里用一个规范的模板统一一下，造成不便望海涵。除了注意事项外，其他部分可以自行根据实际情况做适量修改。
-->

## 注意事项
请确认下列注意事项：

* 我已仔细阅读下列文档，都没有找到答案：
  - [首页文档](https://github.com/hankcs/HanLP)
  - [wiki](https://github.com/hankcs/HanLP/wiki)
  - [常见问题](https://github.com/hankcs/HanLP/wiki/FAQ)
* 我已经通过[Google](https://www.google.com/#newwindow=1&q=HanLP)和[issue区检索功能](https://github.com/hankcs/HanLP/issues)搜索了我的问题，也没有找到答案。
* 我明白开源社区是出于兴趣爱好聚集起来的自由社区，不承担任何责任或义务。我会礼貌发言，向每一个帮助我的人表示感谢。
* [x ] 我在此括号内输入x打钩，代表上述事项确认完毕。

## 版本号
<!-- 发行版请注明jar文件名去掉拓展名的部分；GitHub仓库版请注明master还是portable分支 -->

当前最新版本号是：portable-1.3.3
我使用的版本是：portable-1.3.2


## 我的问题
 trie = new AhoCorasickDoubleArrayTrie<String>();
        TreeMap<String, String> patternMap = new TreeMap<String, String>();
        addKeyword(patternMap, ""CCCCCCCCD"");
        addKeyword(patternMap, ""CCCCCCCD"");
        addKeyword(patternMap, ""CCCCCCD"");
        addKeyword(patternMap, ""CCCCCCGD"");
        addKeyword(patternMap, ""CCCCCCICCCCD"");
        addKeyword(patternMap, ""CCCCCCPD"");
        addKeyword(patternMap, ""CCCCCD"");
        addKeyword(patternMap, ""CCCCCDD"");
需要怎么处理才能避开记载你们内部的这个角色标注列表，我有自己的角色标注列表。
机构名角色观察：[  S 1169909 ][沈阳 G 97524 ][硕 P 7 ][润 C 116 ][机械电子 F 1000 ][设备 C 273 B 2 ][有限公司 K 1000 D 1000 ][  B 8425 ]
机构名角色标注：[ /S ,沈阳/G ,硕/P ,润/C ,机械电子/F ,设备/C ,有限公司/D , /S]
识别出机构名：机械电子设备有限公司 FCD
识别出机构名：润机械电子设备有限公司 CFCD
识别出机构名：设备有限公司 CD
###期望输出
机械电子设备有限公司 FCD
输出这个结果的原因是我的列表中不存在C开头的角色标注组合
### 实际输出
[沈阳/ns, 硕/ag, 润机械电子设备有限公司/nt]   注意第二个  润机械电子设备有限公司 CFCD

每次调用都会默认记载内部的角色标注列表
"
HanLP,hankcs,226971973,关于添加自定义词典分词与词性标注问题,open,提问,https://github.com/hankcs/HanLP/issues/525,"
当前最新版本号是：
我使用的版本是：HanLP1.3.2
## 我的问题

在词性标注中，我们使用了用户自定义词典，发现自定义词典中的大部分词语能按用户自定义记号进行标注，而存在少部分词语无法按自定义标注进行词性标注。

## 复现问题

如：用户添加自定义词典userdict.txt 存在如下几个词语：
性别  sex
男 man
女 woman
故意伤害罪 anyou

其中，“性别”、“男”、“女”这三个词可以按自定义词典的标注进行标注，结果为 ：性别/sex、男/man、女/ woman，而“ 故意伤害罪”这个词语词性标注为“nt”,结果为：故意伤害罪/nt。

### 期望输出

我们的期望输出是：
性别 / sex
男 /man
女 /woman
故意伤害罪/ anyou



"
HanLP,hankcs,226516158,关于层叠HMM中文实体识别的过程,open,提问,https://github.com/hankcs/HanLP/issues/521,"hankcs你好！
我在用python做中文实体识别的分词器，想请教一个问题，你的层叠HMM中文实体识别中，不同的实体识别之间是如何传递的。我的策略是先进行人名识别，将结果传递，再进行地名识别，再将结果传递，最后进行机构名识别。但是当多个实体识别同时开启时，会出现一点问题。
比如：
`s = '南翔向宁夏固原市彭阳县红河镇黑牛沟村捐赠了挖掘机'`

当只开启地名识别时，可以正确分词：
`['南翔', '向', '宁夏', '固原市', '彭阳县', '红河镇', '黑牛沟村', '捐赠', '了', '挖掘机']`

但是当我同时开启人名识别和地名识别时，分词结果就不太好了。
人名识别结果：
 `['南翔', '向', '宁夏', '固原市', '彭阳县', '红河镇', '黑', '牛沟村', '捐赠', '了', '挖掘机']`

将结果作为传递给地名识别，结果不能正确分出`'黑牛沟村'`
地名识别结果：
 `['南翔', '向', '宁夏', '固原市', '彭阳县', '红河镇', '黑', '牛沟村', '捐赠', '了', '挖掘机']`

我用HanLP试了下，发现，当只开启人名识别时，和我错误的结果一致：
`[南翔/ns, 向/p, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红河镇/ns, 黑/a, 牛沟村/nr, 捐赠/v, 了/ule, 挖掘机/n]`

但是同时开启人名和地名识别，则可以得到正确的结果：
`[南翔/ns, 向/p, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红河镇/ns, 黑牛沟村/ns, 捐赠/v, 了/ule, 挖掘机/n]`

我想请教下，当同时开启了人名和地名识别时，你是如何处理的。




"
HanLP,hankcs,225914405,分词词性重训练中某些词不在CoreNatureDictionary词表中,open,提问,https://github.com/hankcs/HanLP/issues/518,"我在训练集中有这样一句话      我/rr 喜欢/v 蓝宝石酒店/nt 可是/c 已经/d 回不去/vf 了/ule
生成的核心词典CoreNatureDictionary的词条中没有   蓝宝石酒店这个词条   怎么能使得生成的核心词表中有这个词。"
HanLP,hankcs,225900152,字母数字混合的编号分词错误,open,提问,https://github.com/hankcs/HanLP/issues/517,"HanLP版本`1.3.2`

测试用例如下，
```java
@Test public void wordSegment2() {
        HanLP.Config.enableDebug();
        assertThat(new NShortSegment().enableCustomDictionary(true).seg(""1000000438L01"").stream().map(w -> w.word))
                .containsExactly(""1000000438L01"");
    }
```

HanLP按词性把编号拆分了。如果想保留完整编号，应该如何设置？

> 打印词图：========按终点打印========
> to:  1, from:  0, weight:04.56, word:始##始@未##数
> to:  2, from:  1, weight:07.60, word:未##数@未##串
> to:  3, from:  2, weight:07.06, word:未##串@未##数
> to:  4, from:  3, weight:03.69, word:未##数@末##末
> 
> 粗分结果[1000000438/m, L/nx, 01/m]"
HanLP,hankcs,225658017,在C#中使用自定义词典，新建Nature失败,open,求助,https://github.com/hankcs/HanLP/issues/516,"当前最新版本号是：v1.3.2
我使用的版本是：v1.3.2


## 我的问题

我意图在C#中使用HanLP，按照教程[在CSharp中调用HanLP](http://www.hankcs.com/nlp/call-hanlp-in-csharp.html)我已经成功使用iKVM打包，并在C#运行用例。不过我想新建用户字典并新建Nature，以满足我识别自定义类型实体的要求，如新建名人类型：nrmy。

在Java中通过修改hanlp.properties可以添加新的类型nrmy，在写代码的时候添加Nature.create(""nrmy"")也没问题。以上两步做完之后Java代码可以识别新添加的nrmy实体。

不过C#代码会显示无法从hanlp.properties中读取新的类型nrmy。新的类型完全失效。

## 复现问题
```Java
        static void Main(string[] args)
        {
            java.lang.System.getProperties().setProperty(""java.class.path"", ""mypath"");
            Nature.create(""nrmy"");
            CustomDictionary.insert(""白富美"", ""nrmy 1024"");
            Console.WriteLine(HanLP.segment(""欢迎在CSharp中调用HanLP的API""));
            Console.ReadKey();
        }
```

## 报错：
WARNING: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!
如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类

Unhandled Exception: java.lang.IllegalArgumentException: Could not set the enum ---> java.lang.IllegalAccessException: Can not set static final [Lcom.hankcs.hanlp.corpus.tag.Nature; field com.hankcs.hanlp.corpus.tag.Nature.$VALUES to [Lcom.hankcs.hanlp.corpus.tag.Nature;
   at IKVM.NativeCode.sun.reflect.ReflectionFactory.FieldAccessorImplBase.FieldAccessor`1.lazySet(Object obj, T value)
   at IKVM.NativeCode.sun.reflect.ReflectionFactory.FieldAccessorImplBase.FieldAccessor`1.lazySet(Object obj, T value, FieldAccessor`1 acc)
   at IKVM.NativeCode.sun.reflect.ReflectionFactory.FieldAccessorImplBase.ObjectField.set(Object obj, Object value)
   at com.hankcs.hanlp.corpus.util.ReflectionHelper.setStaticFinalField(Field field, Object value)
   at com.hankcs.hanlp.corpus.util.EnumBuster.addByValue(Enum e)
   --- End of inner exception stack trace ---
   at com.hankcs.hanlp.corpus.util.EnumBuster.addByValue(Enum e)
   at com.hankcs.hanlp.corpus.util.CustomNatureUtility.addNature(String name)
   at com.hankcs.hanlp.corpus.tag.Nature.create(String name)
   at HanLPSharp.Program.Main(String[] args) in mypath
"
HanLP,hankcs,224324867,情感分析,open,提问,https://github.com/hankcs/HanLP/issues/503,Hanlp为什么没有实现情感分析相关的功能呢？
HanLP,hankcs,224008175,python如何调用model下面的模型,open,重复,https://github.com/hankcs/HanLP/issues/501,"python如何调用model下面的模型,Hanlp 1.3.2"
HanLP,hankcs,223309677,使用solr搜索分词后的结果搜不到，麻烦解答一下。,open,提问,https://github.com/hankcs/HanLP/issues/496,"使用hanlp分词,索引内容：“中医药大学附属医院”，solr测试字段用的是“hanlp”。
搜hanlp:""中医""能搜到，搜hanlp:""中医药""却搜不到。具体问题截图在这里：https://www.oschina.net/question/145106_2239234
麻烦作者帮忙看下，多谢！"
HanLP,hankcs,222637556,发现一个词性处理的问题 词性为w的内容会有乱码，这个是什么情况,open,提问,https://github.com/hankcs/HanLP/issues/493,"示例：
���������ֱ���ǻ��� w 0
���������ڽ��ϵ������µĽ���ȷʵ����� w 0
���������ܻ���������� w 0
���������� w 0
����������Ƥ w 0
����������ǿ�ҵĸɵĸ w 0
����������ȶȡ� w 0
����������Ȼ���ۣ�����̫������̼��� w 0
����������ʳ�ķ������������˳��ϸ� w 0
����������Եĳ����𣿿������ԣ��� w 0
����������　　 w 0
����������� w 0
�����������ǲ��ǿ��ž����㵽һ����� w 0
�����������ʳ�أ� w 0
�����������ҩ�ŵĴ����Ʒ�� w 0
�����������　　 w 0
������������ w 0
������������ˣ�Ů�˵���ѹ����ӳ w 0
������������һ�𿴿� w 0
������������� w 0
�������������¹�Ҷ��� w 0
�������������΢���� w 0
�������������　　 w 0
�������������� w 0
��������������� w 0
���������������/ w 0
����������������� w 0
�����������������ڽ�ͷ��� w 0
������������������� w 0
�������������������࣬��ʳ���Ļ��� w 0


请问这样的问题该如何处理，谢谢！"
HanLP,hankcs,222603628,关键词提取 如何让其识别外国人名,open,提问,https://github.com/hankcs/HanLP/issues/492,"代码：
        List<String> keyWords = 
                        HanLP.extractKeyword(""亨利·福特（HenryFord，1863年7月30日—1947年4月8日），美国汽车工程师与企业家，汽车公司的建立者。"",30);
        keyWords.forEach(System.out::println);

结果：
汽车 美国汽车工程师 企业家 HenryFord 公司 福特 建立

请问 如果想让关键词 启用人名识别 怎么启用？ 主要是目的 不是让其解析出“福特” 而是用关键词解析出“亨利·福特”"
HanLP,hankcs,222344186,都误标成人名   nr.txt中L、B、D含义,open,提问,https://github.com/hankcs/HanLP/issues/491,现在遇到问题：都买/nr    都碰/nr  都好大/nr  都和谁/nr等，想改都作为姓氏的频数。谁能告诉我nr.txt中的L、B、D等都表示什么意思       都 L 587 B 308 D 32 C 16 E 6 K 3
HanLP,hankcs,222100780,HanLp 有敏感信息识别功能？,open,提问,https://github.com/hankcs/HanLP/issues/488,
HanLP,hankcs,222072581,chartype识别能自定义么？,open,重复,https://github.com/hankcs/HanLP/issues/486,在进行分词调试过程中，字符类型是CharType.dat.yes从加载的。能否自定义chartype类型呢？实现更精细的字符类型控制？
HanLP,hankcs,222057110,自定义词典采用AhoCorasickDoubleArrayTrieSegment发生了重复分词的问题,open,无效,https://github.com/hankcs/HanLP/issues/485,"hankcs，你好。
只用一个自定义词典“mydict.txt”，里面内容有“北京”、“北京市”。对“北京市长安街”用AhoCorasickDoubleArrayTrieSegment分词，结果为【北京，北京市】，不符合我的预期。
我希望达到的效果是词典中一个词包含另一个词时，只采用最长的那个词匹配，即分词结果为【北京市】。请问我换一个算法还是怎么做？谢谢。"
HanLP,hankcs,222011201,自定义词典中标为ntc的词分词会显示为nt,open,重复,https://github.com/hankcs/HanLP/issues/484,"比如例句""教授在教授课程""，如果我在自定义词典加入“教授课程 ntc 1000”则默认分词HanLP.newsegment能够正确分词，但会将“教授课程”标为nt 
教授/nnt,在/p,教授课程/nt
然而教授课程不是只有自定义的ntc这个词行吗？为何会识别为nt
但是如果自定义词典中将教授课程改为全新的词性，或是一些较为常见的，如n,a，v之类的就没有问题
非常困扰，在此先感谢拉"
HanLP,hankcs,221790179,邮箱分词,open,提问,https://github.com/hankcs/HanLP/issues/483,"测试用例中采用sunny800629@sina.com进行功能测试，分词的效果是这样的：[0:5 1] sunni/nx
[5:11 1] 800629/m
[11:16 1] @sina/nx
[16:17 1] ./w
[17:20 1] com/nx
这里面的@sina，我想拆成@和sina两个词。经过调试在生成图网的时候就已经是这个样子了。请问，有什么解决办法么？"
HanLP,hankcs,221505256,model.txt如何转换为model.txt.bin,open,提问,https://github.com/hankcs/HanLP/issues/480,"您好，我在[issue#45](https://github.com/hankcs/HanLP/issues/45)中看到您提到将CRF++训练出的model.txt转换成了model.txt.bin，然后将model.txt.bin文件作为HanLP的CRF分词模型，我想请问一下这个转换过程是如何完成的，目的是什么？还望不吝赐教，谢谢！
另，我的目的是想要使用我自己用CRF++训练出来的模型替换您HanLP提供的模型来定制CRF模型。"
HanLP,hankcs,221442820,扩展识别器,open,提问,https://github.com/hankcs/HanLP/issues/478,有没有扩展识别器的接口实现啊？想实现一些自定义的识别实现。
HanLP,hankcs,221179561,HanLP Elasticsearch 5.x 插件,open,改进,https://github.com/hankcs/HanLP/issues/477,"我们实现了 elasticsearch 5.x 的插件：https://github.com/hualongdata/hanlp-ext 。
不过现在遇到个小问题需要手动设置 -Djava.security.policy 和 ES_CLASSPATH，希望有人能帮我解决。"
HanLP,hankcs,220900134,使用CustomDictionary.get获取核心词典单词为null，开启调试显示词库皆已加载,open,提问,https://github.com/hankcs/HanLP/issues/476,"以下每次测试前都删除了bin文件
以下测试均使用源码和jar包导入两种形式测试过，结果一致
望hankcs解惑

1.CustomDictionary.get函数获取核心词典返回为Null
结果为null
但HanLP.Config.enableDebug()开启调试后，调用分词函数HanLP.extractPhrase(""更多精彩请搜索王者荣耀"",10);结果可以看到 搜索/vn
此时调用CustomDictionary.get(“搜索”)获取信息仍然是Null

2.自定词库未载入
HanLP.Config.enableDebug()开启调试后提示加载了自定义词典，该词典只有一条记录（王者荣耀 nz 1），但是调用分词函数HanLP.extractPhrase(""更多精彩请搜索王者荣耀"",10);结果可以看到 王者/n, 荣耀/an.使用CustomDictionary.get(“王者荣耀”)返回为null

3.修改已有的自定义词典未生效
使用CustomDictionary.get(“b超”)获取到了“现代汉语补充词库.txt”中的第一条记录，于是我将""王者荣耀 nz 1""添加在这个词典的开头（失败后又尝试放在结尾，仍然失败）,使用CustomDictionary.get(“王者荣耀”)返回为null，CustomDictionary.get(“b超”)仍然有返回值。"
HanLP,hankcs,220876137,关于一词多词性的获取问题,open,提问,https://github.com/hankcs/HanLP/issues/475,"我自定义两个字典，
test1.txt
汽车 qchy 1
test2.txt
汽车 kxp 1
我想通过“汽车”这个词语获得到 qchy 和 kxp 两个词性。我在使用CustomDictionary.get（“汽车”)，只能获得
kxp 这个么一个。我知道还有一种就是 把这两个词性写入到同一行，但是到时候判断词性的时候就得使用indexOf(""kxp"")等这种方式遍历了。
请问有没有更好的办法解决？
------  奔溃了，一直提交不上去，不知道什么情况，打了好几遍-------"
HanLP,hankcs,220865747,不解：停用词词典中为什么有很多重复的词,open,,https://github.com/hankcs/HanLP/issues/474,"since
since
since
sincere
six
six
sixty
so
so
some
some
somehow
somehow
someone
something
sometime
sometimes
somewhere
still
still
such
such
system
take
ten
ten
than
that
that
that
that
that
the
the
the
their
their
their"
HanLP,hankcs,220515762,但是英文还有一个词根还原的功能，就是went能识别出go。这种情况hanlp有么,open,提问,https://github.com/hankcs/HanLP/issues/471,
HanLP,hankcs,220457783,Python 如何调用词共现统计？,open,提问,https://github.com/hankcs/HanLP/issues/470,"对 java 不甚熟悉，参考 https://github.com/hankcs/HanLP/blob/master/src/test/java/com/hankcs/demo/DemoOccurrence.java
折腾一段时间之后无果。。
求指教 :)"
HanLP,hankcs,220129971,能否对英文文献进行处理吗？,open,提问,https://github.com/hankcs/HanLP/issues/467,想对英文内容进行摘要，关键词查找，评分等功能
HanLP,hankcs,218697721,Dependency viewer,open,提问,https://github.com/hankcs/HanLP/issues/464,可以给一个Dependency viewer下载链接吗？谢谢楼主了。
HanLP,hankcs,218656637,《自动摘要》文章字数达到一定量导致内存溢出,open,提问,https://github.com/hankcs/HanLP/issues/462," java.lang.OutOfMemoryError: GC overhead limit exceeded
    	at com.hankcs.hanlp.summary.TextRankSentence.<init>(TextRankSentence.java:74)
    	at com.hankcs.hanlp.summary.TextRankSentence.getSummary(TextRankSentence.java:253)
    	at com.hankcs.hanlp.HanLP.getSummary(HanLP.java:484)
    	at wisers.wisenews.doc.util.AutoSummaryUtil.getTextRankSummary(AutoSummaryUtil.java:299)
    	at wisers.wisenews.doc.util.AutoSummaryUtil.truncExcerptContent(AutoSummaryUtil.java:133)

我用一一篇 文章 大概有16w字左右，debug 看了 那个 切割后的句子的 sentences size 有1w以上，然后 在调用 分词排序算法时 TextRankSentence textRank = new TextRankSentence(docs);就抛出内存溢出，求大神 帮帮解释下，这个大概消耗内存 是多少，处理的量范围；谢谢

public TextRankSentence(List<List<String>> docs)
    {
        this.docs = docs;
        bm25 = new BM25(docs);
        D = docs.size();
        weight = new double[D][D];
        weight_sum = new double[D];
        vertex = new double[D];
        top = new TreeMap<Double, Integer>(Collections.reverseOrder());
        solve();
    `}```
weight = new double[D][D]; 这个当  docs.size();超过1w是就报内存溢出；"
HanLP,hankcs,218390769,import junit.framework.TestCase这个是什么错误，要怎么解决,open,提问,https://github.com/hankcs/HanLP/issues/459,"Description	Resource	Path	Location	Type
The import junit cannot be resolved	AdjustCorpus.java	/wenben/src/com/hankcs/test/corpus	line 28	Java Problem
"
HanLP,hankcs,218125841,如何让多个tokenizer变量有各自的用户自定义词典CustomDictionary,open,提问,https://github.com/hankcs/HanLP/issues/458,"hi，hankcs，感谢你对中文NLP领域做出的贡献。

根据我自己的场景，请教一个问题：
比如有两个分词变量：tokenizer1和tokenizer2，需要动态增加用户自定义词典，如何让两个分词变量有各自的用户自定义词典，而不互相影响呢？
依据demo中调用CustomDictionary.add(""攻城狮"", ""nz 1024 n 1""); 然后，便会影响所有tokenizer的分词结果了。
请问：是否有方法让多个分词变量，有各自的用户自定义词典，而不互相影响呢？

谢谢！
"
HanLP,hankcs,218076413,Python调用CRF分词,open,提问,https://github.com/hankcs/HanLP/issues/456,"@hankcs  
环境Python3，
参考demo后：https://github.com/hankcs/HanLP/tree/master/src/test/java/com/hankcs/demo 
CRF= JClass('com.hankcs.hanlp.seg.CRF.CRFSegment') 
#引入成功，
#但是  
CRF.seg('""威廉王子发表演说 呼吁保护野生动物\n""')
#报错：

Traceback (most recent call last):

  File ""<ipython-input-41-a8007c89d333>"", line 2, in <module>
    CRF.seg('""威廉王子发表演说 呼吁保护野生动物\n""')

RuntimeError: No matching overloads found. at native\common\jp_method.cpp:121
 求答复，thanks！"
HanLP,hankcs,217557767,核心词典自定义词典同时出现时的词性优先级问题,open,提问,https://github.com/hankcs/HanLP/issues/453,"想请教一下关于优先级的问题，按照文档的描述自定义词典的优先级是全局最高的？但是在核心词典中存在某个词时自定义词典加入其他词性并不能识别出自定义词性，识别出的词性是核心词典中定义的概率最大的（只测试了标准分词的情况，同时开启了隐马尔科夫判断词性），此时如果把核心词典的面这个词条删掉就可以识别出自定义的词性。应该不是转移矩阵和频数的问题测试过各种的词性和频数，似乎都是核心词典优先。不知道是不是程序设定本身就是这样呢，如果是的话除了删除核心词典相关条目不知道有没有什么好的处理方法。或者还是隐马的作用范围只针对核心词典？
非常感谢！"
HanLP,hankcs,216701943,solr6停止词,open,重复,https://github.com/hankcs/HanLP/issues/449,请问在solr6.4里面，是否支持中文停止词，我对默认的停止语字典做了修改后，在使用solr分析时，停止词无效。
HanLP,hankcs,216622326,hanlp.properties在maven项目路径的问题,open,提问,https://github.com/hankcs/HanLP/issues/448,"在eclipse中利用maven的pom.xml加入了hanlp
把hanlp.properties放在maven项目中的src下，通过maven命令编译项目，properties并没有自动复制到classpath中，每次都得手动把properties放到classpath，有没有什么解决办法那？？"
HanLP,hankcs,216105597,请问data\model\dependency\WordNature.txt.bin有介绍么？,open,提问,https://github.com/hankcs/HanLP/issues/446,"请问data\model\dependency\WordNature.txt.bin有介绍么？
我试用crf已存句法分析时，输入“”党参的定义是什么“”，此处“定义”的依存关系为“原处所”？
故而希望您能给出一个WordNaturedescription。谢谢！"
HanLP,hankcs,215352313,"自定义字典报错java.lang.NumberFormatException: For input string: ""gi""",open,提问,https://github.com/hankcs/HanLP/issues/444,"我把之前自定义词性为gi的字典扩充了一些内容进去，之后把缓存删掉重新启动就报了这个错误。。。。
三月 20, 2017 4:13:45 下午 com.hankcs.hanlp.corpus.io.IOUtil readBytes
警告: 读取D:/data/dictionary/CoreNatureDictionary.txt.bin时发生异常java.io.FileNotFoundException: D:\data\dictionary\CoreNatureDictionary.txt.bin (系统找不到指定的文件。)
三月 20, 2017 4:13:54 下午 com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary loadDat
警告: 尝试载入缓存文件D:/data/dictionary/CoreNatureDictionary.ngram.txt.table.bin发生异常[java.io.FileNotFoundException: D:\data\dictionary\CoreNatureDictionary.ngram.txt.table.bin (系统找不到指定的文件。)]，下面将载入源文件并自动缓存……
三月 20, 2017 4:13:58 下午 com.hankcs.hanlp.corpus.io.IOUtil readBytes
警告: 读取D:/data/dictionary/custom/CustomDictionary.txt.bin时发生异常java.io.FileNotFoundException: D:\data\dictionary\custom\CustomDictionary.txt.bin (系统找不到指定的文件。)
三月 20, 2017 4:13:59 下午 com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
警告: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!
如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类
三月 20, 2017 4:13:59 下午 com.hankcs.hanlp.dictionary.CustomDictionary load
严重: 自定义词典D:/data/dictionary/custom/tags.txt读取错误！java.lang.NumberFormatException: For input string: ""gi""
三月 20, 2017 4:13:59 下午 com.hankcs.hanlp.dictionary.CustomDictionary loadMainDictionary
警告: 失败：D:/data/dictionary/custom/tags.txt
导致加载这个字典失败！！！！！！！！！！！！这是为什么？？？？？？？？？？"
HanLP,hankcs,215347137,关于OrganizationDictionary 类中已经存在的角色规则组合,open,,https://github.com/hankcs/HanLP/issues/443,
HanLP,hankcs,215345618,姓氏总结，感觉没有全，应该是训练语料少引起的,open,提问,https://github.com/hankcs/HanLP/issues/442,
HanLP,hankcs,214963771,一些实体识别不准确的情况,open,改进,https://github.com/hankcs/HanLP/issues/440,"机构名前后的一些介词处理的不好，如：
南京的东南大学以前叫南京工学院
[南京/ns, 的东南大学/nt, 以前/f, 叫/vi, 南京工学院/nt]

在\data\dictionary\person\nr.txt中加了黄鹤楼 A 1还是没用
湖北最出名的烟是黄鹤楼
[湖北/ns, 最/d, 出名/a, 的/ude1, 烟/n, 是/vshi, 黄鹤楼/nr]

1998年3月1号是我的生日
[1998/m, 年/qt, 3/m, 月1号/nt, 是/vshi, 我/rr, 的/ude1, 生日/n]
"
HanLP,hankcs,214921938,王国强、高峰、汪洋、张朝阳光着头、韩寒、小四 ，这里面“张朝阳”这个人名分错了,open,提问,https://github.com/hankcs/HanLP/issues/439,"我在自定义词典中加入“张朝阳”这个人名，设定
张朝阳 nr 10000
照样分错
debug看了一下粗分网，里面没有""阳""字：
粗分结果[王国强/nr, 、/w, 高峰/n, 、/w, 汪洋/n, 、/w, 张/q, 朝/tg, 阳光/n, 着/uzhe, 头/n, 、/w, 韩寒/nr, 、/w, 小/a, 四/m]

我应该怎么做，才能把这个人名分对呢？"
HanLP,hankcs,214889352,修改一个提示问题,open,无效,https://github.com/hankcs/HanLP/issues/438,
HanLP,hankcs,214098767,适配 redis 平台，读取字符串，字节数组和整型转换异常，出现负数情况,open,提问,https://github.com/hankcs/HanLP/issues/433,"ByteUtil.bytesHighFirstToInt(byte[] bytes, int start)
麻烦跟踪一下什么情况，谢谢！"
HanLP,hankcs,213987408,发现楼主还在更新，那我提交一个bug。 判断字符类型 这个函数有bug，我修正后的：,open,改进,https://github.com/hankcs/HanLP/issues/432," /**
     * 判断字符类型
     * @param str
     * @return
     */
    public static int charType(String str)
    {
        if (str != null && str.length() > 0)
        {
            if (""零○〇一二两三四五六七八九十廿百千万亿壹贰叁肆伍陆柒捌玖拾佰仟"".contains(str)) return CT_NUM;
            byte[] b;
            try
            {
                b = str.getBytes(""GBK"");
            }
            catch (UnsupportedEncodingException e)
            {
                b = str.getBytes();
                e.printStackTrace();
            }
            byte b1 = b[0];
            byte b2 = b.length > 1 ? b[1] : 0;
            int ub1 = getUnsigned(b1);
            int ub2 = getUnsigned(b2);
            if (ub1 < 128)
            {
            	if(b1>='A' && b1<='Z'){
            		return CT_LETTER;
            	}
            	if(b1>='a' && b1<='z'){
            		return CT_LETTER;
            	}
                if (' ' == b1 || '\t' == b1) return CT_OTHER;
                if ('\n' == b1 || '\r' == b1) return CT_DELIMITER;
                if (""*\""!,.?()[]{}_-+=/\\;:|"".indexOf((char) b1) != -1)
                    return CT_DELIMITER;
                if (""0123456789"".indexOf((char)b1) != -1)
                    return CT_NUM;
                return CT_SINGLE;
            }
            else if (ub1 == 162)
                return CT_INDEX;
            else if (ub1 == 163 && ub2 > 175 && ub2 < 186)
                return CT_NUM;
            else if (ub1 == 163
                    && (ub2 >= 193 && ub2 <= 218 || ub2 >= 225
                    && ub2 <= 250))
                return CT_LETTER;
            else if (ub1 == 161 || ub1 == 163)
                return CT_DELIMITER;
            else if (ub1 >= 176 && ub1 <= 247)
                return CT_CHINESE;

        }
        return CT_OTHER;
    }"
HanLP,hankcs,213724887,那些不在CoreNatureDictionary.txt的中词会标注什么词性,open,提问,https://github.com/hankcs/HanLP/issues/429,"在查看人名词典nr.txt时。采用以下方式查找在nr.txt 但不在CoreNatureDictionary.txt中的词：
awk 'NR==FNR{a[$1]}NR>NFR{if(!($1 in a))print $0}' ../CoreNatureDictionary.txt nr.txt | wc -l
结果1901个词，这部分词该如何标注词性。人名识别中，词的角度有人名前缀，姓，名，后缀，与人名无关词等。如果这1901个词不在CoreNatureDictionary.txt, 那怎么会分出这些词来呢？ 如果分出来了，词性又是如何设置的呢？"
HanLP,hankcs,213683917,ES 集成 hanlp时报错。,open,,https://github.com/hankcs/HanLP/issues/427,"Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.hankcs.hanlp.seg.common.Vertex
	at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:71)
	at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
	at com.hankcs.hanlp.seg.Segment.seg(Segment.java:558)
	at com.hankcs.hanlp.seg.common.wrapper.SegmentWrapper.next(SegmentWrapper.java:68)
	at com.hankcs.lucene.HanLPTokenizer.incrementToken(HanLPTokenizer.java:76)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.simpleAnalyze(TransportAnalyzeAction.java:247)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:225)
	at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:63)
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:282)
"
HanLP,hankcs,213586363,用户自定义词典加载不了,open,,https://github.com/hankcs/HanLP/issues/425,"1、CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; MyDic.txt;
2、MyDic.txt：信息与计算科学 v 1
3、确保是uft-8
4、已删除缓存文件
5、LexiconUtility.getAttribute(""信息与计算科学"")
6、HanLP.Config.enableDebug()看不到有加载MyDic.txt
7、CustomDictionary.add(""信息与计算科学"")可以成功
谢谢！"
HanLP,hankcs,213255622,hanlp-portable 怎样通过properties 文件配置自定义词典呢。,open,,https://github.com/hankcs/HanLP/issues/424,
HanLP,hankcs,212735438,关于Term的offset属性问题,open,,https://github.com/hankcs/HanLP/issues/420,请问怎么开启分词器的offset选项？
HanLP,hankcs,212039213,关于自定义字典的路径问题,open,提问,https://github.com/hankcs/HanLP/issues/419,"看hankcs给出的自定义字典的配置格式是这样的：
data/dictionary/custom/CustomDictionary.txt;CompanyName.txt;school.txt
但实际上这样配置却读不到，程序运行时直接找了根路径+CompanyName.txt文件，
改成：
data/dictionary/custom/CustomDictionary.txt;data/dictionary/custom/CompanyName.txt;data/dictionary/custom/school.txt
这个样子的绝对路径就可以读到了，不知是哪里的错误，还是我理解有偏差，望指教"
HanLP,hankcs,211957836,NShortSegment分词 java.lang.OutOfMemoryError: GC overhead limit exceeded,open,提问,https://github.com/hankcs/HanLP/issues/417,"代码如下：
`NShortSegment segment=new NShortSegment().enableAllNamedEntityRecognize(true).enablePlaceRecognize(true);
						List<Term> terms=segment.seg(text);`

总是分到这段话时出来内存错误：
“第一部分马克思主义哲学原理　　复习总思路……………………………………………………………………1　　第一章马克思主义哲学是科学的世界观和方法论………………………3　　第二章世界的物质性和人的实践活动……………………………………15　　第三章世界的联系、发展及其规律………………………………………30　　第四章认识的本质和过程…………………………………………………51　　第五章人类社会的本质和基本结构………………………………………76　　第六章社会发展规律和历史创造者………………………………………97　　第七章社会发展和人的发展………………………………………………112　　第二部分马克思主义政治经济学原理　　复习总思路……………………………………………………………………125　　第一章导论…………………………………………………………………128　　第二章社会经济制度与经济运行的一般原理……………………………134　　第三章资本主义生产关系的实质及其发展阶段…………………………152　　第四章资本的运行…………………………………………………………183　　第五章社会主义生产关系的实质与经济制度……………………………199　　第六章社会主义市场经济体制和经济运行………………………………211　　第七章经济全球化与国际经济关系………………………………………224　　第三部分毛泽东思想概论　　复习总思路…………………………………………………………………231　　第一章毛泽东思想是马克思主义中国化的理论成果……………………234　　第二章新民主主义革命的总路线和基本纲领……………………………241　　第三章新民主主义革命的基本问题………………………………………259　　第四章社会主义改造的理论原则与经验总结……………………………276　　第五章社会主义若干重大理论问题的探索成果…………………………285　　第六章社会主义建设的方针政策…………………………………………296　　第七章掌握毛泽东思想的活的灵魂，坚持和发展毛泽东思想…………310　　第四部分邓小平理论和“三个代表”重要思想概论　　复习总思路…………………………………………………………………323　　第一章邓小平理论是当代中国的马克思主义……………………………327　　第二章“三个代表”重要思想是马克思主义中国化的最新理论成果…332　　第三章解放思想、实事求是、与时俱进…………………………………341　　第四章社会主义的本质和根本任务………………………………………348　　第五章社会主义初级阶段和党的基本路线、基本纲领…………………355　　第六章科学发展观和社会主义建设的发展战略…………………………364　　第七章中国特色社会主义经济……………………………………………389　　第八章中国特色社会主义政治……………………………………………402　　第九章中国特色社会主义文化……………………………………………415　　第十章“一国两制”和实现祖国的完全统一……………………………424　　第十一章维护世界和平，促进共同发展…………………………………430　　第十二章中国特色社会主义事业的依靠力量和领导核心………………437　　第五部分当代世界经济与政治　　复习总思路…………………………………………………………………446　　第一章当代世界经济的发展变化与基本趋势……………………………449　　第二章当代世界政治的发展变化与基本趋势……………………………462　　第三章当今时代主题与建立国际新秩序…………………………………479　　第四章战后发达资本主义国家的经济与政治……………………………487　　第五章战后发展中国家的政治与经济……………………………………502　　第六章战后社会主义国家的经济与政治…………………………………210　　第七章独联体成员国与冷战后东欧国家的经济与政治…………………518　　第八章中国对外关系及在世界上的地位与作用…………………………526更多信息请访问：新浪考研频道考研论坛考研博客圈　　特别说明：由于各方面情况的不断调整与变化，新浪网所提供的所有考试信息仅供参考，敬请考生以权威部门公布的正式信息为准。”

"
HanLP,hankcs,211639299,HanLP.properties文件需要自己写吗？,open,,https://github.com/hankcs/HanLP/issues/416,是自己创建添加root 配置 还有其他的配置吗？ 找不到参考文件
HanLP,hankcs,211633401,elasticsearch-analysis-hanlp添加自定义停用词影响了正常词汇的分词效果,open,提问,https://github.com/hankcs/HanLP/issues/415,"我看这个插件，用的好像是hanlp的维特比算法进行分词的？
我修改了停用词库，删除了默认文件内容，添加了自己定义的几个词，为啥原来能分的词（不包含自定义停用词）加完这个之后就分不了了呢"
HanLP,hankcs,211485951,如何过滤停用词,open,提问,https://github.com/hankcs/HanLP/issues/414,看到了停用词词典，但是不知道如何过滤停用词呢？
HanLP,hankcs,211307163,动态加载词库,open,提问,https://github.com/hankcs/HanLP/issues/412,"大神你好，请问一下，这个可以支持    提供远程加载配置，设置监控，词典更新自动加载，无需重启。

类似这种做法： https://github.com/medcl/elasticsearch-analysis-ik/pull/40   这个可以在这边实现吗？

这样就不需要每次更新词典，又要重新发布到生产环境了。 "
HanLP,hankcs,211273548,demo演示结果和实际运行结果不同,open,提问,https://github.com/hankcs/HanLP/issues/409,"demo某段结果为：
[欢迎/v, 新/a, 老/a, 师生/n, 前来/vi, 就餐/vi]
[工信处/nt, 女干事/n, 每月/r, 经过/p, 下属/v, 科室/n, 都/d, 要/v, 亲口/d, 交代/v, 24/m, 口/n, 交换机/n, 等/udeng, 技术性/n, 器件/n, 的/ude1, 安装/v, 工作/vn]
[随着/p, 页游/nz, 兴起/v, 到/v, 现在/t, 的/ude1, 页游/nz, 繁盛/a, ，/w, 依赖于/v, 存档/vi, 进行/vn, 逻辑/n, 判断/v, 的/ude1, 设计/vn, 减少/v, 了/ule, ，/w, 但/c, 这/rzv, 块/q, 也/d, 不能/v, 完全/ad, 忽略/v, 掉/v, 。/w]
[中国科学院计算技术研究所/nt, 的/ude1, 宗成庆/nr, 教授/nnt, 正在/d, 教授/v, 自然语言处理/nz, 课程/n]
实际运行结果为：
[欢迎/v, 新/a, 老师/n, 生前/t, 来/v, 就餐/v]
[工信处/n, 女/b, 干事/n, 每月/r, 经过/p, 下属/v, 科室/n, 都要/nr, 亲口/d, 交代/v, 24/m, 口/q, 交换机/n, 等/u, 技术性/n, 器件/n, 的/uj, 安装/v, 工作/vn]
[随着/p, 页/q, 游兴/n, 起/v, 到/v, 现在/t, 的/uj, 页游/nz, 繁盛/an, ，/w, 依赖于/v, 存档/vn, 进行/v, 逻辑/n, 判断/v, 的/uj, 设计/vn, 减少/v, 了/ul, ，/w, 但/c, 这块/r, 也/d, 不能/v, 完全/ad, 忽略/v, 掉/v, 。/w]
[中国科学院/n, 计算/v, 技术/n, 研究所/n, 的/uj, 宗成庆/nr, 教授/n, 正在/d, 教授/n, 自然/d, 语言/n, 处理/v, 课程/n]"
HanLP,hankcs,211265092,自定义词典清空,open,重复,https://github.com/hankcs/HanLP/issues/408,我这有个词库是在数据库上，有人一直在维护增删词，我现在是通过com.hankcs.hanlp.dictionary.CustomDictionary#add 方法动态加载的，现在有个需求是刷新重新加载这个词典，请问有什么方法吗？是不是只要CustomDictionary.loadMainDictionary 这个方法就可以重载这个自定义词典？
HanLP,hankcs,211004244,机构名识别错误,open,提问,https://github.com/hankcs/HanLP/issues/407,"您好！请教一个HanLP分词的问题。""为广大运维者所喜爱"",对于这句话的“所”，应该是被动的意思，""运维者所""被识别成了机构名。请教下这个东西怎么优化好?
补充：这里HanLP经过核心词典和用户词典初分词后，分成了运维/者/所三个词，但是在机构名识别的过程中，机构名标注是不怎么管初分词性和前后语义关系的，只要符合机构名的模式匹配，就会不管三七二十一归为机构名。本人通过源代码理解这里是一个缺陷，不知道有没有理解错误。如果没有理解错误，有没有什么好的解决办法，因为无论是人名、地名、机构名识别都好，都是类似的缺陷，对一些文本尤其是特定领域的文本识别准确率是很低的。望赐教，谢谢！"
HanLP,hankcs,210662619,关于名称识别,open,重复,https://github.com/hankcs/HanLP/issues/405,@现在根据分词使用的方式，根据组词的方式组词后进行对名称的判别，这样就存在一个局限性。就拿我现在在一个文本中是识别未在词库中添加那组‘企业名称‘的词对企业名称识别时，很容出现识别出一半甚至不能识别，如果是一直通过添加词来解决这也会导致文件越来越大。想请问一下这个有什么好的建议或者解决方法，来补足这个缺陷。
HanLP,hankcs,209980807,词典分割符（空格）对于英文实体的问题,open,改进,https://github.com/hankcs/HanLP/issues/404,"例如organization.txt的第一条数据
Valor Capital Group	nhy	1
英文实体内存在空格，从而导致词典读取错误
建议修改方式:
１、（强烈推荐）解析词典方式：取倒数第一个为词频、倒数第二个为词性、其它为词
２、将词的分隔符设置为零宽字符等几乎不可能在实际文本中出现的词"
HanLP,hankcs,209165887,ES使用portal默认词典，添加自定义停用词不起作用,open,提问,https://github.com/hankcs/HanLP/issues/402,求问大神如何解决呢
HanLP,hankcs,208891640,如何在文本中提取识别的地址，公司名称，邮箱等。提取方法有哪些？,open,,https://github.com/hankcs/HanLP/issues/401,"![image](https://cloud.githubusercontent.com/assets/19337606/23128473/0a8952de-f7ba-11e6-8640-e72ff313db35.png)
"
HanLP,hankcs,208703808,add nGram distance function between strings,open,改进,https://github.com/hankcs/HanLP/issues/400,"fork you code,change jdk1.8 ,do some code optimize,want to join the project.
the function is in [getNGRAMDistance in EditDistance.java ](https://github.com/DusonWang/HanLP/blob/master/src/main/java/com/hankcs/hanlp/algoritm/EditDistance.java) "
HanLP,hankcs,206942975,很多人名nr被标记状态词zg，这是为什么呢,open,提问,https://github.com/hankcs/HanLP/issues/398,"比如
[按照/dg, 习近平/zg, 指示/yg, 和/pbei, 李克强/zg, 要求/yg, ，/xx, 外交部/ntcb, 和/pbei, 我国/yg, 驻/s, 马来西亚/nr2, 使领馆/yg, 加强/s, 同/dg

我看了下训练字典里面，习近平和李克强都是nr"
HanLP,hankcs,206695460,请问作者如何生成自己的stopwords.txt.bin？,open,提问,https://github.com/hankcs/HanLP/issues/397,"由于工作需要，我要往停用词表data\dictionary\stopwords.txt中添加自定义的词，添加后，删除了原先的stopwords.txt.bin后，启动项目，发现不能像用户自定义词典那样自动生成新的stopwords.txt.bin。

而且，把stopwords.txt.bin删除掉后，原则上停用词应该是能被分出来的对吧，但分词后，发现停用词仍然是起作用的，即仍然是分不出来的。

我想问：
1. 如何生成自己定义的停用词bin文件：stopwords.txt.bin
2. 为什么删除掉stopwords.txt.bin后，停用词依然起作用。

谢谢作者解答。"
HanLP,hankcs,206688928,Integrate HanLP to KNIME Error,open,提问,https://github.com/hankcs/HanLP/issues/396,"    在下试着于 Knime 的 Java Snippet Node 使用 HanLP, 但发现跑到
sentenceList = HanLP.extractSummary(document, 3);
就造成 Knime ""闪退"", 是否我程式哪裡写错?
/************************************************************************************************************/
// Your custom imports:
import java.util.List;
import com.hankcs.hanlp.HanLP;

// Your custom variables:
String document;
List<String> sentenceList;

// Enter your code here:		
document = ""算法可大致分为基本算法、数据结构的算法、数论算法、计算几何的算法、图的算法、动态规划以及数值分析、加密算法、排序算法、检索算法、随机化算法、并行算法、厄米变形模型、随机森林算法。\n"" +
        ""算法可以宽泛的分为三类，\n"" +
        ""一，有限的确定性算法，这类算法在有限的一段时间内终止。他们可能要花很长时间来执行指定的任务，但仍将在一定的时间内终止。这类算法得出的结果常取决于输入值。\n"" +
        ""二，有限的非确定算法，这类算法在有限的时间内终止。然而，对于一个（或一些）给定的数值，算法的结果并不是唯一的或确定的。\n"" +
        ""三，无限的算法，是那些由于没有定义终止定义条件，或定义的条件无法由输入的数据满足而不终止运行的算法。通常，无限算法的产生是由于未能确定的定义终止条件。"";
// below error ! 造成 Knime ""闪退""
sentenceList = HanLP.extractSummary(document, 3);
/************************************************************************************************************/

PS : I add ‘hanlp-1.3.2.jar’ to “Additional Libraries” at Node, not “Knime\plugins” subdirectory.
 
谢谢您"
HanLP,hankcs,206467279,Mac上pycharm在程序中第二次startjvm报错Unable to start JVM at native/common/jp_env.cpp:78,open,提问,https://github.com/hankcs/HanLP/issues/395,"<img width=""812"" alt=""2017-02-09 6 42 30"" src=""https://cloud.githubusercontent.com/assets/6072224/22779906/922f6c94-eef7-11e6-80e1-6d353a245f78.png"">
我做的是网页上用户每一次提交输入数据后都进行一次分词，第一次start进行分词没有问题 。之后就报错，使用isstart进行判断发现JVM没有shutdown  貌似是只有python程序结束这个JVM才会完全关闭。希望大大解答！！怎么办"
HanLP,hankcs,206394681,‘无锡金鑫集团’ 这个词怎么搞都分不对,open,,https://github.com/hankcs/HanLP/issues/394,"使用索引分词，预期结果是：无锡，无锡金，锡金，金鑫，集团。
得到的结果是：无，锡金，鑫，集团
该咋整？"
HanLP,hankcs,205335411,动态添加词无效,open,提问,https://github.com/hankcs/HanLP/issues/392,"我尝试用使用  CustomDictionary.add(""厚底"") 动态添加词，以便使“厚底情侣白鞋”能正确的进行分词，但是结果是 “厚底” 并没有生效。

```java
CustomDictionary.add(""厚底"");
CustomDictionary.add(""白鞋"");
System.out.println(HanLP.segment(""厚底""));
System.out.println(HanLP.segment(""厚底情侣白鞋""));
```

输出：
[厚底/nz]
[厚/a, 底情/n, 侣/n, 白鞋/nz]

然后我看你readme里面有写“在基于层叠隐马模型的最短路分词中，并不保证自定义词典中的词一定被切分出来。如果你认为这个词绝对应该切分出来，那么请将词频设大一些”，所以我尝试了将词频调大，但是不起作用，这个应该怎么处理？
"
HanLP,hankcs,200860498,用户自定义词典不起作用，谢谢解答！,open,提问,https://github.com/hankcs/HanLP/issues/386,"1、CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; define.txt
2、define.txt：信息与计算科学 v 1
3、确保是uft-8
4、已删除缓存文件
谢谢！"
HanLP,hankcs,200567771,HanLP可以情感分析吗？,open,重复,https://github.com/hankcs/HanLP/issues/385,"您好！
       HanLP可以进行情感分析吗（正面、负面、中性）？能否提供一个思路。谢谢！"
HanLP,hankcs,199754042,利用hanlp标注词性,open,提问,https://github.com/hankcs/HanLP/issues/382,如何利用hanlp标注（已经分好词）的词性？
HanLP,hankcs,199489362,用户意图分析,open,提问,https://github.com/hankcs/HanLP/issues/381,"有这样的一段话""hello,hello,hello 我要去徐家汇"" 采用什么样的模型可以分析出""我要去徐家汇""的意图了？ hankcs能否帮助看一下，多谢"
HanLP,hankcs,199465405,关于hanlp.properties中定义的data路径问题,open,提问,https://github.com/hankcs/HanLP/issues/380,"最近一直在使用hanlp分词工具，发现hanlp.properties在data/dictionary路径的定义上并不灵活，root只能定死绝对路径。在将程序打包放到spark集群上运行，不可行。尝试改IOUtils的   
public static InputStream newInputStream(String path) throws IOException
    {
        if (IOAdapter == null) return new FileInputStream(path);
        return IOAdapter.open(path);
    }
通过classloader的getresourceasStream方式读取，发现之后能实现将data放入到classpath下通过相对路径读取，但是牵一发动全身，很多地方都有异常，希望hancks大神有时间能解决下hanlp.properties只能通过绝对路径读入data的问题"
HanLP,hankcs,198900364,Hanlp的提取摘要的方法,open,,https://github.com/hankcs/HanLP/issues/379,"在Hanlp给出的例子中，main方法里面使用HanLP.extractPhrase(document,5)，这个方法都很正常，但是在其他的地方方法调用这个方法的时候就不成功"
HanLP,hankcs,198875929,hankcs，多模式字符串匹配由于模式不是特别多，我觉得不需要使用自动机，坏字符跳转预处理后，使用一个hashmap就可以了,open,提问,https://github.com/hankcs/HanLP/issues/378,
HanLP,hankcs,198719322,BM25文本相似度为负,open,无效,https://github.com/hankcs/HanLP/issues/377,在用textrank做关键句提取时发现，BM25计算的相似度结果部分为负。连句子与本身的相似度都是负的。应该是idf公式的分母多减了一个文档频率
HanLP,hankcs,198095709,关于词的标注,open,提问,https://github.com/hankcs/HanLP/issues/375,除了在nt.txt中直接标记成C之外，还有没有其他的标注方式是这个词性变成C的 比如一个词 我需要怎么标注什么就能在使用的时候 让它变成角色 C
HanLP,hankcs,197638211,用户自定义词典,open,提问,https://github.com/hankcs/HanLP/issues/374,"请问如果在hanlp.properties文件中配置了用户自定义词典

CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 流行新歌词汇.txt; 电影大全词典.txt;　现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 
之后
 val content=""我喜欢看小岛惊魂，阿甘正传，致命ID，禁闭岛,小叮当与海盗仙子，肖申克的救赎.我喜欢听阿波狂想曲""
　println(StandardTokenizer.segment(content))

是优先去根据用户自定义的词典进行分词吗，如果我希望先标准分词，之后再用户自定义词典分词应该怎样操作呢？
有没有可以显示手动关闭用户自定义词典分词的功能，之后要用的时候再启用的函数呢？"
HanLP,hankcs,195463536,采用自定义字典不同场景下的词性标注问题,open,提问,https://github.com/hankcs/HanLP/issues/371,"自定义song.txt 我在Nature里面定义了一种新的类型为song与之相对应，采用下面的维特比算法进行分词。
 HanLP.newSegment()enablePartOfSpeechTagging(true)
对于“后来”
  如何实现 后来的故事 以及刘若英的后来  中的“后来”两种不同的场景的标注。 前者标注为t，后者标注为song.

是否可以在转移矩阵中实现？ 我试了一下，感觉还是很繁琐的，而且也没有得到正确的结果。能否协助看一下呢？
"
HanLP,hankcs,195419481,自定义词典不起作用,open,提问,https://github.com/hankcs/HanLP/issues/370,"1、在data\dictionary\custom目录下新建自定义词典my.txt，内容如下：
      风中有朵雨做的云
      一千个伤心的理由
      忘情水
2、在hanlp.properties中增加my.txt
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; my.txt ns; 现代汉语补充词库.txt; 全国地名大全.txt ns; 人名词典.txt; 机构名词典.txt; 上海地名.txt;data/dictionary/person/nrf.txt nrf
3、测试：
   String content = ""张学友的一千个伤心的理由是一首好听的歌"";
   Segment seg = HanLP.newSegment().enableCustomDictionary(true);
   List<Term> termList = seg.seg(content);
   System.out.println(termList);
   List<String> keywordList = HanLP.extractKeyword(content, 5);
   System.out.println(keywordList);

   输出结果如下：
[张学友/nr, 的/ude1, 一千个/nz, 伤心/a, 的/ude1, 理由/n, 是/vshi, 一/m, 首/q, 好听/a, 的/ude1, 歌/n]
[张学友, 理由, 伤心, 一千个, 好听]

4、结论：
     自定义词典没起到作用
     （如果在分词之前执行了CustomDictionary.add(""一千个伤心的理由"");就能起作用）


请问我的用法有问题吗？
"
HanLP,hankcs,194929603,关于动态添加字典的问题,open,提问,https://github.com/hankcs/HanLP/issues/367,"hankcs，您好，非常感谢您为大家提供了如此方便的NLP工具。我现在遇到的问题是在分词之前可以获取到当前文档的很多实体缩写名称。然后想通过CustomDictionary.insert()方法动态添加字典增加识别的精确度。但是很多添加的字典没法准确识别，不知道是因为什么？是不是我使用的方法有错误？

@Test
	public void dictionaryTest() {
		Segment segment = HanLP.newSegment().enableCustomDictionary(true)
				.enableOrganizationRecognize(true);
		String str = ""现在总统数码港、自由闲、北海高岭、总统大酒店四家签订了协议"";
		List<Term> termList = segment.seg(str);
		CustomDictionary.insert(""总统数码港"", ""nt 1024"");
		CustomDictionary.insert(""自由闲"", ""nt 1024"");
		CustomDictionary.insert(""北海高岭"", ""nt 1024"");
		CustomDictionary.insert(""总统大酒店"", ""nt 1024"");
		System.out.println(termList);
	}
结果是这样的
[现在/t, 总统/nnt, 数码港/nz, 、/w, 自由/a, 闲/v, 、/w, 北海/ns, 高/a, 岭/ng, 、/w, 总统/nnt, 大酒店四家/nt, 签订/v, 了/ule, 协议/n]
"
HanLP,hankcs,194559078,日志问题,open,提问,https://github.com/hankcs/HanLP/issues/366,建议大大将日志改为slf4j日志组件，使用起来方便很多
HanLP,hankcs,193974097,数据包下载问题：data-for-1.3.0.zip,open,无效,https://github.com/hankcs/HanLP/issues/364,"啊哦，你所访问的页面不存在了。
可能的原因：
1.在地址栏中输入了错误的地址。
2.你点击的某个链接已过期。"
HanLP,hankcs,193950046,检查是否建立了failure表 ,open,提问,https://github.com/hankcs/HanLP/issues/363,"请问一下 在Trie文件中
![image](https://cloud.githubusercontent.com/assets/20895017/20953814/0fc56f82-bc71-11e6-86dd-58020b95298a.png)
这个表示在什么情况建立的  failure表指的又是又是什么"
HanLP,hankcs,193691766,初始化分词器时，字符类型对应表加载失败（CharType.dat.yes）,open,提问,https://github.com/hankcs/HanLP/issues/361,"大神好，这个异常不是每一次都出现（偶尔出现）

下面我在eclipse上测试时的输出的信息

**********************************************************************************

十二月 06, 2016 1:59:13 下午 com.hankcs.hanlp.HanLP$Config <clinit>
严重: 没有找到HanLP.properties，可能会导致找不到data
========Tips========
请将HanLP.properties放在下列目录：
D:\workspace\****\target\classes
Web项目则请放到下列目录：
Webapp/WEB-INF/lib
Webapp/WEB-INF/classes
Appserver/lib
JRE/lib
并且编辑root=PARENT/path/to/your/data
现在HanLP将尝试从D:\workspace\****读取data……
十二月 06, 2016 1:59:13 下午 com.hankcs.hanlp.corpus.io.IOUtil readBytes
警告: 读取data/dictionary/CoreNatureDictionary.txt.bin时发生异常java.nio.channels.ClosedByInterruptException
十二月 06, 2016 1:59:25 下午 com.hankcs.hanlp.corpus.io.IOUtil readBytes
警告: 读取data/dictionary/other/CharType.dat.yes时发生异常java.nio.channels.ClosedByInterruptException
字符类型对应表加载失败：data/dictionary/other/CharType.dat.yes

****************************************************************************************

然后程序到这里就结束了

对应目录 文件是存在的 【data/dictionary/other/CharType.dat.yes】"
HanLP,hankcs,193347788,分词后Nature如何得到完整词性,open,提问,https://github.com/hankcs/HanLP/issues/360,"在分词后，统计各个词性下出现的词，以w为例，得到如下结果：
w

,，,）,（,。,、

按照http://www.hankcs.com/nlp/part-of-speech-tagging.html说明，像顿号、应该为：
wn
          顿号，全角：、
如何得到wn?

分词代码：
HanLP.segment(msg)"
HanLP,hankcs,192204789,关于分词的一个问题,open,提问,https://github.com/hankcs/HanLP/issues/355,在做一个爬虫的项目，用到Hanlp 分词，遇到一些问题。比如说：我的关键词设为“长安人民政府”，Hanlp分词会分出“长安”这个词来，然后爬虫去抓的时候，会把一些人名也抓进来，多出很多垃圾信息。这样该怎么处理？
HanLP,hankcs,192204549,索引分词粒度能配置吗？,open,提问,https://github.com/hankcs/HanLP/issues/354,具体问题：比如 “张常宁在常宁打球”  怎么样不对张常宁这个人名继续分割
HanLP,hankcs,191951402,关于角色标注的问题,open,提问,https://github.com/hankcs/HanLP/issues/353,"#我标注一个词 比如：丹芭碧  nr  16  之后会在程序中显示 丹芭碧 F 8290 B 769 A 266 X 6
现在我想通过同样的方式  怎么把  丹芭碧  这个词标注成C 呢  除了在nt.txt 直接标记成C之外，还有其他的方式让这个词变成 C吗？ "
HanLP,hankcs,191420815,nr词典格式问题,open,提问,https://github.com/hankcs/HanLP/issues/349,"你好！我用的是1.3.1的程序和1.3.0的data。在分词时会有一些词被错误的标成了人名，比如考考你，都喜欢。按照您的说明，我把这些词加上 A 1添加在nr.txt的后面，但是运行时就出现错误了，提示是：十一月 24, 2016 10:27:04 上午 com.hankcs.hanlp.corpus.io.IOUtil readBytes
警告: 读取D:/data/data-for-1.3.0/data/dictionary/person/nr.txt.value.dat时发生异常java.io.FileNotFoundException: D:\data\data-for-1.3.0\data\dictionary\person\nr.txt.value.dat (系统找不到指定的文件。)
十一月 24, 2016 10:27:04 上午 com.hankcs.hanlp.dictionary.nr.NRDictionary onLoadValue
严重: 读取D:/data/data-for-1.3.0/data/dictionary/person/nr.txt失败[java.lang.NumberFormatException: For input string: ""1阿哈""]
该词典这一行格式不对：欧对 A 1阿哈 A 1考考你 A 1微盘 A 1布用谢 A 1南阳 A 1都喜欢 A 1
人名词典加载失败：D:/data/data-for-1.3.0/data/dictionary/person/nr.txt
请问这是什么原因？"
HanLP,hankcs,191181044,我发现有很多应该是名词的词被归为Nature.b区别词导致去停用词的时候被去掉,open,提问,https://github.com/hankcs/HanLP/issues/347,"比如：保安，机电工程师
使用List<Term> list = HanLP.segment(text);分词后为：[保安/b, ，/w, 机电/b, 工程师/nnt]
去停用词CoreStopWordDictionary.apply(list);后的结果  工程师/nnt
我发现CoreStopWordDictionary中有对                
                case 'm':
                case 'b':
                case 'c':
                case 'e':
                case 'o':
                case 'p':
                case 'q':
                case 'u':
                case 'y':
                case 'z':
                case 'r':
                case 'w':
词性开头的词都过滤了，我想问保安和机电为何会被为认为是b 区别词？"
HanLP,hankcs,190895348,语义依存,open,,https://github.com/hankcs/HanLP/issues/345,有计划 做 语义依存吗？
HanLP,hankcs,189573953,请教解决方向：“后来”这个词，如何在不同场景下标注为正确的词性？,open,提问,https://github.com/hankcs/HanLP/issues/342,"遇到了一个case, 请教一下解决的方向。《后来》这首歌，在句中会被标注为 t. 如果加自定义词典 nz，在其他非指代这首歌的句中会被标注为 nz, 想要区分这两种场景下的词性，该如何解决？"
HanLP,hankcs,188967907,类似  xxx-xxx-xxx  被识别成一个词,open,提问,https://github.com/hankcs/HanLP/issues/340,应该怎么处理下呢?
HanLP,hankcs,188236932,使用默认分词，全角标点符号词性标注均为“/xu”,open,无效,https://github.com/hankcs/HanLP/issues/339,"博主您好，源码是最新版本的。按照词典中的标注中文标点符号词性应该为“w”，Nature中更是定义了更细分的类型，但是分词后确都是“/xu”。
======================分词代码：
Segment nShortSegment = new NShortSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
        Segment shortestSegment = new DijkstraSegment().enableCustomDictionary(false).enablePlaceRecognize(true).enableOrganizationRecognize(true);
        String[] testCase = new String[]{
                ""\""（F-35）“闪电工。，【】‘、”\""""
        };
        for (String sentence : testCase)
        {
            System.out.println(""N-最短分词："" + nShortSegment.seg(sentence) + ""\n最短路分词："" + shortestSegment.seg(sentence));
        }
==================分词结果：
[F-/nx, 35/m, “/xu, 闪电/n, 工/n, ”/xu]
N-最短分词：[""/w, （/xu, F-/nx, 35/m, ）/xu, “/xu, 闪/vx, 电工/nnt, 。/xu, ，/xu, 【/xu, 】/xu, ‘/xu, 、/xu, ”/xu, ""/w]
最短路分词：[""/w, （/xu, F-/nx, 35/m, ）/xu, “/xu, 闪/vx, 电工/nnt, 。/xu, ，/xu, 【/xu, 】/xu, ‘/xu, 、/xu, ”/xu, ""/w]"
HanLP,hankcs,187277794,单个汉字的词性(nature)是如何给定的？,open,提问,https://github.com/hankcs/HanLP/issues/337,单个汉字如果没有出现在核心词典中，它的词性是如何给定的？
HanLP,hankcs,186452084,关于机构识别的问题,open,提问,https://github.com/hankcs/HanLP/issues/334,1、企业名称可以识别但是企业的简称怎么去识别呢，有没有类似的方法呢
HanLP,hankcs,185911149,HanLP修改主词典需要注意什么？,open,提问,https://github.com/hankcs/HanLP/issues/333,"请问一下：
项目需要，想用自己的词库分词。
但是做带词性和词频的词典，还有对应的BiGram词典又没有工具做，所以我想把HanLP词典中和我自己词典中都有的词保留；HanLP词典中有的，我自己词典中没有的词删掉；HanLP词典中没有，我自己词典中有的词放到custom词典中，这样靠谱不？

我能想到的问题有两个：
1. CoreDictionaryPath这个配置对应的词典中的内容，除了带“##”的字符串外（这个删了会报java.lang.ArrayIndexOutOfBoundsException异常），别的词条我可以随意增删吗？
2. 改了CoreDictionaryPath那个词典，对应的BiGramDictionaryPath词典，如果还是用原来的，会不会有什么问题？
3. 还有没有别的问题？

谢谢。
"
HanLP,hankcs,184911345,建议采用MapDB项目提供的Map来减少内存占用。,open,提问,https://github.com/hankcs/HanLP/issues/332,"[MapDB](https://github.com/jankotek/mapdb)是一个基于mmap或RAF的通用容器库。
特点就是非常节约Java托管堆。
"
HanLP,hankcs,184773543,CRF分词与自定义词典的结合分词。,open,提问,https://github.com/hankcs/HanLP/issues/331,"Hi，你好，请教一个问题，关于使用CRF统计分词切词时，出现badcase修复，然后希望可以通过自定义词典的方式去调整切词结果。这个功能有实现吗？或者CRF切词出现了badcase，我这边有什么方法去调整呢。
"
HanLP,hankcs,184632453,認為-认为（而非：认為）,open,改进,https://github.com/hankcs/HanLP/issues/330,"to have a particular opinion about sth 認為；視為；相信—— 牛津高阶英汉双解词典第八版（繁体版）

作者是否考虑香港、台湾异体字处理，参见opencc项目
"
HanLP,hankcs,184387079,能否加入拆词和合词相应的支持,open,重复,https://github.com/hankcs/HanLP/issues/329,"“美国签证”->""美国""，""签证""
“中国”，""人民"" ->""中国人民""
参考`https://github.com/ysc/word`的12、refine
"
HanLP,hankcs,183851738,采用默认分词结果不准确,open,提问,https://github.com/hankcs/HanLP/issues/328,"采用默认分词
输入内容为： 金阳观音山支行，分词结果：
[金阳/ns, 观音山/nz, 支行/n]
但是采用 NShortSegment分词，结果为
[金阳观音山支行/nt]

默认的分词实例为：
public static Segment segment = HanLP.newSegment().enableCustomDictionary(true).enableNameRecognize(true).enablePlaceRecognize(true).enableOrganizationRecognize(true);
Nshort分词实例为：
private static Segment nShortSegment = new NShortSegment().enableCustomDictionary(true).enablePlaceRecognize(true).enableOrganizationRecognize(true);

另外一个问题是： 为什么Nshort分词不支持自定义字典？
"
HanLP,hankcs,182997052,如何将HanLP代码与data目录下的词典文件打成一个jar?,open,重复,https://github.com/hankcs/HanLP/issues/325,
HanLP,hankcs,182796163,简繁体转换是否用到jpinyin包？,open,提问,https://github.com/hankcs/HanLP/issues/324,
HanLP,hankcs,181868546,动态自定义词典,open,提问,https://github.com/hankcs/HanLP/issues/320,"saas云平台使用，每个客户都有自己的自定义词典，需要在每次分词过程中将客户的自定义词典传入分词方法。实现针对不同客户的不同分词效果。
分词的配置也是如此。
"
HanLP,hankcs,181089835,请问能否根据机构名、地址名识别方法自定义其它实体识别方法，有通用的模板么？谢谢！,open,提问,https://github.com/hankcs/HanLP/issues/318,
HanLP,hankcs,179970936,请问如果将自己的文本训练为CRF字典？,open,提问,https://github.com/hankcs/HanLP/issues/317,
HanLP,hankcs,178229345,MDAGset的作用?,open,提问,https://github.com/hankcs/HanLP/issues/316,
HanLP,hankcs,177135895,HanLP.s2hk报错未定义,open,提问,https://github.com/hankcs/HanLP/issues/315,"您好，新建了一个测试项目，引用1.2.11版本的jar 一些test下的代码报错，如标题所示，请本这是怎么回事？
"
HanLP,hankcs,176840933,同义词典的格式,open,提问,https://github.com/hankcs/HanLP/issues/314,"hankcs你好，可以解释一下同义词典的格式吗？
"
HanLP,hankcs,175203273,CRF分词出现内存溢出,open,改进,https://github.com/hankcs/HanLP/issues/310,"Hankcs~我在使用各类分词器比较结果时，如果直接使用CRF分词器结果正常，如果前面代码已经运行过一个分词器，接着运行CRF的时候会报内存溢出。
"
HanLP,hankcs,172447833,和elasticsearch2.3.4集成后报错,open,提问,https://github.com/hankcs/HanLP/issues/309,"用1.2.5和es2.3.4集成后，插入es数据和测试es分词效果时报如下错误：
{
  ""error"" : {
    ""root_cause"" : [ {
      ""type"" : ""no_class_def_found_error"",
      ""reason"" : ""no_class_def_found_error: Could not initialize class com.hankcs.hanlp.seg.common.Vertex""
    } ],
    ""type"" : ""no_class_def_found_error"",
    ""reason"" : ""no_class_def_found_error: Could not initialize class com.hankcs.hanlp.seg.common.Vertex""
  },
  ""status"" : 500
}

堆栈为：

Caused by: NotSerializableExceptionWrapper[no_class_def_found_error: Could not initialize class com.hankcs.hanlp.seg.common.Vertex]
        at com.hankcs.hanlp.seg.common.WordNet.<init>(WordNet.java:71)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:40)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:422)
        at com.hankcs.hanlp.seg.common.wrapper.SegmentWrapper.next(SegmentWrapper.java:62)
        at com.hylanda.hanlp.plugin.HanlpTokenizer.incrementToken(HanlpTokenizer.java:101)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.simpleAnalyze(TransportAnalyzeAction.java:247)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:225)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:63)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:282)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:275)
        at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
请问Vertex这个类初始化的时候是不是有操作文件的代码？我怀疑是权限问题
"
HanLP,hankcs,172155112,Hanlp的中文分词和依存句法分析使用的词库是否一致？,open,提问,https://github.com/hankcs/HanLP/issues/308,"请问hancks，Hanlp的中文分词和依存句法分析使用的词库是否一致？对同一段话，分词功能的段词方法是否会跟依存句法分析功能不一致？多谢大神！
"
HanLP,hankcs,171349924,怎么将HanLP的data和source一起打包,open,重复,https://github.com/hankcs/HanLP/issues/304,"现在需要修改data中停用词的一些数据，然后将source和data重新打包成jar，然后再开发中使用，其中data打包成类似官方sbt中的二进制形式，请问要怎么做啊？ 
"
HanLP,hankcs,171309128,自定义词典分词问题,open,提问,https://github.com/hankcs/HanLP/issues/303,"遇到这么一个问题. 请指教
输入 ""微搜集成手册""     分词结果   [微, 搜集, 成, 手册]
如果把  ""微搜"" 加入自定义词典中.  分词结果还是这样.
在 WordBasedGenerativeModelSegment 中的GenerateWordNet方法 生成词网,注释了自定义分词的代码.   如果开启后. 可以得到分词结果  [微搜, 集成, 手册].   这段代码开启是否有其他影响
![image](https://cloud.githubusercontent.com/assets/4652356/17685809/050db75a-639a-11e6-9e72-58464762d8bf.png)

在生成词网后再引入自定义词典是不是就只能对未成词的字进行合并了
if (config.useCustomDictionary)
 {
        combineByCustomDictionary(vertexList);
}
"
HanLP,hankcs,171302697,使用AhoCorasickDoubleArrayTrieSegment分词，自定义词典加载成功，自定义词典中有一条为“酸 n 1000”和“碱 n 1000” “金属 n 1000”，如果待分词的句子为“酸的”，分词结果为“酸的 nz”。如果是“金属的”，结果就正常，为“金属 n  的nz”，经测试的，自定义词典中有单个字的情况都有这个bug,open,提问,https://github.com/hankcs/HanLP/issues/302,
HanLP,hankcs,170804310,全国行政区划地址,open,改进,https://github.com/hankcs/HanLP/issues/300,"最近工作需要从统计局网站爬的一个数据，看有需要不？百度云链接: https://pan.baidu.com/s/1kUADAYV 密码: hau3
"
HanLP,hankcs,170789137,java Servlet 中 out.println(word.HEAD);无法输出,open,无效,https://github.com/hankcs/HanLP/issues/299,"   对于CoNLL格式，很奇怪，其他字段都可以输出，唯独HEAD，加了这行代码就错误。
             CoNLLSentence sentence = HanLP.parseDependency(""""+u);
            //out.println(sentence);
            // 可以方便地遍历它
           for (CoNLLWord word : sentence)
            {
                out.println(word.LEMMA);
                out.println(word.CPOSTAG);
               // out.println(word.HEAD);
                out.println(word.DEPREL);  
            }
"
HanLP,hankcs,170785332,java servlet调用依赖分析，除了加载配置文件。还需要显式输出一下才可以,open,无效,https://github.com/hankcs/HanLP/issues/298,"博主，你好。感谢你的付出。我在调用依赖分析的时候。发现
                        Properties properties = new Properties();
                        //第一行代码
            properties.load(getServletContext().getResourceAsStream(""/WEB-INF/hanlp.properties""));
                        //第二行代码
            out.println(""root=""+properties.getProperty(""root""));
                         //第三行代码
                  如果只有第一行和第二行代码，调试不成功。如果执行一遍第三行代码，调试就通过。
                   如果调试通过，注释第三行代码，依然可以通过。不知道这个是怎么个原理。
"
HanLP,hankcs,169882536,用户自定义词典,open,提问,https://github.com/hankcs/HanLP/issues/294,"CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 我的词典.txt;
通过这种方式追加了自己创建的词典，但是使用的时候并没有调用
"
HanLP,hankcs,168500860,CRFmodel计算结果与crf++的crf_test结果不一致。,open,求助,https://github.com/hankcs/HanLP/issues/285,"CRFmodel加载了crf++ 0.58版本训练出来的模型后，tag后的结果与采用该crf++自带的crf_test计算出来的结果不一样。这是为什么呢，算法有什么不一样的地方吗？从结果来看crf_test的结果要准确很多，CRFmodel计算出来的结果很多是错误的。
`CRFModel model = CRFModel.loadTxt(""F:\\studio\\CRF++-0.58\\CRF++-0.58\\example\\model.txt"",
                new CRFModel(new DoubleArrayTrie<FeatureFunction>()));
        System.out.println(model);
        Table table = new Table();
        table.v = new String[][]{
            {""微鲸"", ""ns"", null},
            {""创新"", ""vn"", null},
            {""营销"",  ""n"", null},
            {""抢占"",  ""v"", null},
            {""电视"",  ""n"", null},
            {""先机"",  ""n"", null}
        };
        model.tag(table);
        System.out.println(table);`

[crf_train.txt](https://github.com/hankcs/HanLP/files/392253/crf_train.txt)
[crf_test.txt](https://github.com/hankcs/HanLP/files/392252/crf_test.txt)
"
HanLP,hankcs,168291545,请教一个问题，如何使用索引分词模式并删除停用词？,open,提问,https://github.com/hankcs/HanLP/issues/283,
HanLP,hankcs,168270020,使用java servlet 实现有点问题。,open,提问,https://github.com/hankcs/HanLP/issues/282,"博主，你好，辛苦了。我用java servlet 调用语言包，运行不成功，这是测试代码。调试运行，提示“网站无法显示该页面/  HTTP 500 /最可能的原因是:
•该网站正在进行维护。
•该网站有程序错误。
”
 请教哪里出问题了。

我用的编译器是eclipse-jee-neon-R-win32，   demo在控制台模式下没有问题。这是部分代码

import java.io.IOException;
import java.io.PrintWriter;
import javax.servlet.ServletException;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

import com.hankcs.hanlp.HanLP;

public class HelloWorld extends HttpServlet implements javax.servlet.Servlet{

```
    public HelloWorld() {
        super();
    }       
    protected void doGet(HttpServletRequest request, HttpServletResponse response) 
        throws ServletException, IOException {
        //response.getWriter().write(""Hello"");
        request.setCharacterEncoding(""utf-8"");
        response.setContentType(""text/html;charset=utf-8"");
        PrintWriter out = response.getWriter();
        String u=request.getParameter(""userinput"");
        out.println(""<br/>输入:""+u);
        out.println(""首次编译运行时"");
        out.println(HanLP.segment(""你好，欢迎使用HanLP汉语处理包！接下来请从其他Demo中体验HanLP丰富的功能~""));
    ....................................
```
"
HanLP,hankcs,167783983,CRFModel的构造函数有个问题，直接使用会失败！,open,无效,https://github.com/hankcs/HanLP/issues/281,"你好，想通过你的CRFModel的loadBin加载你自带的bin模型或使用loadTxt加载自己训练的crf++模型。加载没有报错，但是tag工作却提示空指针。

看了你CRFSegmentModel类的加载代码，这里你用的是`new BinTrie<FeatureFunction>()`初始化CRFModel，而在CRFModel的构造函数中使用的是`new DoubleArrayTrie<FeatureFunction>()`。这个是不是版本升级，不一致问题。

修改相关的DoubleArrayTrie为BinTrie后，tag函数工作正常。
"
HanLP,hankcs,167498487,CRF新词识别,open,重复,https://github.com/hankcs/HanLP/issues/280,"Segment segment = new CRFSegment();
segment.enablePartOfSpeechTagging(true);
List<Term> termList = segment.seg(""你看过穆赫兰道吗"");
System.out.println(termList);
for (Term term : termList)
{
    if (term.nature == null)
    {
        System.out.println(""识别到新词："" + term.word);
    }
}

用master分支上的代码运行这段代码，识别不出新词。运行结果如下：
[你/rr, 看过/v, 穆赫兰道/nz, 吗/y]
"
HanLP,hankcs,167228549,运行自定义词性Demo，NoSuchFieldException:$VALUES,open,改进,https://github.com/hankcs/HanLP/issues/279,"您好，使用MacBook Pro 10.11，Eclipse Mars.1 ，JDK1.7，运行1.2.10版本的DemoCustomNature.java后如下报错：
七月 24, 2016 6:55:03 下午 com.hankcs.hanlp.corpus.util.CustomNatureUtility <clinit>
警告: 已激活自定义词性功能,由于采用了反射技术,用户需对本地环境的兼容性和稳定性负责!
如果用户代码X.java中有switch(nature)语句,需要调用CustomNatureUtility.registerSwitchClass(X.class)注册X这个类
Exception in thread ""main"" java.lang.IllegalArgumentException: Could not create enum
    at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:99)
    at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:68)
    at com.hankcs.hanlp.corpus.util.CustomNatureUtility.addNature(CustomNatureUtility.java:58)
    at com.hankcs.hanlp.corpus.tag.Nature.create(Nature.java:829)
    at com.hankcs.demo.DemoCustomNature.main(DemoCustomNature.java:41)
Caused by: java.lang.IllegalArgumentException: Could not create the class
    at com.hankcs.hanlp.corpus.util.EnumBuster$Memento.<init>(EnumBuster.java:453)
    at com.hankcs.hanlp.corpus.util.EnumBuster$Memento.<init>(EnumBuster.java:439)
    at com.hankcs.hanlp.corpus.util.EnumBuster.make(EnumBuster.java:91)
    ... 4 more
Caused by: java.lang.NoSuchFieldException: $VALUES
    at java.lang.Class.getDeclaredField(Class.java:1961)
    at com.hankcs.hanlp.corpus.util.EnumBuster.findValuesField(EnumBuster.java:349)
    at com.hankcs.hanlp.corpus.util.EnumBuster.values(EnumBuster.java:429)
    at com.hankcs.hanlp.corpus.util.EnumBuster.access$0(EnumBuster.java:426)
    at com.hankcs.hanlp.corpus.util.EnumBuster$Memento.<init>(EnumBuster.java:443)
"
HanLP,hankcs,166772393,歧义分词解决,open,提问,https://github.com/hankcs/HanLP/issues/277,"我第一次试这个hanlp，觉得分词很好，试了一些容易引起歧义句子，大部分没问题。以下4句是CRFSeg出错的：
""在这些企业中国有企业有十个"",
""新建地铁中将禁止商业摊点"",
""方程的解除了零以外还有其它的…"",
""这的确定不下来"",

CRFSeg的结果：
[在, 这些, 企业, 中国, 有, 企业, 有, 十个]
[新建, 地铁, 中将, 禁止, 商业, 摊点]
[方程, 的, 解除, 了, 零以外, 还有, 其它, 的, …]
[这, 的, 确定, 不, 下来]

segment(标准分词)的结果：
[在/p, 这些/rz, 企业/n, 中/f, 国有企业/nz, 有/vyou, 十/m, 个/q]
[新建/v, 地铁/n, 中/f, 将/d, 禁止/v, 商业/n, 摊点/n]
[方程/n, 的/ude1, 解除/v, 了/ule, 零/m, 以外/f, 还有/v, 其它/rz, 的/ude1, …/w]
[这/rzv, 的/ude1, 确定/v, 不/d, 下来/vf]

标准分词中前两个对了，后两个依然是错的。我的问题是：
1、segment使用的什么方法？
2、可以通过什么办法进一步改进分词的准确率吗？（你的分词准确率已经很好了）

谢谢！

Henry
"
HanLP,hankcs,166509884,默认的繁体转换好奇怪,open,重复,https://github.com/hankcs/HanLP/issues/276,"舞台：舞颱
因为：囙爲
青梅竹马：青楳竹馬
"
HanLP,hankcs,166048916,按第二种方法配置，调用依赖解析Demo发生错误。,open,提问,https://github.com/hankcs/HanLP/issues/275,"博主，
你好，感谢你的付出。我在测试依存关系demo的时候发生了这个错误，请教哪里没有设置对？我是按第二种方法配置的。即使用了hanlp.properties
使用的工具是
Eclipse IDE for Java Developers
Version: Mars.2 Release (4.5.2)
demo 是 DemoDependencyParser

Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    at com.hankcs.hanlp.dependency.nnparser.Matrix.load(Matrix.java:1305)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkParser.load(NeuralNetworkParser.java:259)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkParser.load(NeuralNetworkParser.java:135)
    at com.hankcs.hanlp.dependency.nnparser.parser_dll.<clinit>(parser_dll.java:34)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkDependencyParser.parse(NeuralNetworkDependencyParser.java:53)
    at com.hankcs.hanlp.dependency.AbstractDependencyParser.parse(AbstractDependencyParser.java:46)
    at com.hankcs.hanlp.dependency.nnparser.NeuralNetworkDependencyParser.compute(NeuralNetworkDependencyParser.java:93)
    at com.hankcs.hanlp.HanLP.parseDependency(HanLP.java:407)
    at hanlp.DemoDependencyParser.main(DemoDependencyParser.java:26)
"
HanLP,hankcs,166010739,java.lang.ExceptionInInitializerError,open,提问,https://github.com/hankcs/HanLP/issues/274,"hankcs好，我用Python调用hanlp做依存句法分析时遇到如下问题，请问该怎么解决？ps：使用分词等功能时正常，多谢！！！
![sft cu_6w3pp c0 9aa11](https://cloud.githubusercontent.com/assets/19303343/16905158/5a4487a8-4cd4-11e6-97e0-8cf378bc35d9.png)
"
HanLP,hankcs,165974928,NLPTokenizer分词时用到的核心词典转移矩阵,open,无效,https://github.com/hankcs/HanLP/issues/272,"我现在用NLPTokenizer分词方法进行分词，在Nature中添加词性text，把核心词典中合肥市的词性标注为text，但是会出现下面的错误，
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 148
    at com.hankcs.hanlp.algoritm.Viterbi.compute(Viterbi.java:121)
    at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.speechTagging(WordBasedGenerativeModelSegment.java:531)
    at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:118)
    at com.hankcs.hanlp.seg.Segment.seg(Segment.java:454)
    at com.hankcs.hanlp.tokenizer.NLPTokenizer.segment(NLPTokenizer.java:37)
    at com.hankcs.test.seg.Test.main(Test.java:11)

显示应该是核心词典转移矩阵的问题，核心矩阵中不包含新的词性，那这样的话，核心词典转移矩阵需要重新生成吗，要怎么生成啊
"
HanLP,hankcs,164865703,Class com.hankcs.hanlp.HanLP not found,open,提问,https://github.com/hankcs/HanLP/issues/270,"hankcs大神，调用hanlp时碰到如下问题，怎么解决？另外，对项目文档里的这句话也不能理解“最后将HanLP.properties放入classpath即可，对于任何项目，都可以放到src或resources目录下，编译时IDE会自动将其复制到classpath中。‘’,求解答？多谢！！！

![78_d1 v13tj1r_6 0 lqu9w](https://cloud.githubusercontent.com/assets/19303343/16736512/d3455124-47bf-11e6-94b4-b6d686e2c43c.png)
"
HanLP,hankcs,163787199,地址集体识别,open,提问,https://github.com/hankcs/HanLP/issues/267,"您好，我是第一次使用hanlp，我想请教一下对于“北京市海淀区学院路31号北京航空航天大学体育馆”这个整体地址有什么方法可以集体一次性识别出来吗？而不是北京市/ns海淀区/ns学院路/nz31/m号/q北京航空航天大学/nt体育馆/n这中怎么把他们变为北京市海淀区学院路31号北京航空航天大学体育馆/ns。麻烦您了
谢谢您。
"
HanLP,hankcs,163772663,HanLP自定义字典不生效,open,提问,https://github.com/hankcs/HanLP/issues/266,"您好，我按照官网的方法配置了properties，而且也进行了对bin的删除，但是就是不能进行对我的字典.txt里的添加的词进行识别，字典里定义了人才报告厅，麻烦您帮我看一下。
![qq 20160705130443](https://cloud.githubusercontent.com/assets/20194150/16574453/a1e3515c-42b1-11e6-91f3-750f619e34d2.png)
![qq 20160705130632](https://cloud.githubusercontent.com/assets/20194150/16574452/a1d56236-42b1-11e6-8783-48e99c4c4cd9.png)
![qq 20160705130748](https://cloud.githubusercontent.com/assets/20194150/16574451/a1d3eeba-42b1-11e6-9cd1-5384baadecda.png)
"
HanLP,hankcs,163641088,自行添加custom词典问题,open,提问,https://github.com/hankcs/HanLP/issues/263,"目前应用是在一个具体领域，希望分词结果以出现在我自行添加的词典中的名词为最高优先级，词典添加方式参照了指引、在property文件中加了配置，并删除了bin文件让它重新生成。目前看效果有的词生效有的没有，还是会被切开。使用indextokenizer，不知道是否哪里没弄对？例子：“正中神经”，切为正中和神经，谢谢！
"
HanLP,hankcs,163556943,方言的字典處理挑戰,open,提问,https://github.com/hankcs/HanLP/issues/261,"你好，

我在建立方言(香港)字庫/同義詞

我發現方言的處理有些時候不容易處理，例如同義詞便可用三個不同的字典處理：
1. 繁體字典
什麼=甚麼
1. 同義詞
   不好=唔好
2. 方言CustomDictionary
   有無攪錯 

例子3 是比較好處理的，因為不影響其他字詞的分析。

例子1, 2 會影響現存字庫的分析結果。

我想這問題和一般 domain 問題的處理有不少相似。

例如能否在hanlp.properties 加入section/domain ，可以 在運算時換dictionary/corpus.
"
HanLP,hankcs,163420823,关于自定义词典的处理,open,提问,https://github.com/hankcs/HanLP/issues/259,"关于自定义词典的处理有几个疑问，请教一下
1.目前自定义词典都是用于对粗分词结果进行合并处理的，不知道这个是否理解有误？
2.对于粗分词为""AB/C""情况下，如果需要调整分词为""A/BC""是否必须修改核心词典CoreNatureDictionary.txt？
3.如果想动态修改核心词典，除了文件系统修改再触发加载词典这种方法外，是否可以直接通过内存变量修改实现？
"
HanLP,hankcs,163336347,假设有一个自定义的词库，输入一个词或者语句去寻找出最相近的词,open,提问,https://github.com/hankcs/HanLP/issues/258,"大神， 你这个项目目前能够实现这个需求吗， 文字上接近匹配就可以
"
HanLP,hankcs,163091386,按标点符号分割文章，用哪个方法,open,提问,https://github.com/hankcs/HanLP/issues/257,"按标点符号分割文章，用哪个方法
"
HanLP,hankcs,163073328,C#中调用Hanlp.dll文件执行程序出现以下错误,open,提问,https://github.com/hankcs/HanLP/issues/256,"![image](https://cloud.githubusercontent.com/assets/19245664/16475947/29d8d9b6-3eb5-11e6-8873-5a4770a76945.png)
"
HanLP,hankcs,162910990,这个能不能实现搜索联想推荐,open,提问,https://github.com/hankcs/HanLP/issues/255,"现在可以实现x5巧克力 分词推荐成巧克力。
 采用同义词词库推荐的话，有没有方法只获取到同义词词库中的词， 目前用elasticsearch的ik分词和同义词filter过滤， 或出现很多不相关的词，比如方便面，会出来方便，泡面...  怎么让不在分词词库中的词不出来。
可以认为有没有实现根据一个指定的词库，有同义词的概念，只分词成词库中词，不在词库中的丢弃
"
HanLP,hankcs,162839118,词的细分问题,open,提问,https://github.com/hankcs/HanLP/issues/253,"当我输入“洗车行，理发店，游泳馆”这类词时，它们被切分成""洗/nz, 车行/nis, ，/w, 理发店/nis, ，/w, 游泳馆/n]""。其实正确的切分应该时“洗车／行/, 理发／店/ , 游泳／馆/n]”，这样的话在搜索“洗车，理发，游泳”时也能精确匹配到
"
HanLP,hankcs,162735959,词典加载错误,open,重复,https://github.com/hankcs/HanLP/issues/252,"下载的600m的data词典，删除dic中bin类型的文件，在本地ide中能加载生成bin文件，也能正常分词，但是打包放到服务器上，生成bin 的时候报错，想请问下是什么原因呢，信息如下：
`at com.hankcs.hanlp.seg.Segment.quickAtomSegment(Segment.java:161)
        at com.hankcs.hanlp.seg.WordBasedGenerativeModelSegment.GenerateWordNet(WordBasedGenerativeModelSegment.java:458)
        at com.hankcs.hanlp.seg.Viterbi.ViterbiSegment.segSentence(ViterbiSegment.java:42)
        at com.hankcs.hanlp.seg.Segment.seg(Segment.java:454)`

`Caused by: java.lang.ArrayIndexOutOfBoundsException: 32618
        at com.hankcs.hanlp.utility.ByteUtil.bytesHighFirstToChar(ByteUtil.java:255)
        at com.hankcs.hanlp.corpus.io.ByteArray.nextChar(ByteArray.java:81)
        at com.hankcs.hanlp.dictionary.other.CharType.<clinit>(CharType.java:77)
        ... 15 more`
"
HanLP,hankcs,162054431,新词发现的问题,open,提问,https://github.com/hankcs/HanLP/issues/248,"请教一个问题，从您的文档中，我知道可以透过自定义词典的方式来加入新词(TXT或代码中直接加入)，经实际测试，自定义的新词都可以被辨识的很好。但想请教一个更根源的应用问题，假设待处理的文档源源不断的进来，则可以透过什麽方式来发现新词呢？
比方您的文档提到，CRF发现新词能力较强，我做了一个实验，输入带有「理都懂，然并卵，城会玩，日了狗」等网路语言，我发现这些人民日报语料库里不大可能出现的词，都不会被识别为整词，而是被拆分为粒度更小的词，并识别出来，换言之，它们不被识别为新词。
即使是透过CRF这种辨识能力强的算法，这样的问题有没有解？是不是一定得透过语料库才有办法处理，比方在训练CRF模型的时候，语料库里就必须有这些网路词？
"
HanLP,hankcs,161601438,"hankcs, 为啥在CoreNatureDictionary.tr.txt里没有Yg标签啊？",open,提问,https://github.com/hankcs/HanLP/issues/246,"是因为它出现的次数老少了吗，在那个北大标注集里有这个标签的
"
HanLP,hankcs,160596681,请教一下,open,提问,https://github.com/hankcs/HanLP/issues/245,"小白请教一个问题，请问您这个data目录中的dictionary下面的数据是哪边来的啊？
"
HanLP,hankcs,160363301,开启词性问题？,open,提问,https://github.com/hankcs/HanLP/issues/244,"大神好，我用默认的标准分词器，显式开启词性与不设置，分词后都能够获取词性？

List<Term> list=HanLP.newSegment().enablePartOfSpeechTagging(true).seg(document);

List<Term> list = HanLP.segment(document);

上面两种 分词结果都能够获取词性（貌似源码里面默认配置是不开启词性的）
"
HanLP,hankcs,160119167,TraditionalChineseToker cannot use CustomerDictionary,open,提问,https://github.com/hankcs/HanLP/issues/242,"I am testing different tokenizer performance, I found that the TraditionalChineseTokenizer does not apply the CustomDicionary as the others do.

```
CustomDictionary.add(""銀主盤"", ""nz 1024 n 1"");
  Segment segment = HanLP.newSegment().enableCustomDictionary(true).enableNameRecognize(true);

        for (String sentence : testCase)
        {
            System.out.println( ""\n\n"" + ""Sentence"" + sentence);
            System.out.println(""HanLP"");
            List<Term> termList = segment.seg(sentence);
            System.out.println(termList);

            termList = TraditionalChineseTokenizer.segment(sentence);
            System.out.println(""TraditionalChineseTokenizer"");
            System.out.println(termList);


            termList = NLPTokenizer.segment(sentence);
            System.out.println(""NLPTokenizer"");
            System.out.println(termList);
        }
```

Results
Sentence (Only TraditionalChineseTokenizer does not apply the ""銀主盤"" new word in CustomDictionary.)

```

銀主盤,好多人等緊樓市大跌!
HanLP
[銀主盤/nz, ,/w, 好多/mq, 人/n, 等/udeng, 緊/n, 樓/n, 市/n, 大跌/v, !/w]
TraditionalChineseTokenizer
[銀/ng, 主盤/n, ,/w, 好多/mq, 人/n, 等/udeng, 緊/d, 樓市/n, 大跌/v, !/w]
NLPTokenizer
[銀主盤/nz, ,/w, 好多人/nt, 等/udeng, 緊/n, 樓/n, 市/n, 大跌/v, !/w]
```
"
HanLP,hankcs,159996498,追加繁簡體詞典,open,重复,https://github.com/hankcs/HanLP/issues/240,"在追加字典的例子中：

```
8. 用户自定义词典
...
追加词典

    CustomDictionary主词典文本路径是data/dictionary/custom/CustomDictionary.txt，用户可以在此增加自己的词语（不推荐）；也可以单独新建一个文本文件，通过配置文件CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 我的词典.txt;来追加词典（推荐）
```

我明白推荐新加一個詞典。

那我新的詞典應否把繁體和簡體分開？ (有沒有實際的效果/效能影響)。
例如我新加一個`經濟字典.txt`，是否如下新增最佳？

`````` 經濟字典.zh-TW.txt```
```經濟字典.zh-CN.txt```

``````
"
HanLP,hankcs,159822906,CRF的特征模板和特征函数,open,提问,https://github.com/hankcs/HanLP/issues/238,"对CRF不是特别理解想请教下
1 特征模板是自己手工编写的吗? 如果有有很多行普通文本，是1行文本就是一个特征模板吗？
2 对于CRF++,如果有语料和特征模板是不是就可以生成模型咯?
3  第2点如果是，那语料该从那里获取呢，它和(1)中的普通文本有什么关系。
"
HanLP,hankcs,159784162,"如何建立 情感分析, 识别文本蕴涵(RTE)等平台",open,求助,https://github.com/hankcs/HanLP/issues/236,"在英文文本挖掘，可以找到一些情感分析, 识别文本蕴涵 的應用和平台。

在中文文本挖掘，我找到的是東拼西湊的方案，例如 python 的nltk +jieba (分詞)。

作為統一的中文文本挖掘系統，HanLP 是非常合適的。 

HanLP 有沒有相關的方法/方案，能建成如下的平台：

情感分析、 识别文本蕴涵(RTE)系統、Hadoop/Spark 結合、机构名识别模块的核心模型 (http://www.hankcs.com/nlp/ner/place-name-recognition-model-of-the-stacked-hmm-viterbi-role-labeling.html)

我十分樂意參與相關開發。
"
HanLP,hankcs,158487368,SpeedTokenizer分词器词性提取不出来,open,提问,https://github.com/hankcs/HanLP/issues/228,"您好，StandardTokenizer标准分词器可以提取出词性，但是SpeedTokenizer极速分词器提取不出词性，是什么原因导致的。
"
HanLP,hankcs,158286375,关于词典打包问题,open,无效,https://github.com/hankcs/HanLP/issues/226,"我修改了hanlp算法里的功能，重新打包成jar文件，其他程序调用，运行成功。但是把调用程序打包成.jar文件严重: 没有找到HanLP.properties，可能会导致找不到data
========Tips========
请将HanLP.properties放在下列目录：
Web项目则请放到下列目录：
Webapp/WEB-INF/lib
Webapp/WEB-INF/classes
Appserver/lib
JRE/lib
并且编辑root=PARENT/path/to/your/data,请问这个问题怎么解决，我尝试把对应的properites版本修改打包调用的时候报错：核心词典加载失败。
"
HanLP,hankcs,158068261,如果crf训练等需要用到好一点的服务器,open,改进,https://github.com/hankcs/HanLP/issues/225,"如果crf训练等需要用到好一点的服务器
可以找我搞一个独立linux服务器环境给你使用

jimichan@gmail.com
"
HanLP,hankcs,157870594,data/dictionary/person/familyname.txt并不存在,open,提问,https://github.com/hankcs/HanLP/issues/224,"在FamilyName类中会去读取familyname.txt，然而这个文件并不存在
"
HanLP,hankcs,157824764,分词和句法依存的一些疑问,open,提问,https://github.com/hankcs/HanLP/issues/223,"比如样本数据如下：
李明和高洁一起来到岭南区的百货商场附近，联系了李师傅，手机号18622004501，共同乘坐了一辆牌照为京H56802的大众轿车去北京站。
1.手机、车牌这种号码分词和词性不准，能否将这类号码类分词纠正，能否扩展几个词性如：电话号码、车牌号码。
李明和/nr, 高洁/a 人名划分不对
2.句法分析参看了一下哈工大http://www.ltp-cloud.com/demo/，语义角色标注通过程序怎么获取？
"
HanLP,hankcs,156672579,请问一下有没有把书名号，引号在分词的时候合并成一个词的情况加进来,open,无效,https://github.com/hankcs/HanLP/issues/218,"不会玩java，都不知道怎么改
"
HanLP,hankcs,156215197,HanLP.parseDependency 问题,open,忽略,https://github.com/hankcs/HanLP/issues/215,"HanLP.parseDependency(""美丽又善良的你被卑微的我深深的喜欢着"");
结果
美丽 --(定中关系)--> 你
又 --(状中结构)--> 善良
善良 --(并列关系)--> 美丽
的 --(右附加关系)--> 美丽
你 --(主谓关系)--> 喜欢
被 --(状中结构)--> 卑微
卑微 --(定中关系)--> 我
的 --(右附加关系)--> 卑微
我 --(主谓关系)--> 喜欢
深深 --(状中结构)--> 喜欢
的 --(右附加关系)--> 深深
喜欢 --(核心关系)--> ##核心##
着 --(右附加关系)--> 喜欢

其中这块应该不是主谓关系吧
你 --(主谓关系)--> 喜欢
"
HanLP,hankcs,156184867,请问词共现统计如何使用,open,提问,https://github.com/hankcs/HanLP/issues/214,"看到语料库工具里介绍有词共现统计功能，但是帮助文档里没有介绍，请问如何使用
"
HanLP,hankcs,156113132,char数组使用建议,open,改进,https://github.com/hankcs/HanLP/issues/212,"程序中多出使用
char[] chars = string.toCharArray()
然后通过下标访问char ，如 chars[i] 这类代码

其实JDK中toCharArray的具体实现是
 char result[] = new char[value.length];
        System.arraycopy(value, 0, result, 0, value.length);
相当于每次toCharArray就会多出堆上copy的动作

建议直接使用 
string.charAt(i)代替

测试结论：charAt方法在性能稍微优势的情况下，减少GC消耗

=========性能测试代码=============
public static void main(String[] args) {
        String string = ""abcedfghijklml"";
        int len = string.length();
        char[] cc = string.toCharArray();

```
    for(int k=0;k<len;k++){
        char x = cc[k];
    }

    for(int k=0;k<len;k++){
        char x = string.charAt(k);
    }
    long t1 = System.currentTimeMillis();
    for(int i=0;i<500000;i++){
        char[] c = string.toCharArray();

        for(int k=0;k<len;k++){
            char x = c[k];
        }
    }
    long t2 = System.currentTimeMillis();       
    for(int i=0;i<500000;i++){
        for(int k=0;k<len;k++){
            char x = string.charAt(k);
        }
    }
    long t3 = System.currentTimeMillis();

    System.out.println(t2-t1);
    System.out.println(t3-t2);
}
```
"
HanLP,hankcs,155671376,能否把繁简转换的字库分开？,open,改进,https://github.com/hankcs/HanLP/issues/211,"在进行繁简转换时会遇到以下问题：
1.一个简体字对应多个繁体字的情况，比如：
台    臺 檯  颱 台
几    幾 機 几
在字库里面写：
臺=台
檯=台
颱=台
现在如果只是转换简体 “台”，它会转成哪一个？最上面一个？

如果字库里面只记录第一条 “臺=台”  那如果繁体里面有 “檯” 字， 就转不了简体了。

请问可不可以把繁简转换和简繁转换的字库分开？
"
HanLP,hankcs,155648007,关于机构识别,open,提问,https://github.com/hankcs/HanLP/issues/210,"K,L,M,P,S,W这几个标注分别代表什么？我只找到了一部分的标注的意义。
但是这几个的意义在网页上找不到。
我最近手工标注了一批语料。
汇总的结果如下:
模式  正确  错误  总共  正确率
CD  513 2512    3025    0.169586777
GD  2055    564 2619    0.78465063
FD  1695    694 2389    0.709501884
CCD 555 1790    2345    0.236673774
有全部的标注语料，但是没贴出来。
有没有什么比较好的改进建议？
我目前想到的只有一些统计加规则的方法。比如：CD的正确率较低。就定一些类型C和D无法成词。
"
HanLP,hankcs,154295467,Question:关于portable,open,提问,https://github.com/hankcs/HanLP/issues/209,"您好，我想请教一下如果我在maven配置中version为portable的话是否本地不用再配置字典了（dictionary）？
"
HanLP,hankcs,154199176,Unable to start JVM at src/native/common/jp_env.cpp:54,open,无效,https://github.com/hankcs/HanLP/issues/208,"hancks你好，我在用python调用hanlp时，如果重复调用，程序会报错，并且提示：Unable to start JVM at src/native/common/jp_env.cpp:54，尝试过换编译器，未能解决，拜托给看看，先谢过！
代码如下：

![ail okn 15t 6 pkg petfn](https://cloud.githubusercontent.com/assets/19303343/15182773/c1d0a396-17c0-11e6-8d03-1d1e178586ec.png)
"
HanLP,hankcs,154152138,字典新增一列属性，代码修改量是不是很大。,open,提问,https://github.com/hankcs/HanLP/issues/207,"字典现在的格式是“词 词性 词频”，我想改为“词 词性 词频 专业词”，举例说明：
“查 v 1000” 改为 “查 v 1000 查询”，
"
HanLP,hankcs,153611517,繁体分词,open,提问,https://github.com/hankcs/HanLP/issues/206,"当前分词词典和模型好像都是基于简体中文的，效果不错。但是对于繁体中文,似乎效果不佳，即使加入一些词语，也还是会被当做简体中文出来。

```
    CustomDictionary.add(""捷運站"");
    CustomDictionary.add(""臺北"");

""臺北大眾捷運股份有限公司""
[臺/n, 北大/j, 眾/nz, 捷/j, 運/n, 股份有限公司/nis]
```

但是如果把文本转化为简体，分词就蛮不错的。

```
input = HanLP.convertToSimplifiedChinese(input);
[台北/ns, 大众/n, 轻轨/n, 股份有限公司/nis]
```
1. 是否有比较好的繁体词典推荐
2. 能否支持繁体中文分词（分词时候可以有繁体模型和简体模型）
3. 是否可以根据转化为简体的结果来分原来的句子（根据简体中文分词的位置）
"
HanLP,hankcs,152736180,弱问如何提取固定格式的文本？,open,提问,https://github.com/hankcs/HanLP/issues/204,"你好hankcs, 如日期[2016/m, 年/qt, 4/m, 月/n, 29/m, 日/b], 如何设定[m, qt, m, n ,m , b]规则来提取这样的日期格式文本。工具里有现成的么？因为用python，看java不太熟练。
bow
"
HanLP,hankcs,152707391,目前hanlp能对接ES么？,open,提问,https://github.com/hankcs/HanLP/issues/203,"<(￣︶￣)>！
"
HanLP,hankcs,152478327,超赞学习中,open,,https://github.com/hankcs/HanLP/issues/202,"超赞学习中
"
HanLP,hankcs,152029356,句法分析问题,open,无效,https://github.com/hankcs/HanLP/issues/201,"你好，请问用HanLp进行句法分析，是不进行句法成分标识的吗？
"
HanLP,hankcs,151552280,python 调用句法分析， 怎么获得对应的结果？,open,提问,https://github.com/hankcs/HanLP/issues/198,"使用python调用HanLP，用下面的代码调用得到了st， 请问怎么获得里面的内容呢？ 为什么用for word in st的方式获取不了。 python应该如何调用？ 有人可以写一个简单的例子吗，多谢！ （类似官网的例子：word.LEMMA, word.DEPREL, word.HEAD.LEMMA）

`st = HanLP.parseDependency(u""徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。"")
`
"
HanLP,hankcs,151543682,自定义词典的词数量或是档案大小是否有限制？,open,无效,https://github.com/hankcs/HanLP/issues/197,"hankcs,您好！

   想请教您自定义词典的词数量或是档案大小是否有限制？超过百万以上的词典是否可以载入？先前以solr5.2搭配载入200万以上的词典，系统会崩溃，想请您协助，谢谢

Vincent
"
HanLP,hankcs,151073497,CRF新词识别,open,提问,https://github.com/hankcs/HanLP/issues/196,"这个方法一旦开启自定义词典就无法识别新词了么？
Segment segment = new CRFSegment();
segment.enableCustomDictionary(true);// 开启自定义词典
所以enableCustomDictionary()必须设置为false?
感觉这样有点不太合理啊。
我还要吐槽！！！！
感觉对数字和字母的识别意义不大。如：
public static void main(String[] args) {
        Segment segment = new CRFSegment();
        segment.enableCustomDictionary(false);// 开启自定义词典
        segment.enablePartOfSpeechTagging(true);
        List<Term> termList = segment.seg(""    阿莫西林是""
                + ""TJGP-2015-ZP-5722,""
                + ""乐视超级手机能否承载贾布斯的生态梦 ""
                + ""停业整顿停业"");
        System.out.println(termList);
        for (Term term : termList) {
            if (term.nature == null) {
                System.out.println(""识别到新词："" + term.word);
            }
        }

```
}
```

结果：
识别到新词：  
识别到新词：阿莫西林
识别到新词：TJGP
识别到新词：-
识别到新词：2015
识别到新词：-
识别到新词：ZP
识别到新词：-
识别到新词：5722
识别到新词：,
识别到新词：贾布斯
识别到新词： 
识别到新词：停业

这些汉字的词，我觉得还行。但是这里各种非汉字都会被识别成新词。我觉得这个问题造成这个方法的可用性。大大降低啊。

我目前想做一个给很多新的文章，识别出文章里面新的词。
现有的方法：
使用过的方法。先分词，如果有一个一个的字没有成词。如：我的名字-李健博不会成任何词。就把他们组合起来。用HMM，或者CRF识别一下是不是新词。
优点：能够剔除上面[TJGP,2015]这样的词，召回应该是比较高的。
缺点：准确率低。大量的(实体名，尤其是人名，会混在里面)使得还需要大量人工。
创新方法：
用上面的方法和hanlp结合：
比如:李健博在hanlp中会被识别为人名，这样的词就不要了。
楼主觉得我这个创意咋样？因为我对hanlp不是太了解。所以希望楼主看一看这样做的可行性。
个人觉得hanlp的CRF方法很好，但是因为[TJGP,2015]这样的原因使得无法真正的投入使用，实在是十分可惜。
"
HanLP,hankcs,149664590,请问是否将来会加入情感分析接口,open,提问,https://github.com/hankcs/HanLP/issues/192,"1. 就是对文本进行一个情感值的量化分析，输出一个代表正负面情感的数值，比如0是绝对负面，1是绝对正面，输出的结果就在0~1之间这种类似的功能
"
HanLP,hankcs,149491827,发现新词汇,open,提问,https://github.com/hankcs/HanLP/issues/191,"如题，给一定的微博语料，能否提取出哪些词汇是新生词汇，如发现新的网络用语。这个功能能否实现。
"
HanLP,hankcs,149061685,Python调用hanlp如何将HanLP.properties放图classpath中。,open,提问,https://github.com/hankcs/HanLP/issues/189,"这是项目文档说明：最后将HanLP.properties放入classpath即可，对于任何项目，都可以放到src或resources目录下，编译时IDE会自动将其复制到classpath中。

我的IDE是eclipse。请问python项目，应该将HanLP.properties放在哪里，怎么放入classpath中~
"
HanLP,hankcs,148563089,如何取自定义词表中最近似的单词？,open,提问,https://github.com/hankcs/HanLP/issues/188,"我现在自定义的字典中定义了“大王叫我来巡山”， 我输入了的字符为“大王派我来巡山”， 怎么样能让系统给出这个最相似的单词？ 能否指点一下多谢。
"
HanLP,hankcs,147646693,初始化能否支持从Classpath读取data下的自定义字典元数据？,open,提问,https://github.com/hankcs/HanLP/issues/185,"如题，目前需要在hanlp.properties设置root的值为绝对路径，能否设置为classpath下的相对路径？
"
HanLP,hankcs,147596667,直接调用句法分析器，显示源码错误,open,无效,https://github.com/hankcs/HanLP/issues/184,"直接调用CRF句法分析
`System.out.println(CRFDependencyParser.compute(""把市场经济奉行的等价交换原则引入党的生活和国家机关政务活动中""));`
出现问题：

```
Exception in thread ""main"" java.lang.NullPointerException
    at com.hankcs.hanlp.dependency.CRFDependencyParser.parse(CRFDependencyParser.java:123)
    at com.hankcs.hanlp.dependency.AbstractDependencyParser.parse(AbstractDependencyParser.java:46)
    at com.hankcs.hanlp.dependency.CRFDependencyParser.compute(CRFDependencyParser.java:78)
    at bistu.idcc.features.ParseDependency.main(ParseDependency.java:19)

```

请教一下，应该如何解决
"
HanLP,hankcs,147340752,HanLP对于用户自定义词典的官方推荐方法是怎样的?,open,提问,https://github.com/hankcs/HanLP/issues/182,"你好,我在测试的时候,发现通过`CustomDictionary.add()`方法添加的新词不会自动持久化,没有动态修改二进制缓存文件.
HanLP官方推荐的方法是记录添加成功的自定义词语,然后在下次项目启动之前追加到自定义词典文件吗?
"
HanLP,hankcs,147106806,如何增加歌曲类的分词？,open,提问,https://github.com/hankcs/HanLP/issues/181,"比如说我想把长江之歌， 或者爱的奉献作为整个一个词可以吗？
"
HanLP,hankcs,146523115,python开启JVM调用HanLP时报错Unable to start JVM at native\common\jp_env.cpp:60,open,提问,https://github.com/hankcs/HanLP/issues/179,"python开启JVM调用HanLP时报错Unable to start JVM at native\common\jp_env.cpp:60请问如何解决
"
HanLP,hankcs,145891232,关于HanLP计算文章相似度问题,open,提问,https://github.com/hankcs/HanLP/issues/178,"之前用Ansj，现刚发现HanLP，还不熟悉，但觉得HanLP非常不错。冒昧请教博主，我希望通过HanLP实现计算两个文章的相似度，盼大神提供使用HanLP实现文章相似度的大概思路或功能，感激不尽~我看https://github.com/ysc/word/tree/master/src/main/java/org/apdplat/word/analysis已有诸多算法实现，但我依然希望使用HanLP，可惜没有类似功能，着实遗憾，亟盼作者提供帮助。
"
HanLP,hankcs,145871750,在phrase识别中能否指定最大到二阶共现？,open,提问,https://github.com/hankcs/HanLP/issues/176,
HanLP,hankcs,145867293,如何获得分词词频？,open,提问,https://github.com/hankcs/HanLP/issues/175,"使用segment的时候，分词的词频如何获得呢？
"
HanLP,hankcs,144507704,三元组提取的问题,open,提问,https://github.com/hankcs/HanLP/issues/169," @hankcs 感谢你开发的HanLP工具，实在太棒了！

我刚学NLP，有个问题想向你请教。
我有这么个需求，输入是一个句子，输出是能表达句子语义的三元组（主谓宾）。例如：

原句：格言是简练而含义深刻并具有教育意义的警句。
结果：（格言，定义，简练而含义深刻并具有教育意义的警句）

原句：儿化具有区别词义、区分词性和表示感情色彩的作用。
结果：（儿化，作用，区别词义、区分词性和表示感情色彩）

原句：点号又分句末点号和句内点号。
结果：（点号，包含，句末点号和句内点号）

我看过你写的提取句子主谓宾的项目，所以我打算用类似的方法来做，但是自己刚入门NLP，不知道行不行。你有什么方法或者建议么？

谢谢！
"
HanLP,hankcs,143191582,维特比分隔识别人名的问题,open,提问,https://github.com/hankcs/HanLP/issues/164,"新华社专电（记者熊琳）《鬼吹灯》作者张牧野认为电影《九层妖塔》涉嫌侵犯著作权，将中国电影股份有限公司和导演陆川起诉至北京市西城区人民法院。记者20日从法院获悉，在受理该案后，应被告之一陆川申请，法院追加梦想者电影（北京）有限公司、乐视影业（北京）有限公司为本案共同被告。今年1月7日，《鬼吹灯》作者张牧野起诉称。电影《九层妖塔》系由《鬼吹灯之精绝古城》改编拍摄而成，但《九层妖塔》的故事情节、人物设置、故事背景均与原著相差甚远，超出了法律允许的必要的改动范围，构成对原著的歪曲和篡改，给原告造成了精神伤害，侵犯了原告的保护作品完整权。故将中国电影股份有限公司及陆川诉至法院，请求法院判令二被告立即停止侵权行为，向张牧野公开赔礼道歉、消除影响，并赔偿张牧野损失100万元人民币。本案立案后，被告之一陆川申请追加梦想者电影（北京）有限公司、乐视影业（北京）有限公司为共同被告。陆川认为梦想者电影（北京）有限公司与乐视影业（北京）有限公司为电影《九层妖塔》出品方即影片著作权人，应当作为共同被告参加本案诉讼。张牧野亦同意将梦想者电影公司和乐视影业追加为共同被告。西城法院在充分考虑原被告意见的基础上，为了便于查清案件事实，按照相关法律规定，已同意追加上述二公司为本案被告。

上面这段文本，在 PersonRecognition.Recognition.roleTag 后，熊琳 的角色标注为 [熊 B 885 D 16 C 4 E 4 K 1 ][琳 D 511 E 355 C 89 L 1 ]，这两个词是从词典里查出来的，但是在 PersonRecognition.Recognition.viterbiExCompute 后将 熊琳 的角色标注改为了 熊/D ,琳/L 。

这个是转移概率矩阵有问题吗？
"
HanLP,hankcs,142568686,python调用人名识别接口的问题,open,提问,https://github.com/hankcs/HanLP/issues/160,"我不太清楚是不是这样调用
Name_rec = JClass('com.hankcs.hanlp.seg.Dijkstra.DijkstraSegment')
Name_rec.seg(u'北川景子参演了林诣彬的速度与激情3')
这样会出错，不知道怎么用这个接口
"
HanLP,hankcs,141827242,请问是否支持url识别？,open,改进,https://github.com/hankcs/HanLP/issues/159,"RT
"
HanLP,hankcs,141612337,baidu 盘墙外超级慢能不能提供别的链接?,open,提问,https://github.com/hankcs/HanLP/issues/158,"baidu 盘墙外超级慢能不能提供别的下载方法?
"
HanLP,hankcs,140844642,最大熵依存句法分析器的实现,open,提问,https://github.com/hankcs/HanLP/issues/154,"hankcs,你说的字符串特征是什么？
"
HanLP,hankcs,140622754,viterbi所有概率取对数,open,提问,https://github.com/hankcs/HanLP/issues/153,"求解HMM模型，所有概率请提前取对数？什么原理
"
HanLP,hankcs,140559336,HanLP如何在Elasticsearch上使用？,open,提问,https://github.com/hankcs/HanLP/issues/152,"可否提供相应插件？
"
HanLP,hankcs,131628510,翻译人名、日本名识别为啥不采用HMM？,open,提问,https://github.com/hankcs/HanLP/issues/141,"中国人名识别采用HMM模型，而翻译人名、日本名识别采用规则来处理。
作者在翻译人名、日本名识别时有考虑采用HMM处理吗？是不是效果不好转而采用规则来识别呢？
"
HanLP,hankcs,127090986,词性标准和依存句法分析问题,open,提问,https://github.com/hankcs/HanLP/issues/126,"您的词性标准是自己定义的还是参考某标准？谢谢
"
HanLP,hankcs,126826494,运用简繁转换到安卓应用当中时txt字库的问题。,open,提问,https://github.com/hankcs/HanLP/issues/124,"我现在把整个工具的jar包导入到了安卓项目当中，不过txt等字库文件都还在电脑里，在java代码里调用简繁转换没有问题，但是请问怎么把txt放入安卓应用中？
因为jar包当中的路径是死的，我改不了，不然就可以把txt放入手机，路径改成相应的位置了。
"
HanLP,hankcs,126597090,关于添加词典的问题,open,提问,https://github.com/hankcs/HanLP/issues/122,"我新添加一个词典，出现这个错误
严重: 自定义词典D:/hanlp/data/dictionary/custom/word.txt读取错误！java.lang.IllegalArgumentException: No enum constant com.hankcs.hanlp.corpus.tag.Nature.INT'L，这是怎么回事呢
"
HanLP,hankcs,126521054,求问使用自定义辞典对初始分词结果修改的机制,open,提问,https://github.com/hankcs/HanLP/issues/121,"v1.2.8版本下的标准分词结果：
String text = ""攻城狮逆袭单身狗"";
System.out.println(HanLP.segment(text));
分词结果是：攻城/狮/逆袭/单身/狗

但若增加了自定义词，如下：
CustomDictionary.add(""城狮"",""nz 100000"");   //假设""城狮""是词
CustomDictionary.add(""单身狗"");
分词结果是：攻城/狮/逆袭/单身狗
单身狗的分词是对的，可是为什么没有把“城狮”分成一个词呢？
我知道现在的HanLP标准分词是先对句子粗分词，再利用自定义辞典进行修正。那么现在自定义辞典修正的基本思路是什么？(相关部分代码我没看明白)
自定义词的词频对修正有影响吗？(为什么我给""城狮""的词频设为100000还是没法分出来？)
"
HanLP,hankcs,126457697,维特比算法分词的平滑问题,open,提问,https://github.com/hankcs/HanLP/issues/120,"MathTools.java中有静态函数`calculateWeight(Vertex from, Vertex to)`计算两个结点的转移概率：
line 40: `double value = -Math.log(dSmoothingPara * frequency / (MAX_FREQUENCY) + (1 - dSmoothingPara) * ((1 - dTemp) * nTwoWordsFreq / frequency + dTemp));`
既然有了平滑参数dSmoothingPara，为何要用dTemp调整nTwoWordsFreq / frequency的结果呢？
"
HanLP,hankcs,126439218,CharTable转换有误,open,提问,https://github.com/hankcs/HanLP/issues/119,"下载的v1.2.8版本，CharTable.java中利用CONVERT对字符转换，偶然发现：
全角和半角的逗号、问号、感叹号都会转换成全角的句号。
其他的还没测试。
"
HanLP,hankcs,126343803,词性错误问题,open,提问,https://github.com/hankcs/HanLP/issues/118,"比如“苏宁电器集团增持苏宁云商0.4%股份”得到的“增持”是nz，应该是动词，我想修改这个，应该怎么做
"
HanLP,hankcs,126128770,词性nx与nz的区别是什么？,open,提问,https://github.com/hankcs/HanLP/issues/117,"在Nature.java中说nz是其他专名，nx是字母专名，这是什么意思？另外，在包com.hankcs.hanlp.utility的Predifine.java预定义了
`/**
             * 专有名词 nx
             */
            TAG_PROPER = ""未##专"";
`
而没有预定义nz。
我在人民日报语料库中发现“人民网”的词性是nz，不是nt。
那么在程序中，nt、nz、nx到底是如何区别的？
"
HanLP,hankcs,125868453,关于基于神经网络的高性能依存句法分析器的问题,open,提问,https://github.com/hankcs/HanLP/issues/114,"您好，请问NNParserModel.txt.bin是如何训练出来的，训练部分的代码具体在哪
"
HanLP,hankcs,123447295,python 调用时候出现的乱码问题,open,提问,https://github.com/hankcs/HanLP/issues/101,"本人环境是win7 64bit，python2.7.*版本，按照http://www.hankcs.com/nlp/python-calls-hanlp.html用的时候，在输出print(HanLP.segment('你好，欢迎在Python中调用HanLP的API'))时出现乱码，问下怎么可以解决，现在就是卡在<class 'jpype._jclass.java.util.ArrayList'>（(HanLP.segment('你好，欢迎在Python中调用HanLP的API'))的数据类型是<class 'jpype._jclass.java.util.ArrayList'>）数据类型往python转化时不知道怎么做，对java一点都不了解。
"
HanLP,hankcs,110940289,DAT内存分配算法,open,提问,https://github.com/hankcs/HanLP/issues/70,"src/main/java/com/hankcs/hanlp/collection/trie/DoubleArrayTrie.java

```
resize(65536 * 32); // 32个双字节
```

```
            if (allocSize <= (begin + siblings.get(siblings.size() - 1).code))
            {
                // progress can be zero // 防止progress产生除零错误
                double l = (1.05 > 1.0 * keySize / (progress + 1)) ? 1.05 : 1.0
                        * keySize / (progress + 1);
                resize((int) (allocSize * l));
            }
```

此段代码好像小规模字典不会运行到，例如在人名识别时，21个字典项的小字典，也需要开个65535内存。
"
HanLP,hankcs,107829910,关于版权问题,open,提问,https://github.com/hankcs/HanLP/issues/62,"hankcs，你好。
之前我一直使用python+jpype的方式，使用你的HanLP。我的工作中所有的代码全部是基于python开发，并且考虑以后要在hadoop平台上使用。所以我用python实现了一个HanLP的子集。等成熟后也希望放到github上。python版本直接使用了data目录下的数据文件，希望可以得到你的授权。根据你的要求，我会在项目首页注明字典数据的来源，以及python版本与HanLP的关系。
"
HanLP,hankcs,101331085,有冲动想改成C#,open,,https://github.com/hankcs/HanLP/issues/46,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm-scalable,myudelson,381935370,"Wrong number of columns in line 1. Expected 75, found 4",open,,https://github.com/myudelson/hmm-scalable/issues/1,"When I was trying to run the example:
""./trainhmm -s 1.1 -m 1 -p 1 toy_data.txt model.txt predict.txt""
I got the following error:
""trainhmm starting...
Wrong number of columns in line 1. Expected 75, found 4""

Any idea why this happened?"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Algorithm-Viterbi,arnsholt,347924736,Un bitrot,open,,https://github.com/arnsholt/Algorithm-Viterbi/pull/8,"Hi,
this rolls up all the existing PRs and fixes one outstanding issue to make this installable.

As per

https://github.com/perl6/ecosystem-unbitrot/issues/305"
Algorithm-Viterbi,arnsholt,224127541,Use modern META filename,open,,https://github.com/arnsholt/Algorithm-Viterbi/pull/7,"The `META.info` is a legacy, pre-Christmas name. While it's still currently supported,
`META6.json` is the new name. And since a lot of people simply copy some module's 
structure, the old name still proliferates, so we're trying to get rid of it for good 
by sending PRs to any modules that use the old name, to switch to the modern name."
Algorithm-Viterbi,arnsholt,185495916,"Add missing ""provides"" in META",open,,https://github.com/arnsholt/Algorithm-Viterbi/pull/6,"We were just reviewing the log of the modules list builder.
"
Algorithm-Viterbi,arnsholt,185394117,"Add mandatory ""perl"" META field",open,,https://github.com/arnsholt/Algorithm-Viterbi/pull/5,"The `perl` field specifies the minimal perl version for which this distribution can be installed and is a mandatory field. The value of `""6.*""` indicates any version suffices.

It is recommended to use [Test::META](https://modules.perl6.org/repo/Test::META) module as an author test, to catch any issues with the META file.
"
Algorithm-Viterbi,arnsholt,121568259,Fix tests,open,,https://github.com/arnsholt/Algorithm-Viterbi/pull/4,
Algorithm-Viterbi,arnsholt,78481248,Add unit declarator to class declarations,open,,https://github.com/arnsholt/Algorithm-Viterbi/pull/3,"As of Rakudo 2015.05, the `unit` declarator is required before using
`module`, `class` or `grammar` declarations (unless it uses a block).  Code
still using the old blockless semicolon form will throw a warning. This
commit stops the warning from appearing in the new Rakudo.
"
Algorithm-Viterbi,arnsholt,65055608,add provides section for latest panda (S11 support),open,,https://github.com/arnsholt/Algorithm-Viterbi/pull/2,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
np-hmm,jamesmcinerney,94561903,how to install?,open,,https://github.com/jamesmcinerney/np-hmm/issues/1,"ubgpu@ubgpu:~/github/np-hmm$ python testing/example.py
Traceback (most recent call last):
  File ""testing/example.py"", line 2, in <module>
    from model import sensors, general_inf
ImportError: No module named model
ubgpu@ubgpu:~/github/np-hmm$ 
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,andywhite37,203693789,Empty project folder in .haxelib leads to failures,open,,https://github.com/andywhite37/hmm/issues/12,"```
$ ls .haxelib/pal
# nothing in here, man
```

```
Warning: command ""haxelib path pal"" failed with status: 1 in dir: /Users/michaelmartin/Projects/cmsapi/
Execution error: failed to extract expected ""haxelib path pal"" information (path, name, and version) for library: pal
```"
hmm,andywhite37,130819439,update git ref,open,,https://github.com/andywhite37/hmm/issues/7,"It would be nice to have a command `hmm upgrade ?libname` that checks if `libname` has been updated and it changes the version (or git ref) accordingly; it also performs the update for the lib. If `libname` is omitted all the libraries in the project will be upgraded that way.
"
hmm,andywhite37,113077226,More robust command line argument processing,open,,https://github.com/andywhite37/hmm/issues/6,"Right now, it's a fairly fragile DIY solution, which will not scale well, and is not very robust.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
EDHMM,mikedewar,171692061,experiments all fail due to sample_mean_prec() mean calculation,open,,https://github.com/mikedewar/EDHMM/issues/1,"Hi. I read the paper, I wanted to try the code!

I'm using python 2.7. If I run any of the experiments (i.e. `cd experiments; python experiment_1.py`) I get something like this:

<pre>
../transition.py:25: RuntimeWarning: divide by zero encountered in log
  return np.log(self.A[i,j])
../log_space.py:7: RuntimeWarning: underflow encountered in exp
  return lny + log(1 + exp(lnx - lny))
Traceback (most recent call last):
  File ""experiment_1.py"", line 70, in <module>
    online=True, sample_U = True
  File ""../edhmm.py"", line 537, in beam
    self.O.update(Z_samples, Y)
  File ""../emission.py"", line 140, in update
    mu, tau = self.sample_mean_prec(Z, Y)
  File ""../emission.py"", line 59, in sample_mean_prec
    ybar = np.mean(n[i],1)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py"", line 2885, in mean
    out=out, keepdims=keepdims)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/_methods.py"", line 56, in _mean
    rcount = _count_reduce_items(arr, axis)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/_methods.py"", line 50, in _count_reduce_items
    items *= arr.shape[ax]
IndexError: tuple index out of range
zsh: exit 1     python experiment_1.py
</pre>


I notice that the line that chokes is [protected by an `except ValueError`](https://github.com/mikedewar/EDHMM/blob/sampler/emission.py#L60). Maybe this should, or should also, guard against IndexError, but I don't know the code so I don't know the intention.

(In case it matters, on my system I also had to [mildly rearrange the order of imports](https://github.com/danstowell/EDHMM/commit/8502ca1802b6fd36a00616ebc5d8566eeddb5993))
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
gse,go-ego,368737137,Add HMM and CRF support,open,,https://github.com/go-ego/gse/issues/8,"- Gse version (or commit ref): last

Add HMM and CRF support."
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HmmUFOtu,Grice-Lab,361856470,new user problem,open,,https://github.com/Grice-Lab/HmmUFOtu/issues/1,"Hi, i am analyzing a MinION sequencer dataset that contains 16S gene sequences (1500 pb each) but i am having the next issue with the hmmufotu command : 

MSA loaded
HMM profile read
CSFM-index loaded
Phylogenetic tree loaded
Determining read strand by alignment cost ...
terminate called after throwing an instance of 'std::invalid_argument'
  what():  Your sequence contains invalid alphabet charactersCAGACGACTGCAAACGGAATCGGAGTCACGCACC
Aborted (core dumped)

i have tried with another 16S small dataset from MinION and it worked well....

can someone help me please?

Thanks! 
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
AndroidMaryTTS,AndroidMaryTTS,326459767, Error reading from input stream,open,,https://github.com/AndroidMaryTTS/AndroidMaryTTS/issues/10,"Dear Team, 
          Getting below error . Kindly help
com.google.android.apps.gsa.shared.speech.b.g: Error reading from input stream
        at com.google.android.apps.gsa.staticplugins.microdetection.d.k.a(SourceFile:91)
        at com.google.android.apps.gsa.staticplugins.microdetection.d.l.run(Unknown Source:14)
        at com.google.android.libraries.gsa.runner.a.a.b(SourceFile:32)
        at com.google.android.libraries.gsa.runner.a.c.call(Unknown Source:4)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:458)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at com.google.android.apps.gsa.shared.util.concurrent.b.g.run(Unknown Source:4)
        at com.google.android.apps.gsa.shared.util.concurrent.b.au.run(SourceFile:4)
        at com.google.android.apps.gsa.shared.util.concurrent.b.au.run(SourceFile:4)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
        at java.lang.Thread.run(Thread.java:764)
        at com.google.android.apps.gsa.shared.util.concurrent.b.i.run(SourceFile:6)"
AndroidMaryTTS,AndroidMaryTTS,301720278,Trying to Add New Language Support,open,,https://github.com/AndroidMaryTTS/AndroidMaryTTS/issues/9,"Hi, I have build Hmm Voice model for a language which is working fine with desktop MaryTTS. I am trying to use it in Android MaryTTS. So I need your help. :)  @farizaghayev @amirashad "
AndroidMaryTTS,AndroidMaryTTS,265878710,Missing gradle depenencies,open,,https://github.com/AndroidMaryTTS/AndroidMaryTTS/issues/8,"Adding AndroidMaryTTS to build.gradle(app) causes gradle sync to break  : 

`implementation 'com.marytts.android:marylib:1.0.1'`

causes : 

`Error:Failed to resolve: commons-logging:commons-logging:1.1.1`"
AndroidMaryTTS,AndroidMaryTTS,242599370, Cannot start MARY server,open,,https://github.com/AndroidMaryTTS/AndroidMaryTTS/issues/7,"i got this error 

i use android studio 

07-13 09:26:11.504 14153-14233/marytts.android D/OpenGLRenderer: Use EGL_SWAP_BEHAVIOR_PRESERVED: false
07-13 09:26:45.937 14153-14232/marytts.android E/art:     at marytts.server.Mary.startModules(Mary.java:162)
07-13 09:26:45.937 14153-14232/marytts.android E/art:     at marytts.server.Mary.startup(Mary.java:300)
07-13 09:26:45.937 14153-14232/marytts.android E/art:     at marytts.server.Mary.startup(Mary.java:220)
07-13 09:26:46.188 14153-14232/marytts.android D/test: marytts.exceptions.MaryConfigurationException: Cannot start MARY server
07-13 09:26:46.208 14153-14163/marytts.android W/System.err:     at org.apache.harmony.dalvik.ddmc.DdmServer.dispatch(DdmServer.java:171)
"
AndroidMaryTTS,AndroidMaryTTS,213977425,How to add more supported languages?,open,,https://github.com/AndroidMaryTTS/AndroidMaryTTS/issues/6,How to add more supported languages? for example russian.
AndroidMaryTTS,AndroidMaryTTS,169199151,Android marshmallow,open,help wanted,https://github.com/AndroidMaryTTS/AndroidMaryTTS/issues/1," Android MaryTTS generated audio is lagging with the version marshmallow.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Sinsy-Remix,hyperzlib,296132904,Can we add English support to sinsy?,open,,https://github.com/hyperzlib/Sinsy-Remix/issues/2,
Sinsy-Remix,hyperzlib,295560420,这个咋用啊,open,,https://github.com/hyperzlib/Sinsy-Remix/issues/1,在Debian8上编译了，htsvoice从 `https://sourceforge.net/projects/hts-engine` 下载，出来的音频文件只有杂音 ：（
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
BayesHMM,luisdamiano,377014884,Error Trying some examples.,open,,https://github.com/luisdamiano/BayesHMM/issues/1,"Hi!
Really love the pacakge, looks awesome!
Im on an arch linux machine.

im getting an error -

![screenshot_2018-11-03_13-24-25](https://user-images.githubusercontent.com/2296335/47947174-9f2fba00-df6b-11e8-9ed7-c6dbcafe88be.png)

Thank you  @luisdamiano 

Best,
Andrew"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Digital-Speech-Recognizer,RockmanZheng,341335162,模块导入问题,open,,https://github.com/RockmanZheng/Digital-Speech-Recognizer/issues/2,"file: Recognizer.py

line 7:  from x64.Release.TrainCore import VDecode,BWDecode

can't import 'x64'

楼主这是c/c++文件么，没有找到该模块，该如何导入"
Digital-Speech-Recognizer,RockmanZheng,273422276,代码运行问题，请回复。,open,,https://github.com/RockmanZheng/Digital-Speech-Recognizer/issues/1,在微信公众号中山大学数学学院运行您发的一个语音识别小试牛刀的案例。也下载了您的代码。但是跟着您的步骤发现有些运行不出来，想联系您本人。关于这一类的代码实例实在太少。我找了很久。希望您能回复我，可以有偿。其中有些想咨询您。我qq 787630497 电话15198951346.这个对我来说很重要。请求你联系我。
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm-lda,katsuya94,262240500,"Redundant looping in calculating ""num_same_words.."" counts",open,,https://github.com/katsuya94/hmm-lda/issues/1,"Hi,
I was trying to re-implement your python code to c++ so that it can handle larger data. While doing so I realized that the function `run_counts()` is the main performance bottleneck. Specifically the loops where you compute 
`self.num_same_words_assigned_to_topic = [self.count_num_same_words_assigned_to_topic(word) for word in xrange(self.vocab_size)]` 
`count_num_same_words_assigned_to_topic(word)` is a 2D loop of `documents * topics` and it is called `vocab_size` times (one time for each word in the vocab. Calling this function for each word is redundant because it can be done in `documents * topics` iterations like so
```
for(int i = 0; i < n_documents; i++) {
	doc_size = documents[i].size();
	for(int j = 0; j < doc_size; j++) {
		word = documents[i][j];
		if(class_assignments[i][j] == 0)
			num_same_words_assigned_to_topic[word][topic_assignments[i][j]] += 1;
	}
}
```
Kindly note that this is a c++ code snippet but this can be very easily ported to python. This should give a good speed improvement in the overall performance. Correct me if I'm doing something wrong."
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Poccala,Byshx,369766976,你好，我想问下你用的viterbi解码方法是token_pass还是类似level-building？,open,,https://github.com/Byshx/Poccala/issues/4,你好，我想问下你用的viterbi解码方法是token_pass还是类似level-building？
Poccala,Byshx,306163878,请问如何运行训练和运行解码?,open,,https://github.com/Byshx/Poccala/issues/3,请问如何运行训练和运行解码?
Poccala,Byshx,260936206,语料数据,open,,https://github.com/Byshx/Poccala/issues/1,你好，能分享下模型用到的一些语料数据吗？
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
bhmm,bhmm,174045091,AttributeError: 'HMM' object has no attribute 'Pi',open,,https://github.com/bhmm/bhmm/issues/44,"This error is raised when I try to call `estimator.stationary_probability`. 
Here's the traceback:

```
/Users/sternc1/anaconda/lib/python2.7/site-packages/bhmm-0.6.1+22.gcc92643-py2.7-macosx-10.6-x86_64.egg/bhmm/estimators/maximum_likelihood.pyc in stationary_probability(self)
    217         r"""""" Stationary probability, if the model is stationary """"""
    218         assert self._stationary, 'Estimator is not stationary'
--> 219         return self._hmm.Pi
    220 
    221     def _forward_backward(self, itraj):

AttributeError: 'HMM' object has no attribute 'Pi' 
```

`hmm.Pi` doesn't exist but `hmm._Pi` does. 

I can open a PR to fix this by changing `return self._hmm.Pi` to `return self._hmm._Pi`
"
bhmm,bhmm,147237321,Fix stochastic test failures,open,,https://github.com/bhmm/bhmm/issues/43,"```
======================================================================
FAIL: test_discrete_4_2 (bhmm.tests.test_init_discrete.TestHMM)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/travis/miniconda/envs/_test/lib/python3.5/site-packages/bhmm-0.6.1+22.g6f0de1e-py3.5-linux-x86_64.egg/bhmm/tests/test_init_discrete.py"", line 85, in test_discrete_4_2
    assert(np.max(B-Bref) < 0.05 or np.max(B[[1, 0]]-Bref) < 0.05)
AssertionError
----------------------------------------------------------------------
```

Either set the random number seed to make this deterministic or use the statistical error and a 6-sigma tolerance in order to make these tests fail only when things are broken.
"
bhmm,bhmm,146327388,Real value is not within confidence interval for synthetic three stat model,open,,https://github.com/bhmm/bhmm/issues/42,"I ran the synthetic model example in the [singlemoleculetools](https://github.com/bhmm/singlemoleculetools) repository using the [generate-figure.py](https://github.com/bhmm/singlemoleculetools/blob/master/examples/synthetic-three-state-model/generate-figure.py.) script. Some of real values are not within the confidence interval. Here's the table:

https://github.com/ChayaSt/singlemoleculetools/blob/master/examples/synthetic-three-state-model/synthetic-three-state-model-bhmm-statistics.tex

![screen shot 2016-04-06 at 10 12 59 am](https://cloud.githubusercontent.com/assets/6598229/14319762/763ac66a-fbe0-11e5-8f19-432276d5cb91.png)
"
bhmm,bhmm,130690609,error initializing HMM ,open,,https://github.com/bhmm/bhmm/issues/31,"Hi,

I ran into an error when initializing HMMs with `init_hmm`:

```
import bhmm
observations = [np.array([1, 1])]
bhmm.init_hmm(observations, 2)
```

it gives the error:

```
[...]
  File ""/Users/yarden/anaconda/lib/python2.7/site-packages/bhmm/init/discrete.py"", line 195, in estimate_initial_hmm
    + str(active_nonseparate.size) + '-state MSM.')
NotImplementedError: Trying to initialize 2-state HMM from smaller 1-state MSM.
```

Although only one output state is observed in data, it's technically possible to (over)fit a 2-state HMM to the data, rather than force a 1-state HMM. (For example, fit a two-state HMM with a sparse prior on transition matrix, which would behave similarly to 1-state HMM.) Is there a way around this?

Thanks very much, Yarden
"
bhmm,bhmm,100122359,error in chi_square call,open,,https://github.com/bhmm/bhmm/issues/20,"```
File ""C:\projects\bhmm\bhmm\estimators\bayesian_sampling.py"", line 161, in bhmm.estimators.bayesian_sampling.BayesianHMMSampler.sample
Failed example:
    samples = sampled_model.sample(nsamples, nburn=nburn, nthin=nthin)
Exception raised:
    Traceback (most recent call last):
      File ""C:\Python27_32\lib\doctest.py"", line 1315, in __run
        compileflags, 1) in test.globs
      File ""<doctest bhmm.estimators.bayesian_sampling.BayesianHMMSampler.sample[5]>"", line 1, in <module>
        samples = sampled_model.sample(nsamples, nburn=nburn, nthin=nthin)
      File ""C:\projects\bhmm\bhmm\estimators\bayesian_sampling.py"", line 168, in sample
        self._update()
      File ""C:\projects\bhmm\bhmm\estimators\bayesian_sampling.py"", line 196, in _update
        self._updateEmissionProbabilities()
      File ""C:\projects\bhmm\bhmm\estimators\bayesian_sampling.py"", line 258, in _updateEmissionProbabilities
        self.model.output_model._sample_output_model(observations_by_state)
      File ""C:\projects\bhmm\bhmm\output_models\gaussian.py"", line 417, in _sample_output_model
        chisquared = np.random.chisquare(nsamples_in_state-1)
      File ""mtrand.pyx"", line 2167, in mtrand.RandomState.chisquare (numpy\random\mtrand\mtrand.c:16888)
    ValueError: df <= 0
```
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HoneyBADGER,JEFworks,380035505,"error about ""getCellAlleleCount"" command line",open,,https://github.com/JEFworks/HoneyBADGER/issues/17,"Hi,
   I am trying to use HonryBADGER to call CNV from 10X Genomics RNAseq data. Initially, I used the snps matrix from the HoneyBadger package and loaded my own bam, bai and barcode file to run
>results <- getCellAlleleCount(snps, bamFiles, indexFiles, cellBarcodes)
However, I got error like this: 
Error in .io_bam(.c_Pileup, file, bamReverseComplement(scanBamParam),  : 
seqlevels(param) not in BAM header:
seqlevels: ‘NC_007605’.
I used the Rsamtools to check the header of BAM file and there was no ""NC_007605"" in the header. I have googled about NC_007605. It means ""Human gammaherpesvirus 4, complete genome"".  So what does it work for here. How can I figure this out. Really appreciated if anyone can help. Thanks.
"
HoneyBADGER,JEFworks,375170172,Docs fixes,open,,https://github.com/JEFworks/HoneyBADGER/pull/16,
HoneyBADGER,JEFworks,370936559,Usage of HoneyBADGER on 10x data?,open,,https://github.com/JEFworks/HoneyBADGER/issues/14,"Hi! I installed the HoneyBADGER on MacOS with R 3.5.0. Afterwards went though tutorial and everything worked fine. My main question: is the package suitable for 3' focused 10X data? Are there some settings adjustment for such data analysis?  
I tried to apply the tool usage on 10X sample with gene counts CPM normalized and log2 adjusted (~2500 cells). However, several issues were introduced. The visualisation function hb$plotGexpProfile() got stuck and did not work in RStudio. Then I tried to make a PDF and this was OK, however did not work for PNG or SVG generation. Further the analysis got stuck on function  hb$calcGexpCnvBoundaries(init=TRUE, verbose=FALSE), it took more than 3 hours  and R session crashed. What could’ve been the problem? RAM limits? I am going to try rerunning it on Linux cluster as well, but thought maybe this could be improved from settings control. 
 "
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
linear_chain_crf,heshenghuan,312911182,预测时候加偏移量的问题,open,,https://github.com/heshenghuan/linear_chain_crf/issues/9," def inference(self, X, X_len, reuse=None):
        with tf.name_scope('score'):
            # The weight matrix is treated as an embedding matrix
            # Using lookup & reduce_sum to complete calculation of unary score
            features = tf.nn.embedding_lookup(self.W, X)
            feat_vec = tf.reduce_sum(features, axis=2)
            feat_vec = tf.reshape(feat_vec, [-1, self.nb_classes])
            scores = feat_vec + self.b
            # scores = tf.nn.softmax(scores)
            scores = tf.reshape(scores, [-1, self.time_steps, self.nb_classes])
        return scores

请问上面的scores = feat_vec + self.b 这句，为什么要加self.b这个偏移量呢，crf的公式里面也没有加偏移量啊。多谢。
"
linear_chain_crf,heshenghuan,310664651,加入embedding特征后性能反而下降？,open,,https://github.com/heshenghuan/linear_chain_crf/issues/8,实验发现其它条件都完全相同的情况下，加入embedding特征后模型性能反而比不加embedding要差，您看这是为什么啊？
linear_chain_crf,heshenghuan,306732410,存在大量对于flags的未定义赋值,open,,https://github.com/heshenghuan/linear_chain_crf/issues/6,"例如：

https://github.com/heshenghuan/linear_chain_crf/blob/5c47fb531d558d6b740d8409b9ee2bc797fc43ff/emb_crf_tagger.py#L282

由于`label2idx`没有预先使用`tf.app.flags.DEFINE_*`进行定义，运行到此处时会报错，类似的错误还有很多"
linear_chain_crf,heshenghuan,294361294,发射概率矩阵的行数为什么是self.feat_size + 1？,open,,https://github.com/heshenghuan/linear_chain_crf/issues/4,"请问
with tf.name_scope('weights'):
self.W = tf.get_variable(
shape=[self.feat_size + 1, self.nb_classes],
initializer=tf.truncated_normal_initializer(stddev=0.01),
name='weights'
#regularizer=tf.contrib.layers.l2_regularizer(0.001)
)
这里为什么是self.feat_size + 1呢？不应该就是self.feat_size吗？非常感谢。"
linear_chain_crf,heshenghuan,258475357,Comparison to CRFsuite,open,question,https://github.com/heshenghuan/linear_chain_crf/issues/3,"Hello,

Thank you for providing this awesome tool. 

Do you have any idea how your tools compares to the performance of tools like CRF++ or CRFSuite ?

What is the main use case for linear_chain_crf ??

Thanks
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
SimpleHMMER,minillinim,156827147,Didn't parse HMMER output file.,open,,https://github.com/minillinim/SimpleHMMER/issues/6,"I just tried SimpleHMMER put it didn't want to parse my file. Here is what I did:

```
from simplehmmer.simplehmmer import HMMERParser
handle = open('test.hmmout')
h = HMMERParser(handle)
h.next()
```

The error:

```
/home/lucas/.local/lib/python2.7/site-packages/simplehmmer/simplehmmer.pyc in next(self)
    195         while 1:
    196             if self.mode == 'domtblout':
--> 197                 hit = self.readHitsDOM()
    198             elif self.mode == 'tblout':
    199                 self.hit = self.readHitsTBL()

/home/lucas/.local/lib/python2.7/site-packages/simplehmmer/simplehmmer.pyc in readHitsDOM(sel
f)
    242                         raise FormatError( ""Something is wrong with this line:\n%s"" %
 (line) )
    243                     refined_match = dMatch[0:22] + ["" "".join([str(i) for i in dMatch[
22:]])]
--> 244                     return HmmerHitDOM(refined_match)
    245             except IndexError:
    246                 return {}

/home/lucas/.local/lib/python2.7/site-packages/simplehmmer/simplehmmer.pyc in __init__(self,
values)
    312             self.target_name = values[0]
    313             self.target_accession = values[1]
--> 314             self.target_length = int(values[2])
    315             self.query_name = values[3]
    316             self.query_accession = values[4]

ValueError: invalid literal for int() with base 10: 'Collagen'
```

The file:

```
$ head test.hmmout
#                                                               --- full sequence ---- --- best 1 domain ---- --- domain number estimation ----
# target name        accession  query name           accession    E-value  score  bias   E-value  score  bias   exp reg clu  ov env dom rep inc description of target
#------------------- ---------- -------------------- ---------- --------- ------ ----- --------- ------ -----   --- --- --- --- --- --- --- --- ---------------------
contig40885_1        -          Collagen             PF01391.15   1.6e-81  254.3 292.4     4e-13   35.3  24.9  10.3   2   2   6   8   8   8   7 # 1 # 2004 # 1 # ID=1_1;partial=11;start_type=Edge;rbs_motif=None;rbs_spacer=None;gc_cont=0.526
contig40885_1        -          DUF1180              PF06679.9          1  -55.5  74.9         1   -9.7  11.4   6.1   2   2   0   2   2   2   0 # 1 # 2004 # 1 # ID=1_1;partial=11;start_type=Edge;rbs_motif=None;rbs_spacer=None;gc_cont=0.526
contig40885_1        -          Glutenin_hmw         PF03157.10         1  -21.4 235.2   0.00023    5.7  41.1   6.5   2   2   3   5   5   5   0 # 1 # 2004 # 1 # ID=1_1;partial=11;start_type=Edge;rbs_motif=None;rbs_spacer=None;gc_cont=0.526
contig40885_1        -          Herpes_capsid        PF06112.8          1 -108.9 183.7         1   -5.9  12.6   8.6   2   2   4   6   6   6   0 # 1 # 2004 # 1 # ID=1_1;partial=11;start_type=Edge;rbs_motif=None;rbs_spacer=None;gc_cont=0.526
contig40885_1        -          Med15                PF09606.7          1  -78.9 142.1         1  -12.0  26.3   2.9   2   1   1   3   3   3   0 # 1 # 2004 # 1 # ID=1_1;partial=11;start_type=Edge;rbs_motif=None;rbs_spacer=None;gc_cont=0.526
```
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
mage,numediart,179617862,Combined pitch scaling and shifting,open,,https://github.com/numediart/mage/issues/7,"Is there some way to compose both scaling and shifting of the pitch? That is, I would like to be able to pipe the pitch generated by the model through this formula:

```
pitch = modelPitch * scale + shift
```
"
mage,numediart,19532367,"Max/Pd externals, example texts",open,,https://github.com/numediart/mage/pull/4,"-modified mage.cpp/h and vocoder.cpp/h to add vibrato function used in Max and Pd externals. The modificatio in fft-sptk.h is just a blank line, sorry for that.
-reorganized codes in mage~.cpp for Pd and Max external so that they are more easy to compare
-added functions to those : labelnextvowel, labelfilluntilend, vibrato
-added tutorial and helpfile for Pd and Max
-example texts in zip archive : example_processed_texts.zip
-compiled Pd and Max externals which should work on OSX 10.6.8
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMMM.js,shickey,150181955,Load file doesn't work in Chrome,open,,https://github.com/shickey/HMMM.js/issues/3,"Chrome Version 49.0.2623.112 (64-bit) on Mac OS X 10.10.5. You can click the Load button and select a .hmmm file to load, but code doesn't appear in the code editor. The spinning ball also appears several times before it's possible to select the file, but this also happens when loading a .hmmm from Safari.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
fast,ml-smores,165479529,Problematic feature,open,,https://github.com/ml-smores/fast/issues/24,"The problem came from one feature (number of turns) that was not binary but is increasing over multiple turns (1, 2, 3 etc). I haven't figured out why yet but when i turned this into a binary feature (new turn = 0 or re-try =1), FAST worked. I think it has something to do with the ordering of the data but i am still working on it. 
"
fast,ml-smores,164611856,set parameters fixed; initialize feature parameters,open,,https://github.com/ml-smores/fast/issues/23,
fast,ml-smores,164411438,prepare PFA feature example,open,,https://github.com/ml-smores/fast/issues/22,
fast,ml-smores,164411306,trace subskills' knowledge levels directly,open,,https://github.com/ml-smores/fast/issues/21,
fast,ml-smores,163407012,Firecracker needs (from Elliott),open,,https://github.com/ml-smores/fast/issues/20,"@e-bartsch
1. how to make sure knowledge inference doesn't decrease with correct response (output guess+slip<1 for all records)
2. the update is insensitive
3. initializing by reasonable bounds (now random)
4. constrain the search
5. what's the impact of features for updating (e.g., item)?
"
fast,ml-smores,80738139,Some conveniences for users,open,,https://github.com/ml-smores/fast/issues/19,"-- remove ""++"" for specifying "".conf"" files
-- explain running multiple files better 
-- explain the naming conventions better
-- give specific error types for users to debug the input files.
   -- outcome should only be ""correct|incorrect""
   -- input header one column shouldn't have space between words.
"
fast,ml-smores,67639519,Speed up / debugging when running a large number of subskills,open,,https://github.com/ml-smores/fast/issues/18,
fast,ml-smores,65332135,change to allow multiple hidden/observed states,open,,https://github.com/ml-smores/fast/issues/16,
fast,ml-smores,56927145,curvature or line searchstep size underflow,open,,https://github.com/ml-smores/fast/issues/14,"It seems when we have features with collinearity issues, LBFGS will output such warnings. 

For example: 
-- subskill practice features
-- item practice features (when ranging up to 15)
-- past N action features in a data where students have almost the same activity order.

It seems that seeting initialWeightsBounds = 0.1 can practically avoid such problems. Yet this setting will limit the initial value range. So you should be careful if you want to get truly randomized initial values.
"
fast,ml-smores,56043021,feature selection in FAST? (From Rohit),open,,https://github.com/ml-smores/fast/issues/13,
fast,ml-smores,52607643,"change to throw new RuntimeException(""..."");",open,,https://github.com/ml-smores/fast/issues/12,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm_ner_organization,lipengfei-558,345670274,generate_datas.py中有一个错误,open,,https://github.com/lipengfei-558/hmm_ner_organization/issues/2,"python generate_datas.py
Traceback (most recent call last):
  File ""generate_datas.py"", line 91, in <module>
    genertate_initial_vector(hidden_states)
  File ""generate_datas.py"", line 20, in genertate_initial_vector
    the_hidden_states[tmp_list[0]] += eval(tmp_list[1])
KeyError: '2'"
hmm_ner_organization,lipengfei-558,323557196,generate_datas.py有一处错误？,open,,https://github.com/lipengfei-558/hmm_ner_organization/issues/1,"def generate_emit_probability(initial_freq)函数中，
result.append([tmp_list[0],observed_state,float(tags_and_freq[1])/initial_freq[tmp_list[0]]])  应该改为
result.append([tmp_list[0],observed_state,float(tmp_list[1])/initial_freq[tmp_list[0]]])  吧？
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Single-Word-Speech-Recognition-using-GMM-HMM-,jayaram1125,269282509,ValueError in accumulate_sufficient_statistics(),open,,https://github.com/jayaram1125/Single-Word-Speech-Recognition-using-GMM-HMM-/issues/1,"Hi,
           Thanks for sharing the code and the explanation in the document. I try to run your code and I got the following error in speechmodel.model.fit(speechmodel.traindata):

File ""/usr/local/lib/python2.7/dist-packages/hmmlearn/hmm.py"", line 607, in _accumulate_sufficient_statistics
    raise ValueError

Do you know how to solve this problem?

--Ben

"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
autohmm,mackelab,292713378,indexing error,open,,https://github.com/mackelab/autohmm/issues/30,"Hi,
You have a filetype-bug in the current version of the sample function that causes trouble in python 2.7.
`[ar.py:565] states = np.zeros(n_samples)`
should be changed to
`[ar.py:565] states = np.zeros(n_samples, dtype=int)`
otherwise 
`[ar.py:595] newstate = (transmat_cdf[states[idx-1]] > nrand[idx-1]).argmax()`
wont work.
Cheers
Pavol"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
BP-AR-HMM,mathDR,161444294,Is it possible to add inputs to this model?,open,,https://github.com/mathDR/BP-AR-HMM/issues/1,"Dear Dan,

   I am very excited to have found this Python port of Emily's matlab code.  I was wondering though, is it possible to extend the model to accommodate multiple time series with covariates?  In other words, is there a way to explicitly introduce some external factors that are likely to drive the time series?

  Thanks!
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
SALB,m-toman,216399570,syntax error,open,,https://github.com/m-toman/SALB/issues/3,"Hi,
Thanks for your response. We are building this project using Visual studio 12.0 with Windows SDK 8.0 at Windows 8 machine. During the building of project sapiPS we getting below syntax error:
1>------ Build started: Project: sapiPS, Configuration: Release x64 ------
1>  dlldata.c
1>  htssapi_i.c
1>  htssapi_p.c
1>C:\Program Files (x86)\Windows Kits\8.0\Include\um\sapiddk.h(2309): error C2059: syntax error : ':'
1>C:\Program Files (x86)\Windows Kits\8.0\Include\um\sapiddk.h(2790): error C2143: syntax error : missing ')' before '*'
1>C:\Program Files (x86)\Windows Kits\8.0\Include\um\sapiddk.h(2790): error C2081: 'SPRECORESULTINFOEX' : name in formal parameter list illegal
1>C:\Program Files (x86)\Windows Kits\8.0\Include\um\sapiddk.h(2790): error C2143: syntax error : missing ';' before '*'
1>C:\Program Files (x86)\Windows Kits\8.0\Include\um\sapiddk.h(2790): error C2059: syntax error : '*'
1>C:\Program Files (x86)\Windows Kits\8.0\Include\um\sapiddk.h(2790): error C2059: syntax error : ')'
1>C:\Program Files (x86)\Windows Kits\8.0\Include\um\sapiddk.h(2793): error C2059: syntax error : '}'
1>  Generating Code...
========== Build: 0 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========

It seems, we are using wrong version of windows SDK. Kindly let us know what are the versions of Visual Studio and Windows SDK you used for building this project.
"
SALB,m-toman,80433817,Can not receive all SAPI events,open,help wanted,https://github.com/m-toman/SALB/issues/1,"I have compiled the entire solution in VS 2013. 
After registering SLT voice (using SALBROOT\sapi\bin\register-slt.bat), when i use SAPI5 TTSAPP to test this voice and select all events to be shown, it only receive StartStream, EndStream and VoiceChanged events.

Is it possible to receive all SAPI events?  
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HTS-demo_CMU-ARCTIC-SLT-STRAIGHT-AR-decision-tree,MattShannon,58078663,CalcClusterDistribution,open,,https://github.com/MattShannon/HTS-demo_CMU-ARCTIC-SLT-STRAIGHT-AR-decision-tree/issues/3,"Hello! I'm trying to build regression-class trees with the AR-HMM demo, but I'm getting the error:

ERROR [+9999]  CalcClusterDistribution: not available for autoregressive HMM

Looking into the code of the HTK patch, I see the lines where this statement came out:

@@ -7272,6 +7601,8 @@ void CalcClusterDistribution (RNode _n)
       if (acc != NULL) {
          /_ check pure stream and vecSize */
          if (n->pureVecSize && n->pureStream) {      
-            if (acc->sqrAr != NULL)
-               HError(9999, ""CalcClusterDistribution: not available for autoregressive HMM"");
           for (k=1; k<=n->vSize; k++) {
           sum[k] += acc->sum[k];
           sqr[k] += acc->sqr[k];

Would you mind to explain what the comparison ""if (acc->sqrAr != NULL)"" is doing and why this error happens?
Thanks, and best regards,
Marvin C.
"
HTS-demo_CMU-ARCTIC-SLT-STRAIGHT-AR-decision-tree,MattShannon,53308686,configure file missing,open,,https://github.com/MattShannon/HTS-demo_CMU-ARCTIC-SLT-STRAIGHT-AR-decision-tree/issues/1,"I can see the configure.ac file on the project, but I think it also needs the configure file to run the demo. 
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm-scala,education-service,245269858,use model,open,,https://github.com/education-service/hmm-scala/issues/1,"I want to know how to use your method, the example you has given don't give a method to use the model. Thanks"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
mahjong,yingrui,231924153,Solve Performance Issue of HmmNameFilter,open,,https://github.com/yingrui/mahjong/issues/7,"HmmNameFilter use HMM model for Chinese Name Recognition, the performance is bad due to it try  to search word index every time."
mahjong,yingrui,215259060,Add Elastic Search 5.x Analyzer Plugin,open,,https://github.com/yingrui/mahjong/issues/4,"Support Lucene 4.x, 5.x, 6.x. and Support Elastic Search"
mahjong,yingrui,212963116,Convert date time to ISO date format,open,,https://github.com/yingrui/mahjong/issues/3,"In many cases, the NLP task need to extract exact date time from sentences. Meanwhile, the natural language date time need to convert to machine readable date time format!!!"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
DNN-HMM,wenjiegroup,360994840,Replication domain identification using two fractions data,open,,https://github.com/wenjiegroup/DNN-HMM/issues/1,"Hi there,

It's a very nice paper and analysis.

I am wondering is it possible to use this tool to identify replication domains for two fractions data(G1 and S)?

Thanks,

Weiyan"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,Sumolari,311845946,error during instllation ,open,,https://github.com/Sumolari/hmm/issues/3,".\node_modules/coffee-script/bin/coffee -c -o lib src
it give error and unable to install"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
PyEMMA,markovmodel,383637671,[coordinates/source] do not probe h5 file for mdtraj.HDF5TrajectoryFile,open,,https://github.com/markovmodel/PyEMMA/pull/1378,"The exception triggering and catching was too broad/unspecific to be
triggered in certain cases.

Now we just check, if the user wants to read MD data by checking if
either a topology or a featurizer has been given."
PyEMMA,markovmodel,377982518,Dtraj stats allow parallel no pg,open,,https://github.com/markovmodel/PyEMMA/pull/1375,"* fix ci
* allow bmsm to set njobs for effective cmatrix estimation.
* added null_context (backport python 3.7)"
PyEMMA,markovmodel,364603048,relative entropy between two MSM matrices,open,feature-request,https://github.com/markovmodel/PyEMMA/issues/1367,"feature request:
calculation of the relative entropy between two MSM matrices of the same protein, like in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3637129/

"
PyEMMA,markovmodel,363828676,Tica model analog to vamp model,open,,https://github.com/markovmodel/PyEMMA/pull/1366,"In this PR we moved the handling the dimension and diagonalization from TICA to its model. This turned out to be good solution for the new VAMP estimator/model pair and therefore we should use it from now on for TICA and its children.

In VAMP the dim parameter can either take a fixed dimension (integer), a float for variance cutoff, or None to preserve all available ranks.
To handle optional scaling of the singular/eigenvectors it has a 'scaling' parameter taking strings like 'kinetic_map' etc.

This approach should fix #1075, since it reduces the amount of input parameters and invalid combinations.

The default behavior is not changed, it still uses kinetic map scaling and preservation of 95% of kinetic variance.

The changes have also been applied for NystroemTICA. This avoids hard to maintain copy paste code, as this implementation copied over almost everything from TICA except for the low-rank, column selection section.

Already serialized TICA objects are future proof as well, as these are transformed during loading to fit into the new class layout."
PyEMMA,markovmodel,362335198,Koopman ignores dim setting,open,,https://github.com/markovmodel/PyEMMA/issues/1365,"koopman ignores the number of dimensions requested, and later gives fatal error.
Here koopman method of tica generates 3 dimensions, but I have set dim=5 for the tica. tica_obj calculation works, but get_output fails due to dimension mismatch.

```
Traceback (most recent call last):
  File ""run-tica-msm3.py"", line 712, in <module>
    Runticamsm().run()
  File ""run-tica-msm3.py"", line 154, in run
    y = tica_obj.get_output(stride=tica_stride)
  File ""/mnt/b/projects/sciteam/bamm/hruska/vpy8/lib/python3.5/site-packages/pyemma/coordinates/data/_base/transformer.py"", line 227, in get_output
    return super(StreamingTransformer, self).get_output(dimensions, stride, skip, chunk)
  File ""/mnt/b/projects/sciteam/bamm/hruska/vpy8/lib/python3.5/site-packages/pyemma/coordinates/data/_base/datasource.py"", line 407, in get_output
    trajs[itraj][i, :] = chunk[:, dimensions]
ValueError: could not broadcast input array from shape (100,3) into shape (100,5)
```
This issue is a copy of https://github.com/radical-collaboration/extasy-grlsd/issues/84 
@marscher 

The comand to get tica_obj:
```tica_obj = pyemma.coordinates.tica(get_out_arr, lag=25, dim=5, kinetic_map=True, stride=1, weights='koopman')```
python 3.5.5, pyemma 2.5.4
"
PyEMMA,markovmodel,355925868,Refactoring pyemma.plots.markovtests.py,open,plots,https://github.com/markovmodel/PyEMMA/issues/1353,"The `plot_cktest()` function in the current (pyemma-2.5.4) release is not flexible enough as we have seen in markovmodel/pyemma_tutorials#142.

For instance, I would like to have an auxiliary function for individual panels, like `_add_ck_subplot()` but with better controls (individual line/marker styles, transition labels, ...), exposed in the `pyemma.plots` namespace and `plot_cktest()` itself could also do with better controls.

Further, the entire module uses matplotlib techniques which are deprecated for scripting usage.

Issues #1229 and #1342 could be relevant here."
PyEMMA,markovmodel,353967876,covariance matrices as input for tica,open,feature-request,https://github.com/markovmodel/PyEMMA/issues/1351,"I would like to run the function pyemma.coordinates.covariance_lagged for different trajectories on separate nodes, save the covariance matrices and in the next step combine them to calculate tica. 
Is there a way to give these covariance matrices as input for tica? And then map all input data? 
For many trajectories, I assume calculating the covariances takes some time which can be easily split across different nodes.
Would weights='koopman' make this more difficult?"
PyEMMA,markovmodel,353931186,fix koopman bug,open,,https://github.com/markovmodel/PyEMMA/pull/1350,described in https://github.com/markovmodel/PyEMMA/issues/1349
PyEMMA,markovmodel,353930062,koopman bug and fix,open,,https://github.com/markovmodel/PyEMMA/issues/1349,"Found a bug in the koopman implementation by analysing https://github.com/radical-collaboration/extasy-grlsd/issues/77 together with @fnueske and a suggestion how to solve it.
Take line
https://github.com/markovmodel/PyEMMA/blob/1a280481ed6cbbfc3fbceaacfa22136e26983678/pyemma/_ext/variational/solvers/direct.py#L82
 with ```s = _np.array([ 1.90963508, 1.72242421, 1.18440072, -1.16845181])```
leads to an m=4 in line https://github.com/markovmodel/PyEMMA/blob/1a280481ed6cbbfc3fbceaacfa22136e26983678/pyemma/_ext/variational/solvers/direct.py#L89
, while m=3 is expected (as for 3 positive eigenvalues, removing later the 4th)

Suggestion: increase the value 1e-16 to 1e-14 in line
https://github.com/markovmodel/PyEMMA/blob/1a280481ed6cbbfc3fbceaacfa22136e26983678/pyemma/_ext/variational/solvers/direct.py#L84
This will lead to m=3, but not sure if that's the best fix as this might depend on the individual machine precision of the user

System: pyemma 2.5.4, python 2.7.14, numpy"
PyEMMA,markovmodel,346835170,thermo support for metadynamics,open,,https://github.com/markovmodel/PyEMMA/issues/1345,"Originally, transition-based reweighting analysis method (tram) has been developed to deal with enhanced sampling, such as umbrella sampling or replica exchange. However, metadynamics has not been well supported with tram in [documents](http://www.emma-project.org/latest/api/generated/thermo-api/pyemma.thermo.tram.html#pyemma.thermo.tram). Can anyone provide detailed examples?

For the sake of simplicity, we have only one trajectory generated from metadynamics. In this trajectory, each snapshot corresponds a Hamiltonian due to adding bias more frequently than step of snapshot output. So we can generate ttrajs with linear index of length of trajectory and get dtrajs by clustering on collective variables of metadynamics.

In [documents](http://www.emma-project.org/latest/api/generated/thermo-api/pyemma.thermo.tram.html#pyemma.thermo.tram) of pyemma, the btrajs[i][t, k] is the reduced bias energy in trajectory i and time step t for the k’th thermostate. Here, we have only one trajectory, so i is 0.

How to understand the bias at step t, or snapshot t, for the thermostate of k?
For example, What is the difference between bias for btrajs[0, 0] (step 0 and thermostate 0) and btrajs[0, 1] (step 0 and thermostate 1).

Waiting for your help.

Thanks."
PyEMMA,markovmodel,343564295,Confidence interval for constant data warning,open,msm,https://github.com/markovmodel/PyEMMA/issues/1342,"When doing a CK-test, the following error can be raised (https://github.com/markovmodel/pyemma_tutorials/issues/115):
```
python3.6/site-packages/pyemma/util/statistics.py:60: UserWarning: confidence interval for constant data is not meaningful
  warnings.warn('confidence interval for constant data is not meaningful')
```

The problem is apparently that the sampled transition matrix at 2 x lag time has elements in the order of  `1e-221` that do not change during the Bayesian sampling process. 

A test case is pyemma_tutorials notebook 07 hidden Markov models, 
```python
hmm_6 = pyemma.msm.bayesian_hidden_markov_model(cluster.dtrajs, 6, lag=1, dt_traj='1 ps', nsamples=50)
pyemma.plots.plot_cktest(hmm_6.cktest(mlags=2), units='ps');
```
Pyemma version 2.5.4"
PyEMMA,markovmodel,340806634,Wanted: show_progress parameter in coordinates and thermo api functions,open,thermo,https://github.com/markovmodel/PyEMMA/issues/1341,"Following the example of `pyemma.msm.api`, it would be nice to have a `show_progress` parameter in the API functions of

- [ ] `pyemma.coordinates`
- [ ] `pyemma.thermo`"
PyEMMA,markovmodel,338279572,[plots] accept coordinates DataSource as input,open,plots,https://github.com/markovmodel/PyEMMA/issues/1333,This allows to directly pass readers and avoid to concatenate the datasets prior plotting.
PyEMMA,markovmodel,337594421,computation of chunksize ignores memory heavy computations in featurizer,open,design-discussion,https://github.com/markovmodel/PyEMMA/issues/1332,"Since we use of 256mb for a memory chunk, people are reporting exceeding main memory during computation of pairwise distances. The amount of pairs in these cases were massive (eg. hundreds of thousands). We should incorporate the memory requirements for feature computation, when setting the chunksize automatically. At the moment only the dimension of the output is used.

Either we solve this by setting a smaller chunk (eg. 8 MB, which is still sufficient for NumPys OpenMP parallelization) or think about the memory requirement stuff per frame."
PyEMMA,markovmodel,336200696,HMM submodels,open,design-discussion,https://github.com/markovmodel/PyEMMA/issues/1324,"HMMs have a `submodel` method that should provide the user with a model on a subset of hidden states. As it is implemented now, a full estimator instance will be returned (making it essentially a sub-estimator). This sub-estimator contains a weird mixture of the original estimation parameters and the ones from the sub-model. This potentially causes problems e.g. with CK-tests. 

Let me give an example with a sub-model of the largest strongly connected set (of hidden states). As it is implemented now, an estimator (not a model) will be returned which incorporates the original set of discrete trajectories (i.e. same observable set) and the sub-modelled model (i.e. different number of hidden states `n_states`). 

Having only the parameters of the sub-model, the CK-test will do the estimates at multiples of the lagtime with the decreased number of hidden states. This in general does not yield the same results as with the original number of hidden states and later sub-modelling. It's also a bit inconsistent with the idea of the CK-test to test the prediction of a model against the estimation of a model with the same parameters at higher lag times.

My ideas so far:
a) simplest/preliminary: raise an error instead of the CK-test if n_states decreased after estimation
b) moderate: implement CK-test in a consistent way, i.e. let estimates at multiples of the lagtime be done exactly the same way as the original estimate was.
c) time-consuming: introduce more general consistency check for CK-tests that account for hidden state probabilities (or PCCA memberships) potentially being different at each lag time. 

To b): I personally think that it would make most sense to stick to the more static estimator that does not allow for decreasing `n_states`. The sub-model should return an actual `Model` and not overwrite `self` in any case. This would still allow for derivation of transition matrix related properties, but prevent e.g. CK-tests on reduced sets of hidden states. CK-tests can be done by running the estimator again with the properties, such as `mincount_connectivity`, adjusted to the target sub-model (like the usual workflow with MSMs). 

Any thoughts? "
PyEMMA,markovmodel,330572103,Using the dihedral formed by the COM of groups of atoms as a feature,open,,https://github.com/markovmodel/PyEMMA/issues/1313,"Hello,

For the feature selection step, I would like to use the dihedrals (as well as angles and distances) formed by the centre of mass of groups of atoms. Is there a way to do that?

For example, lets say I would like to use the dihedral formed by the [COM of atoms 0, 1, 2, 3] , [atom 8], [atom 12] and the [COM of atom 16,17,18,19,20]

Currently, in `pyemma.coordinates.featurizer`, the method `add_dihedral` only accepts atom indices. Is there a way to replace indices with group of atoms?

For example, using the example above, doing something like `feature.add_dihedrals([[[0, 1, 2, 3],8,12,[6,17,18,19,20]]])` (although this currently returns an error).

Many thanks in advance for your help,

Eric

*I am using PyEMMA 2.5.2 installed via anaconda (python 3) on a Ubuntu 16.04.4 LTS computer*"
PyEMMA,markovmodel,328758025,"sample_mean, sample_conf and sample_std with relaxation and correlation methods",open,,https://github.com/markovmodel/PyEMMA/issues/1311,"PyEMMA 2.5.2.

Methods which compute summary statistics across samples fail for  `relaxation` and `correlation`. 

Example:
```
eq_time_bayes, eq_acf_bayes = bayesian_msm.sample_mean('correlation', 
                                                       np.array(Markov_average_trp_sasa))
```
fails when computing the average along the samples:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-51-f8d32f7c047c> in <module>()
      1 eq_time_bayes, eq_acf_bayes = bayesian_msm.sample_mean('correlation', 
----> 2                                                        np.array(Markov_average_trp_sasa)), 
      3                                                       # maxtime = 350) # we specify maxtime to avo

~/miniconda3/lib/python3.6/site-packages/pyemma/_base/model.py in sample_mean(self, f, *args, **kwargs)
    200         """"""
    201         vals = self.sample_f(f, *args, **kwargs)
--> 202         return _np.mean(vals, axis=0)
    203 
    204     def sample_std(self, f, *args, **kwargs):

~/miniconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py in mean(a, axis, dtype, out, keepdims)
   2907 
   2908     return _methods._mean(a, axis=axis, dtype=dtype,
-> 2909                           out=out, **kwargs)
   2910 
   2911 

~/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py in _mean(a, axis, dtype, out, keepdims)
     68             is_float16_result = True
     69 
---> 70     ret = umr_sum(arr, axis, dtype, out, keepdims)
     71     if isinstance(ret, mu.ndarray):
     72         ret = um.true_divide(

ValueError: operands could not be broadcast together with shapes (37,) (40,) 
```
Since the output of `correlation` (and `relaxation`) is not uniform, but is computed automatically from the implied time-scales, which may change dramatically from sample to sample.

A current work-around to this is fixing the kwarg `maxtime`
```
eq_time_bayes, eq_acf_bayes = bayesian_msm.sample_mean('correlation', 
                                                       np.array(Markov_average_trp_sasa),
                                                       maxtime = 350) # we specify maxtime to enable computation of summary statistics
```

This may not technically be a bug, but it is definitely unexpected behavior.
"
PyEMMA,markovmodel,325863105,MemoryError when computing TICA with high stride arguments,open,design-discussion,https://github.com/markovmodel/PyEMMA/issues/1305,"I get a memory error, with featurizer.dimension() 3003, I'm not sure why this happens. It happens quite fast, after about 10s. 
```
tica_obj = coor.tica(inp, lag=lag, commute_map=True, kinetic_map=False, var_cutoff=0.8, stride=stride_tica)>>>
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/api.py"", line 1261, in tica
    res.estimate(data, chunksize=cs)
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/transform/tica.py"", line 159, in estimate
    return super(TICA, self).estimate(X, **kwargs)
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/data/_base/transformer.py"", line 212, in estimate
    super(StreamingEstimationTransformer, self).estimate(X, **kwargs)
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/data/_base/streaming_estimator.py"", line 45, in estimate
    super(StreamingEstimator, self).estimate(X, **kwargs)
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/_base/estimator.py"", line 412, in estimate
    self._model = self._estimate(X)
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/transform/tica.py"", line 206, in _estimate
    covar.estimate(iterable, chunksize=self.chunksize, **kw)
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/data/_base/streaming_estimator.py"", line 45, in estimate
    super(StreamingEstimator, self).estimate(X, **kwargs)
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/_base/estimator.py"", line 412, in estimate
    self._model = self._estimate(X)
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/estimation/covariance.py"", line 214, in _estimate
    for data, weight in zip(it, it_weights):
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/data/_base/iterable.py"", line 226, in __next__
    return self.next()
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/data/_base/iterable.py"", line 246, in next
    itraj, data_lagged = self._it.next()
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/data/_base/datasource.py"", line 1005, in next
    X = self._it_next()
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/data/_base/datasource.py"", line 985, in _it_next
    X = self._use_cols(self._next_chunk())
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/data/feature_reader.py"", line 396, in _next_chunk
    res = self._data_source.featurizer.transform(chunk)
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/data/featurization/featurizer.py"", line 915, in transform
    vec = f.transform(traj).astype(np.float32)
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/pyemma/coordinates/data/featurization/distances.py"", line 107, in transform
    D = mdtraj.compute_contacts(traj, contacts=self.contacts, scheme=self.scheme, periodic=self.periodic)[0]
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/mdtraj/geometry/contact.py"", line 208, in compute_contacts
    atom_distances = md.compute_distances(traj, atom_pairs, periodic=periodic)
  File ""/scratch1/eh22/conda/envs/py3/lib/python3.6/site-packages/mdtraj/geometry/distance.py"", line 81, in compute_distances
    out = np.empty((xyz.shape[0], pairs.shape[0]), dtype=np.float32)
MemoryError
```
python 3.6.3, pyemma 2.5.2
[version.txt](https://github.com/markovmodel/PyEMMA/files/2032763/version.txt)

"
PyEMMA,markovmodel,313733158,BHMM CK-test,open,msm,https://github.com/markovmodel/PyEMMA/issues/1289,"The CK-test for BayesianHMMs gives me a cryptic index error in the case that the original estimate is done on an active set smaller than the full set of states. I was finally able to compile a minimal example that contains a disconnected state in the observable space. It only fails with BHMMs (`err_est` kwarg does not matter). 

```python
import pyemma
artificial_poor_dtraj = [[0, 0, 0, 0, 0, 0], 
   [1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2]]
BMSM = pyemma.msm.bayesian_hidden_markov_model(artificial_poor_dtraj, 2, 6, reversible=True)
ck = BMSM.cktest(mlags=3, err_est=False)
```
I tested this on the devel branch where it prints 
`pyemma.msm.estimators.bayesian_hmsm.BayesianHMSM[2] WARNING  Ignored error during estimation: index 2 is out of bounds for axis 1 with size 2`. 

Compared to version 2.4, there seem to be two differences:
a) In 2.4, the above example yields a more useful warning message `Given active set has empty states`. Maybe we should restore this error message? Otherwise, users just get an identity matrix as the new estimate.
b) With my own dataset, I don't get a warning at all in 2.4 and everything works fine (not an identity matrix). Maybe the minimal example is another bug? 

Any ideas? "
PyEMMA,markovmodel,312204538,dtype handling,open,design-discussion,https://github.com/markovmodel/PyEMMA/issues/1286,"I think there is some inconsistency about how to handle data types of sources. 

It seems that data sources are intended to operate on float32, however the yielded data type when iterating is not always float32 (when e.g. passing float64 arrays into a `DataInMemory`). Also quite a few tests assume double precision and fail when single precision is enforced (this can be see when introducing

```python
array = array.astype(np.float32)
```

into `DataInMemory::_add_array_to_storage`).

What do you think is the right way to deal with this? Enforce a float dtype or not? And if not, how should we deal with data sources that yield different dtypes for different trajs? In that case I think we would need an `output_type` per traj and cannot treat it as global property of the data source anymore."
PyEMMA,markovmodel,299773480,split large modules like maximum_likelihood_msm,open,design-discussion,https://github.com/markovmodel/PyEMMA/issues/1260,"These modules are becoming really unhandy (eg. 2000 lines for mlmsm) in terms of developing and maintaining. So I suggest to split them to smaller units, containing only one estimator at a time. People using tools from the 80s (emacs, vi) will hate this change, because they will have to deal with multiple files, but this change is convenient to keep things clean and reduce merge conflicts.
In Java there is also this one class per file rule, which makes a lot of sense if one thinks about it."
PyEMMA,markovmodel,299714566,serialization tests,open,design-discussion,https://github.com/markovmodel/PyEMMA/issues/1259,"We need to store saved models somewhere, and restore them with the current devel in order to ensure we did not break compatibility. We can collect all of the saved models in the unit tests, and save them in a branch for this version.
A separate test suite will then load all of these branches and compare the models. The big question is how to ensure all important attributes are in place and have meaningful values. We do not want to hard code this, as it would be hard to maintain (every change would have to make sure it also updates the tests, which is kind of impossible to do for inexperienced people).
The collection and storage can be somehow automatized via conftest.py (hooks into pytest). The latter is difficult to achieve and we need to think about this."
PyEMMA,markovmodel,298029976,"change patches, when new mdtraj gets released",open,upstream,https://github.com/markovmodel/PyEMMA/issues/1253,"We need to change the stride handling in patches, when  https://github.com/mdtraj/mdtraj/pull/1331 gets merged and released, because xtc format will then handle stride io efficient if offsets are provided (as they always are in pyemma).
It is kind of a follow up to #1252 "
PyEMMA,markovmodel,294498822,HMSMSampled submodel is broken (and untested),open,,https://github.com/markovmodel/PyEMMA/issues/1239,"This method is also never in use, because the model itself is most likely always used in conjunction with BayesianHMSM (which overrides submodel). We should either fix this method or delete it. 

It does not work, because it invokes a parent constructor with wrong arguments."
PyEMMA,markovmodel,291294674,plot_cktest set title hassle,open,plots,https://github.com/markovmodel/PyEMMA/issues/1229,"The legend is always placed on top and matplotlib places a figure suptitle such that it overdraws the legend. I came up with a very ugly workaround, which involves moving the bounding box of the legend after is has been drawn. 
![screenshot_20180124_182729](https://user-images.githubusercontent.com/170287/35347069-48a6f290-0134-11e8-946a-e0958ac3a592.png)

Probably the warning is significant here...
```
matplotlib/figure.py:1743: UserWarning: This figure includes Axes that are not compatible with tight_layout, so its results might be incorrect.
  warnings.warn(""This figure includes Axes that are not ""
```
"
PyEMMA,markovmodel,282235159,configure codecov to be nice,open,,https://github.com/markovmodel/PyEMMA/issues/1203,https://github.com/codecov/support/wiki/Codecov-Yaml
PyEMMA,markovmodel,256625460,ITS/MSM: allow to extend nsamples for error='bayes' a posteriori?  ,open,msm,https://github.com/markovmodel/PyEMMA/issues/1157,"It would be great to be able to keep adding samples to an object without having to re-compute from scratch. 

E.g., after having called msm.its with `nsamples=10`, if I want another 10 more samples, currently (I think) I have to restart the estimation (of the whole ITS or the a single MSM object) with `nsamples=20`, right? 

It's a bit in the spirit of https://github.com/markovmodel/PyEMMA/pull/1030"
PyEMMA,markovmodel,254031232,Invoking parent classes constructors inconsistency because of super() usage,open,design-discussion,https://github.com/markovmodel/PyEMMA/issues/1153,"If a class in a hierarchy misses a call to super, eg. the most basic interface does not call super on object (which is pretty common in our code base), it is not ensured that in multi-inheritance (we use that a lot too) all constructors are being called.
I'm currently not sure what the implications for PyEMMA are, but we should at least avoid this pattern completely or do it properly as may lead to uninitialized fields in the class tree, which can cause very subtle bugs.

Further reading: https://fuhm.net/super-harmful/

"
PyEMMA,markovmodel,240676992,MSM.vamp score documentation,open,documentation,https://github.com/markovmodel/PyEMMA/issues/1129,"The documentation for K in the vamp scores states, that it should have shape (n, k), where k is the rank deficiency. However the routines are failing for this input shape and in MSM.score a (n, n) matrix being passed to vamp_score. So I guess the documentation is just wrong?

@franknoe can you please comment on this?"
PyEMMA,markovmodel,235574742,[RunningMoments] zero weight leads to a ZeroDivisionError,open,low-prio,https://github.com/markovmodel/PyEMMA/issues/1117,"in Moments.combine there is a ZeroDivisionError: float division by zero error for very small weights (eg. 1E-99). Namely the division w = w2 / w1 fails for this case. In general we want to have these small weights in practice.
"
PyEMMA,markovmodel,230499161,provide convenience function to plot TICA.timescales (eg. like for ITS),open,feature-request,https://github.com/markovmodel/PyEMMA/issues/1099,maybe we can simply change the plot_timescales function to accept a list of TICA or a list of timescale arrays.
PyEMMA,markovmodel,225284199,Simplify TICA Parameters,open,coordinates,https://github.com/markovmodel/PyEMMA/issues/1075,"``tica``'s init method has four parameters that will determine the output dimension, and that have accumulated over time as different methods were added (especially kinetic maps and commute maps):
  * dim=-1
  * var_cutoff=0.95
  * kinetic_map=True
  * commute_map=False

Now that's a bit confusing, because these parameters are interdependent, and also the current default behavior is not the most sensible. By default, I will get a scaling by TICA eigenvalues (to a kinetic map at the selected lag time), and the output dimension will be selected such that the kinetic variance adds up to 95%. If I instead want to select a fixed output dimension, I would have, in principle to set two values:
`dim=10, var_cutoff=1.0`. To avoid that, we check if the var_cutoff was set away from the default, i.e. setting `dim=10, var_cutoff=0.95` will end up in 10 dimensions, even if the variance is 99%. That's confusing. Moreover, the current situation allows illegal or inconsistent settings such as `kinetic_map=True, commute_map=True`.

Looking at the sklearn impl of PCA, which has to address the same situation, here's a suggestion how to solve it. We only keep the following two parameters to control the output dimension and scaling:

```python
   """"""
        dim : float or int
            Number of dimensions to keep:
            * if dim is not set all available ranks are kept::
                n_components == min(n_samples, n_features)
            * if dim is an integer >= 1, this number specifies the number
              of dimensions to keep. By default this will use the kinetic
              variance unless scaling=`commute map` is selected.
            * if dim is a float with ``0 < dim < 1``, select the number
              of dimensions such that the amount of kinetic variance 
              that needs to be explained is greater than the percentage 
              specified by dim.
        scaling : None or string
            Scaling to be applied to the TICA modes upon transformation
            * None: no scaling will be applied, variance along the mode is 1
            * 'kinetic map' or 'km': modes are scaled by eigenvalue
            * 'commute map' or 'cm': modes are scaled by :math:`sqrt(t_i/2)`, 
              where :math:`t_i=-lag/|lambda_i|` is the relaxation time computed 
              from the eigenvalue :math:`lambda_i`.
   """"""
```

That way, `dim` is the only parameter that determines the output dimension, and it can be set in different ways to do that. `scaling` is the only parameter that determines the output of `transform(X)` or `get_output()`, it could get more complex (i.e. additional paramters in the string, or accepting a dict),
if we wanted to encode more, such as the function use to penalize small eigenvalues.

Of course we would need to introduce this behavior ""smoothly"", i.e. without killing the other parameters immediately, but we could enable the new parameter behavior first, deprecate the old parameters and remove them in a later version.

We would do the same changes in `pca`, although the situation there is a bit simpler.

What do you think? Is it worth making this change in PyEMMA 2.*, or does it add confusion?"
PyEMMA,markovmodel,219945053,New method: Landmark Kernel tICA,open,question,https://github.com/markovmodel/PyEMMA/issues/1069,"Hi guys,

Just saw this paper from Pande et al and I thought I would share it with you guys: http://biorxiv.org/content/early/2017/04/04/123752

Still haven't read it thoroughly, but it looks promising, specially the non-linearity aspect. They seem to improve on their previous paper: http://pubs.acs.org/doi/abs/10.1021/ct5007357

What do you think? Could this landmark kernel tICA be interesting for PyEMMA?

Thanks,
João
"
PyEMMA,markovmodel,193832821,Order of coarse grained states in coarse grained flux unclear,open,feature-request,https://github.com/markovmodel/PyEMMA/issues/1004,"The order (indices) of the coarse grained states obtained by 

fluxAB = msm.tpt(M, A, B)
fluxAB.coarse_grain(my_coarse_sets)

is unclear. A is always at index 0 and and B is always at index n, but the rest of the assignment is unclear. This is particularly confusing if A and B are two metastable sets obtained by PCCA and my_coarse_sets are all PCCA sets. In this case I get a coarse grained flux plot containing the same number of states as metastable sets provided, but with a different numbering. The assignment can be made, but requires matching of microstate indices, which is rather inconvenient.

It would be nice to have information on the change of indices here and a more convenient way to map the indices."
PyEMMA,markovmodel,193758561,Fast kmeans++,open,feature-request,https://github.com/markovmodel/PyEMMA/issues/1003,"This paper suggests a k-means seeding method that is much faster than kmeans++ and nearly as good. The algorithm constructs a Markov chain on the data points, and in practice k-1 chains of length 100 seem enough to get approximately equal quality as kmeans++. Importantly, only one full pass over the data is needed.

https://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means.pdf

A python implementation is also available and can be installed with
    pip install kmc2

I bet the code is available too, but I haven't searched for it yet.
"
PyEMMA,markovmodel,192624326,Added milestone counting support for maximum likelihood derived estimators,open,,https://github.com/markovmodel/PyEMMA/pull/1000,"* new msm API method ""estimate_core_markov_model"", dtrajs and core_set definition

TODO:
- [x] add tests
- [ ] add references and examples to API docstring"
PyEMMA,markovmodel,192611182,Implement Laio density based clustering algorithm,open,feature-request,https://github.com/markovmodel/PyEMMA/issues/998,"This would be nice to have for estimating core MSMs.
One can find an impl for R here: https://github.com/thomasp85/densityClust

@giopina Frank told me, that you are working on this or might be interested."
PyEMMA,markovmodel,187048295,[msmtools.flux.ReactiveFlux.coarse_grain] sorted by commitor? ,open,question,https://github.com/markovmodel/PyEMMA/issues/981,"AFAIK:
   * the order of the coarse sets does not reflect the order of the input and 
   * they be sorted by committor: 

(https://github.com/markovmodel/PyEMMA/issues/677#issuecomment-172962243 and https://github.com/markovmodel/PyEMMA/issues/677#issuecomment-172962562). 

However, I'm getting this (pyemma version 2.2.7+13.g05f5fc4)
```python
jMSM = pyemma.msm.markov_model(np.load('P.npy'))
set_A = np.load('A.npy')
set_B = np.load('B.npy')
jMSM.pcca(6)
jtpt = pyemma.msm.tpt(jMSM, set_A, set_B)
jtpt_coarse_sets, jtpt_coarse_reactive_flux = jtpt.coarse_grain(jMSM.metastable_sets)
jtpt_coarse_reactive_flux.committor
# array([ 0.        ,  0.64261339,  0.56212569,  0.79059808,  0.39101618,  1.        ])
```

I.e., clearly not sorted wrt. to q. 

I haven't taken a look at the code yet, but can someone confirm if we're only enforcing q[0]=0 and q[-1] = 1 and not touching the rest?

Here's a zipfile if you want to try the test
[issue.zip](https://github.com/markovmodel/PyEMMA/files/568918/issue.zip)
"
PyEMMA,markovmodel,181176209,"[ITS] do we need _estimators, _models by default?",open,low-prio,https://github.com/markovmodel/PyEMMA/issues/953,"These objects can be potentially very memory-consuming and after #952 is merged, we could switch to the evaluate ""timescales"" method. I talked to Nuria and Tim and they said, they consider the inspection of underlying estimators an ""expert"" option, so we should make it optional.
"
PyEMMA,markovmodel,180489970,Continuous time MSM,open,feature-request,https://github.com/markovmodel/PyEMMA/issues/951,"Hi,

Is it possible to generate continuous time MSM with pyEMMA.

Mamta
"
PyEMMA,markovmodel,180365784,Not possible to set a prior for the emission matrix,open,feature-request,https://github.com/markovmodel/PyEMMA/issues/950,"In pyemma/msm/estimators/bayesian_hmsm.py, the class BayesianHMS allows a prior to be input for the transmission matrix but not for the emission matrix. This limits the ways that other information can be brought in to inform the estimation. Although this could, apparently, be carried out using lower-level routines, it takes a rather high level of familiarity with the package to figure this out. So for the convenience of end-users, it would be great if this prior could also be included in the constructor for BayesianHMS.

Alternatively (or additionally?), all the priors could be included as input parameters for the _estimate method. This would allow one BayesianHMS to be used on multiple priors as well as multiple data sets. I'm not sure how much overhead is involved in creating BayesianHMS - if it is not much, then this is perhaps not worth the trouble.
"
PyEMMA,markovmodel,176855802,Implementing core-based (milestone) coarse-graining approaches,open,,https://github.com/markovmodel/PyEMMA/issues/930,"Eric Vanden-Eijnden recently [published a nice paper](http://arxiv.org/abs/1605.01150) on a new approach for finding core sets for coarse-graining. Has anyone started to implement this scheme into pyemma yet?

@maxentile in my group has been looking into implementing this, but I wanted to make sure we weren't duplicating effort.
"
PyEMMA,markovmodel,175965018,Cut out coordinates,open,,https://github.com/markovmodel/PyEMMA/issues/926,"In order to prepare for the transition of coordinates being cut out to a separate package, we need to ensure the following things:
1. [ ] provide an interface to the theobald rmsd library of mdtraj to avoid the dependency in the new pkg.
2. [ ] register the FeatureReader for tested formats in the new FileFormatRegistry.
3. [ ] tidy up variational, so we can use the running covar estimator in TICA.
4. [ ] pyemma.coordinates.data.md.tests leave only tests specific to MD data.
"
PyEMMA,markovmodel,164868050,Microstate label and connected set discussion,open,low-prio,https://github.com/markovmodel/PyEMMA/issues/866,"I've just received a comment from a PyEMMA user:

```
In this work, we assigned an additional label to each microstate in the unused digit position of the state labels, which is cumbersome and also confused functions like active_state_fraction. We think some internal mechanism to assign additional labels for the microstates could be helpful in some use cases, though it might not have general appeals.
```

I think we should reconsider the use of labels. We have discussed this recently in the context of connected sets. Can we have some discussion here how to sort this out / do you already have a prototype or draft how to handle connectivity?
"
PyEMMA,markovmodel,147427886,Kmeans unnecessary copies during clustering phase.,open,design-discussion,https://github.com/markovmodel/PyEMMA/issues/769,"Already discussed with @clonker so far:
ping @franknoe 
1. We can add the possibility to pass an Iterable to the C-extension, so it would not matter how large the data is. There would be no need for copies, but there exists some cases where it will be horrible inefficient (eg. costly features will be re-calculated for every iteration). For the inefficient cases we could show a warning, but it will work.
2. Restrict to DataInMemory as input for kmeans (mini-batch-kmeans is suited for the case of large data-sets). For this case DataInMemory needs to ensure that all arrays are c-contigous.
"
PyEMMA,markovmodel,146319529,"Striding, selecting dimensions or selecting states: Store metainformation in Estimators",open,,https://github.com/markovmodel/PyEMMA/issues/762,"Whenever making selections such as `stride`, `usedims` and also state selection in MSMs / HMMs, the user may get confused as how to map indexes to each other. We should thus think of a general way of recording which input features / dimensions / strides / states are used in the current Estimator, such that the user can trace that back, and we could also think of ways of automatically mapping indexes between different operations using such meta-information. I think the minimum would be to allow the estimator to print this information, e.g. by a `.describe()` function or by treating these information as Estimator parameters for which printing methods are already available.
"
PyEMMA,markovmodel,145386571,Make util.statistics.confidence_interval and util.statistics._confidence_interval_1d robust,open,,https://github.com/markovmodel/PyEMMA/issues/756,"The confidence interval functions crash in pathological cases (for some inf or nan inputs). This code should be tested and I suggest to make the confidence intervals behave similarly as the np.mean function. 

``` python
>>> import numpy as np
>>> np.mean([0.0, 1.0, np.inf])
inf
>>> np.mean([0.0, 1.0, np.nan])
nan
>>> np.mean([0.0, 1.0, np.inf, -np.inf])
nan
```

This will lead to infs and nans for confidence intervals on pathological data, but will not generate an exception. When plotting data this seems like the most desirable behavior, because if single datapoints have such problems, they will simply not be plotted, but the program doesn't raise an exception. We can log a warning if such a case occurs.
"
PyEMMA,markovmodel,130750379,consider using covartools with MPI on segmented trajectories,open,enhancement,https://github.com/markovmodel/PyEMMA/issues/681,"Since covartools supports merging covariance matrices calculated from segments, we could easily exploit this with splitting up the covariance estimation on multiple hosts for different segments.
The partial matrices will have to be communicated, but one can think of a tree pattern here to avoid too much overhead.

The host which will perform the diagonalisation, will have to collect the final matrix:
![unbenannt](https://cloud.githubusercontent.com/assets/170287/12757554/d2025d3a-c9d8-11e5-80f0-4d5421bece44.png)
"
PyEMMA,markovmodel,126903949,[coor_refactor] write performance tests and compare measures with last release ,open,,https://github.com/markovmodel/PyEMMA/issues/674,"We need to have tests for:
- io operations (FeatureReader, NumPyReader, ...)
- estimators (Clustering, TICA, PCA, ...)
- pipelining operations (chunked vs full-file/traj mode)

The data sets should lie on a local hard disk to avoid fluctuations due to network access. All tests should be long running and deterministic (eg. use the same random seed).

Which data sets should we use (eg. they have to fit on the hard drive of the testing system)?

We can think about automatizing this via our Jenkins service as soon we have defined these things. I'll suggest to store these tests in a separate repo to easily test against multiple versions of PyEMMA.
"
PyEMMA,markovmodel,114411536,Simple way of Wiki editing,open,,https://github.com/markovmodel/PyEMMA/issues/603,"It would be nice to have a more convenient way of writing documentation pages that include images and equations, especially for documents where we discuss design and develop issues. github issues are not the right tool for this I think, we'd need rather something like a Wiki. But the github wiki is horrible (embedding images only works through a clone-repository-add-image-commit-push cycle. Could it be less convenient?). sphinx-based doc pages can be used but are similarly inconvenient.

prose.io seems to be a more convenient editor for github md pages, in which we can do most things interactively and don't have to go through manual commits and pull/push cycles. prose.io cannot edit github Wiki pages right away, but there seems to be a way to get this working:

https://macropod.com/blog/building-a-better-github-wiki/

It also looks like there is a way to show latex equations on github pages:

http://christopherpoole.github.io/using-mathjax-on-github-pages/
"
PyEMMA,markovmodel,107940858,PCCA should warn or raise when more states are requested than matrix spectrum admits,open,enhancement,https://github.com/markovmodel/PyEMMA/issues/564,"I was trying to use the pyemma.msm.MaximumLikelihoodMSM.pcca method and I am getting weird results:

```
ipdb> M.pcca(2)
<msmtools.analysis.dense.pcca.PCCA object at 0x7f2a9b0ee3c8>
ipdb> set(M.metastable_assignments)
{0, 1}
ipdb> M.pcca(3)
<msmtools.analysis.dense.pcca.PCCA object at 0x7f2a9b0988d0>
ipdb> set(M.metastable_assignments)
{0, 1, 2}
ipdb> M.pcca(5)
<msmtools.analysis.dense.pcca.PCCA object at 0x7f2a9afe2710>
ipdb> set(M.metastable_assignments)
{0, 1, 2, 3, 4}
ipdb> M.pcca(6)
<msmtools.analysis.dense.pcca.PCCA object at 0x7f2a9b0988d0>
ipdb> set(M.metastable_assignments)
{0, 2, 3, 4, 5}
ipdb> M.pcca(7)
<msmtools.analysis.dense.pcca.PCCA object at 0x7f2a9b044240>
ipdb> set(M.metastable_assignments)
{0, 1, 2, 4, 5, 6}
ipdb> M.pcca(8)
<msmtools.analysis.dense.pcca.PCCA object at 0x7f2a9afe2710>
ipdb> set(M.metastable_assignments)
{0, 1, 5, 6}
```

Why am I getting only 4 macrostates when I am requesting 8?
"
PyEMMA,markovmodel,103937916,Make convergence information available in MaximumLikelihoodMSM,open,,https://github.com/markovmodel/PyEMMA/issues/527,
PyEMMA,markovmodel,99216477,Check if Estimator is estimated,open,enhancement,https://github.com/markovmodel/PyEMMA/issues/484,"... and raise informative error message when using results if not estimated.

Estimator objects such as ImpliedTimescales must be first parametrized with `estimate(X)`. If using a model property before one gets an error which is hard to interpret if the user is not familiar with the Estimator concept. We should check if estimated and if not while trying to call results raise an informative error message

Since that problem is the same for all estimators it would be great if we could come up with a general solution that doesn't require us to write extra code for every possible model property/method or estimator result.
"
PyEMMA,markovmodel,94086218,[msm.estimate_markov_model] parameter sparse=False gets overriden by method=auto of [msm/estimation/api/tmatrix],open,,https://github.com/markovmodel/PyEMMA/issues/404,"Somehow connected with #403
"
PyEMMA,markovmodel,90894943,Hidden path / Viterbi path in models,open,feature-request,https://github.com/markovmodel/PyEMMA/issues/378,"Models that can provide coarse-grained views of the kinetics (MSMs, HMMs) should provide a hidden or viterbi path. In an MSM, we can roughly do this:

``` python
    mem = MSM.metastable_memberships()  # gets membership values for each cluster
    mtraj = mem[dtraj]  # gets the trajectory of metastable state assignments
    plot(mtraj)  # plots it over time
```

In HMMs we have the Viterbi function. It may be even possible to apply the Viterbi algorithm to MSMs if we use Susanna+Marcus' PCCA-based coarse graining to define hidden transition matrix and output probabilities. But I'm not sure if we are guaranteed to get nonnegative numbers that way because their coarse-grained transition matrices can have negative elements.
"
PyEMMA,markovmodel,90894285,coor analysis function,open,,https://github.com/markovmodel/PyEMMA/issues/377,"We want a general way to analyze states, e.g. metastable states obtained from an MSM analysis. A typical analysis would be what is the mean of the input coordinates in each state. Roughly we can do it like this:

``` python
    Xmean = np.zeros((nX, nmeta))  # here nX is the number of dimensions, in your case number of angles *4, and nmeta is 4 for you
    count = np.zeros(nmeta)  # counts how many metastable states we had
    for t in range(len(dtraj)):
        x = X[t]  # your internal coordinate value at this point
        m = mem[dtraj[t]]  # your membership to metastable states at this point
        Xmean += np.outer(x, m)
        count += m
    Xmean /= count
    return Xmean
```

so in this example we need a way to compute metastable memberships (mem) and have discrete trajectories. I bet we can generalize this by just passing a model, but the model needs to have certain abilities, such as to assign macrostate memberships to every trajectory frame. So this affects model design too.
"
PyEMMA,markovmodel,79363432,Develop project concept,open,enhancement,https://github.com/markovmodel/PyEMMA/issues/322,"I suggest to build a prototype for a project object (or package?). The aim of the project package is to provide an easy-to-use environment in which MSM and other nontrivial data analyses could be constructed, conducted and saved. Below is a rough idea for a possible implementation strategy - but there may be other and better approaches.

Construction could be like:

``` python
trajfiles = ['1.xtc', '2.xtc']
topfiles = 'protein.pdb'
proj = pyemma.project(trajfiles, topfile)
```

This object then basicly displays the normal high-level API functions. For example I could do:

``` python
feat = proj.featurizer()
```

and then use and configure the feat object as usual. Note that I didn't have to pass any information to featurizer because the topology file is already known. Of course we don't want to rewrite the featurizer() function or api docs, because duplicate code is annoying and tends to get out of sync. A solution for that could be to make the featurizer() topology argument optional, i.e.:

``` python
def featurizer(top=None)
```

and then simply import coor.featurizer to the Project object.

Of course the project needs to remember the featurizer object, and vice versa the featurizer object needs to know which project it belongs to. So there must be something like `proj._add_featurizer(feat)`  and something like `feat._add_project(proj)` when calling the above function. Every object that can be a project-member would need such a function.

Similarly, you could go on:

``` python
feat = proj.featurizer()
feat.add_distances_ca()
proj.tica(dim=2)
proj.cluster_kmeans(100)
proj.implied_timescales()
M = proj.bayesian_markov_model(lag=100)
M.coarse_grain(3)
```

clearly, some high-level API functions do not need to be mapped, e.g. `proj.source()` would make no sense, but `proj.load()` might be handy.

Not only this is simpler than before, but we could now add useful meta-functionalities, leading to the 
following advantages
- The project remembers what we have done. We can always get the contained objects back, e.g.

``` python
    t = proj['tica']
```

And we can follow through striding and lagging and get back to the physical timescales known from     the trajectory files    

``` python
    proj.set_timeunit('nanoseconds')
    print M.timescales()
    7833.2, 652.2, 77.9, ...
```
- Whereas `EstimatedMSM` already does much of the mapping between full and active set of states, we can probably fully automatize that in a project object, and only give easily interpretable analysis objects to the user.
- We can get overall project info

``` python
    proj.info()
```

```
INPUT
-------
[top] topology:    protein.pdb, 1788 atoms
[trajs] trajectories:    2 trajectories, 200 ns, 2000 frames
    traj1.xtc    100 ns, 1000 frames
    traj2.xtc    100 ns, 1000 frames

FEATURES
--------------
[featurizer]: number of features:   3520
- 3520 ca-distances

TRANSFORMS
-------------------
1. [tica]: TICA, lag = 10, dim = 2
2. [kmeans]: k-means, 1000 clusters

MODELS
------------
1. [bmsm]: Bayesian Markov model, lag = 100, 
2. [hmsm]: Hidden Markov model, lag = 100, 3 states
```
- We can have useful meta-functions, e.g. load and save

``` python
    proj = pyemma.open_project('my_analysis.emma')
    proj.estimate_markov_model(lag=1000)
    proj.save('my_analysis_2.emma')
```

so we can always go back to an old analysis and modify it without redoing all the steps. Furthermore, we can have functions that link functionalities of different packages, for example

``` python
    proj[hmsm].sample_structures('./samples_','.xtc')
```

I guess this list could be extended. I have only a rough idea how to implement this, but ideally we would do it just by clever coupling and importing of existing objects/functions into the Project object and without much new code. We should definitely avoid duplicating code.

Issues / Questions:
1. Should we allow for very general transformation pipelines or fix a certain meaningful order and number of steps, such as in Discretizer? 
2. Which transformation/analysis elements should be unique, for which ones we can have multiple? For example, it probably makes sense to only have one cluster discretization, and that should always be at the end of the transformation pipeline. If we decide to change from regspace to kmeans, the old clustering object should be replaced by the new one. On the other hand, we could have multiple models (its, msm, hmm). But if we do estimate_markov_model twice, probably we should replace the old one.
3. What happens if the user changes something in the middle, e.g. decides to change the TICA transform for an otherwise finished project? Either we need to repeat all subsequent estimation steps or simply don't allow this.
"
PyEMMA,markovmodel,73538516,Examples for featurizing + coordinate loading and sourcing,open,,https://github.com/markovmodel/PyEMMA/issues/294,"Assemble a notebook with examples for featurizing (selected coordinates, angles, distances) and load/source. Put this example into pyemma docs (not msmdocs because there's not associated theory), and add a link to the docs of the corresponding function.

I suggest that @marscher and @cwehmeyer or @fabian-paul  and @marscher do this. (When I suggest pairs like that my idea is that one person could write it and the other person review/correct it, but you can do it differently of course)
"
PyEMMA,markovmodel,72157505,Add feature: convenient plotting of contactmaps,open,plots,https://github.com/markovmodel/PyEMMA/issues/285,"We just plotted contactmaps using a combination of mdtraj and pyemma. I think this is a useful feature and easy to implement, so why not offer a plotting function for it?

I roughly sketch the code here - but this needs to be rethought and done properly.

``` python
def plot_contact_map(topfile, trajfile, difference_to=trajfile2, threshold=5)
    # define featurizer. Perhaps we should optionally give a featurizer to be general
    feat = coor.featurizer()
    pairs = feat.pairs(feat.select_ca())
    feat.add_contacts(pairs)
    # compute mean contact map
    map = coor.load(trajfile, feat)
    mean_map = np.mean(map,axis=0)
    # write the feature vectors back to a contact map
    # I imagine this could be a function on its own, as it might be generally useful.
    top = mdtraj.load(topfile).top
    cmatrix = np.zeros((n_residues,n_residues))
    for i in range(len(pairs)):
        res1 = top.atom(pairs[i,0]).residue.index
        res2 = top.atom(pairs[i,1]).residue.index
        cmatrix[res1,res2] = mean_map[i]
    imshow(cmatrix)
    colorbar()
```
"
PyEMMA,markovmodel,69635267,Provide n_atoms and other useful selection stuff in featurizer,open,feature-request,https://github.com/markovmodel/PyEMMA/issues/267,
PyEMMA,markovmodel,64994709,Develop a concept for dealing with meaningless eigenvalues / timescales,open,,https://github.com/markovmodel/PyEMMA/issues/159,"Depending on the basis set, the lag time and the statistics many eigenvalues of a transition matrix, a TICA problem or a variational problem might be negative. They typically look like this:

![screen shot 2015-03-28 at 22 55 35](https://cloud.githubusercontent.com/assets/6495810/6883071/a5f7ea28-d59d-11e4-9d68-1d1f59d6ee18.png)

One has a bunch of positive eigenvalues, related to actual timescales and useful processes, and then there is a large number of eigenvalues that oscillate around zero. The amplitude of this oscillation can become quite large. Clearly, these are numerical artifacts and it makes no sense to convert them into actual timescales. Currently, we won't notice that because we always compute -tau / log(abs(lambda_i)), which is a meaningful definition by itself, but overlooks this problem.

Should we simply cut the spectrum (i.e. make a rank reduction) for every such model, cutting off all eigenvalues with norms smaller or equal to the largest negative eigenvalue? This would reduce the number of available eigenvalues that we can do anything with.
Downside: this will change the structure of our model. A transition matrix will no longer be a transition matrix after we have done this.

Alternative: Whenenver computing timescales or other spectral properties, prohibit (or warn) to use eigenvalues beyond this limit. Note that our ""rule of thumb"" to not use timescales that are smaller than tau is a way to deal with part of this problem, but it's neither a rigorous nor a complete solution.
"
PyEMMA,markovmodel,64555102,[opinion] dynamic caching is more viable than counting memory consumption,open,,https://github.com/markovmodel/PyEMMA/issues/147,"The implementation of the 'operate in memory' function in the coordinate module is complicated by the fact that it is impossible to reliably estimate the memory consumption in the transfromers. Each transformer calls one or many numpy or mdtraj functions each of which may allocate dynamic memory. Those allocations are rarely documentated and we are forced to inspect the source code of the libraries. These allocations can be of the same size as the chunk size.

Additionally the application runs in a multitasking operating system. So querying the amount of free memory and subtracting the amout of memory that our application is expected to consume is no reliable estimate for the free computer memory. Opening a tab in the Firefox browser may suddenly decrease the free memory by dozens of megabytes. Combined with our memory-greedy strategy of putting as many data into the RAM as possible this is a recipe for disaster.

One solution to this problem is that our application should have the abilty to 'back off' an return memory to the operating system in times of memory shortage. All operating systems have an API to report memory shortage to programs. We could interface this API and replace the 'operate in memory' by a data cache. When there is no memory shortage, all data is held in memory. In case of shortage, the cache is emptied and some chunks have to be recomputed.

Here are some possible guidelines for the implemention of such a cache:
- Transformers should not attempt to allocate one huge block of memory that will hold all data.
  Instead they should allocate a cache page for every chunk they recieve from their data producer. If there is no memory shortage, the transfomer can recover all the data from the cache.
  Example:
  
  ``` Python
  # caching a chunk
  cache.insert(traj_number,frame_number,chunk)
  ```
- Cache pages that are not used at the moment can 'go away'.  If the transfomer tries to fetch a chunk from the cache that has gone away, a KeyError is raised. The transfomer can then react to this by requesting the chunk again from its data producer.
  Example:
  
  ``` Python
  # fetching a chunk
  try:
      chunk = cache.fetch(traj_number,frame_number)
  except KeyError: 
      # cache miss, get chunk from data producer 
  ```
- Deletion of cache pages happens as a rection to memory shortage. Memory shortage is reported directly from the operating system, see https://www.kernel.org/doc/Documentation/cgroups/cgroups.txt http://www.newosxbook.com/articles/MemoryPressure.html 
  In response, the cache can delete chunks that are currently not used. Whether a page is currently in use can be found out by checking the refcount of the numpy array which holds the cached chunk.
  The operating system notifications are received in a second (Python) thread. The cache is protected with a lock to prevent race conditions.
- On memory shortage, the cache attepts to delete those pages first, that have not been accessed for the longest time. The oldest chunks are typically the following:
  - chunks in chains that are not used but where the user still holds a reference 
  - chunks from the first steps of a pipeline. When caching is enabled, these chunks are created first and are only accessed once or twice when the next transformer in the chain is parametrized and run. Therfore these are the oldest cached chunks.
  - The first frames from the first trajectories. They are always accessed before the chunks from the end of the trajectory. These chunks are easy to recover, because each transformer can reset its data producer and query the first chunks from the first trajectories again.

So this simplest form of caching which is based on access times, should give good performance when combined with the transformation pipeline.
"
PyEMMA,markovmodel,54483321,Add support for stationary distribution for disconnected transition matrices,open,enhancement,https://github.com/markovmodel/PyEMMA/issues/45,"Decompose into subsets, compute stationary distribution for each of them, and put them together. Scaling should be set in a well-defined way, i.e. by set size.

Although we don't know the right scaling for disconnected matrices this functionality will be useful as a guess of the stationary distribution for some algorithms.

Apply to initalization of HMMs (computation of chi from the PCCA membership matrix)
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
matlab_htk,ronw,7590781,train_htk_recognizer.m input,open,,https://github.com/ronw/matlab_htk/issues/1,"I want to use htk for a simple problem of single word recognition. however, i do not understand in what format i have to input for example the word_grammar to train_htk_recognizer.m. according to the htk tutorial example i have  $digit = ONE | TWO | THREE | FOUR | FIVE |
             SIX | SEVEN | EIGHT | NINE | OH | ZERO; and grammar ( SENT-START ( <$digit>) SENT-END ). But how do i write the grammar in matlab when i give the digits as an array of words? and what is an fsm data structure? 
i have been trying for  a while without result. would be really great to get some help!
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmmsort.py,grero,349876231,HPC scripts,open,,https://github.com/grero/hmmsort.py/pull/18,
hmmsort.py,grero,337816844,Small templates,open,,https://github.com/grero/hmmsort.py/issues/7,"It seems like templates are selected based on the energy of a noise patch rather than the peak exceeding some multiple of the estimated standard deviation. We could change this, so that the selection is more in line with way we expect from a visual inspection of the templates."
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
mapmatchingkit,oldrev,362850487,[Task] Update To Barefoot 0.1.5,open,task,https://github.com/oldrev/mapmatchingkit/issues/8,Implements the new U-turn shorten function in Barefoot 0.1.5.
mapmatchingkit,oldrev,315440751,How do you save your UBODT table,open,,https://github.com/oldrev/mapmatchingkit/issues/6,"Hello, 
I come from the barefoot project. It seems that you create UBODT table to do some precomputation to speed up the router function in this project. So I wonder how do you save your UBODT table and read it again from the file？
Thank you. "
mapmatchingkit,oldrev,298860480,UBODT Optimization,open,enhancement,https://github.com/oldrev/mapmatchingkit/issues/4,Using the UBODT algorithm to improve the performance of route searching between two GPS points.
mapmatchingkit,oldrev,298522420,Add license header to every file,open,task,https://github.com/oldrev/mapmatchingkit/issues/2,As the Apache License required.
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
pykov,yassersouri,40428274,learning with different length observations,open,,https://github.com/yassersouri/pykov/issues/1,"This is actually a problem that is not fixed by hmmlearn\hmmlearn. 
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,wangkuiyi,48556448,float64 number precision,open,,https://github.com/wangkuiyi/hmm/issues/37,"As we are calculating probability values at the magnitude of e^(-300), we need to make sure the precisions are not underflow
"
hmm,wangkuiyi,48551305,Converge,open,,https://github.com/wangkuiyi/hmm/pull/36,"Fixing issue #30 
"
hmm,wangkuiyi,48448561,"Add command line paremter ""-seed"" to trainer",open,,https://github.com/wangkuiyi/hmm/pull/32,"This fixes https://github.com/wangkuiyi/hmm/issues/27
"
hmm,wangkuiyi,48441459,A bash script to distribute jobs training models from the same corpus but different #states.,open,enhancement,https://github.com/wangkuiyi/hmm/issues/31,"This work depends on 

  https://github.com/wangkuiyi/hmm/issues/28
  https://github.com/wangkuiyi/hmm/issues/29 
  https://github.com/wangkuiyi/hmm/issues/30.
"
hmm,wangkuiyi,48441342,Automatic determine if a training jobs has converged.,open,enhancement,https://github.com/wangkuiyi/hmm/issues/30,"We need to add a command line flag to trainer: ""-converge"", which is a float64 threshold value.

As the EM algorithm converges monotonically, we can simple check if the log-likelihood delta between the current iteration and the previous iteration is smaller than ""-converge"".  If so, we can stop training, compute testing data log-likelihood, save model, and quit.

This depends on https://github.com/wangkuiyi/hmm/issues/29.
"
hmm,wangkuiyi,48441045,Compute testing data log-likelihood in addition to training data log-likelihood.,open,enhancement,https://github.com/wangkuiyi/hmm/issues/29,"Currently we compute training data log-likelihood in every training iteration. This helps us plot the log-likelihood v.s. iteration curve and indicate the convergence of a training job.

We need in addition to this is compute testing data log-likelihood after the model get converged.  This value indicate how ""good"" the model can explain new data, and will be used in model selection (learning the optimal number of states.)
"
hmm,wangkuiyi,48440616,A bash script to distribute jobs training models from the same corpus but different initialization.,open,enhancement,https://github.com/wangkuiyi/hmm/issues/28,"As each trainer jobs would utilize all cores in a computer using multi-threading, we need to use many computers to train models from a corpus but with difference random initialization.

This issue depends on https://github.com/wangkuiyi/hmm/issues/27.

Also, we need to collect volunteer computers.
"
hmm,wangkuiyi,48440142,Add random init seed parameter to trainer,open,enhancement,https://github.com/wangkuiyi/hmm/issues/27,"We need a command line parameter for trainer: ""-seed"", whose value is by default negative, which means fully random initialization of the pseudo random number generator. If users set it to 0 or positive, we use that value as the initialization of our pseudo random number generator, which mean fully deterministic and no randomness.

This is trick for unit testing and debugging machine learning code -- enable the removal of randomness in testing/debugging sessions.
"
hmm,wangkuiyi,48026453,Remove high freq,open,,https://github.com/wangkuiyi/hmm/pull/25,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
msmbuilder,msmbuilder,382722559,Download links for Jupyter Notebook examples result in 404,open,,https://github.com/msmbuilder/msmbuilder/issues/1110,"Apparently, the absolute path of the Travis build is used (`/home/travis/...`) instead of the relative path to the current page.

For example, [Fs Peptide (in RAM)](http://msmbuilder.org/development/examples/Fs-Peptide-in-RAM.html) links (in the bottom) to [this page](http://msmbuilder.org/home/travis/build/msmbuilder/msmbuilder/docs/_build/html/examples/Fs-Peptide-in-RAM/Fs-Peptide-in-RAM.ipynb) (404), but it should point to [this one](http://msmbuilder.org/3.8.0/examples/Fs-Peptide-in-RAM/Fs-Peptide-in-RAM.ipynb). "
msmbuilder,msmbuilder,371455046,Dumping snapshots from the Macrostate,open,,https://github.com/msmbuilder/msmbuilder/issues/1105,"Hi,

I performed the PCCAplus macrostate analysis like following:

```
from msmbuilder.lumping import PCCAPlus
pcca = PCCAPlus.from_msm(msm, n_macrostates=5)
macro_trajs = pcca.transform(clustered_trajs)
```

Plotting it looks like following

![image](https://user-images.githubusercontent.com/33577253/47147242-1a3b7400-d2ce-11e8-8752-9e58ea04efbc.png)


I want to dump snapshots from these colored macrostates (in PDB format) and want to visualise them. Any help?"
msmbuilder,msmbuilder,365176846,Fix const-ness of some functions for rate matrix code,open,,https://github.com/msmbuilder/msmbuilder/pull/1102," - [x] Fix #1101
 - [ ] Add tests
 - [ ] Update changelog
"
msmbuilder,msmbuilder,365175860,Compilation failure with Cython 0.28.5 / Python 3.7,open,,https://github.com/msmbuilder/msmbuilder/issues/1101,"Any ideas? This is with msmbuilder-3.8.0.tar.gz from pypi.

```
    cythoning msmbuilder/msm/_ratematrix.pyx to msmbuilder/msm/_ratematrix.cpp

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
        if (A.shape[0] != B.shape[0]) or (A.shape[1] != B.shape[1]):
            return -1

        for i in range(A.shape[0]):
            for j in range(A.shape[1]):
                A[i, j] = A[i, j] * B[i, j]
                ^
    ------------------------------------------------------------

    msmbuilder/msm/_ratematrix_support.pyx:113:13: Assignment to const dereference

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
        cdef double rowsum
        # T = np.dot(np.dot(V, np.diag(expwt)), U.T)
        for i in range(n):
            for j in range(n):
                temp[i, j] = V[i, j] * expwt[j]
        cdgemm_NT(temp, U, T)
                       ^
    ------------------------------------------------------------

    msmbuilder/msm/_ratematrix_support.pyx:132:20: Memoryview 'const double[:, ::1]' not conformable to memoryview 'double[:, ::1]'.

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            On exit, out[i] contains the derivative of the `i`th eigenvalue
            of K with respect to \theta_u.
        """"""
        cdef npy_intp i
        for i in range(n):
            cdgemv_N(dKu, V[:, i], temp)
                    ^
    ------------------------------------------------------------

    msmbuilder/msm/_ratematrix_support.pyx:162:17: Memoryview 'const double[:, ::1]' not conformable to memoryview 'double[:, ::1]'.

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            On exit, out[i] contains the derivative of the `i`th eigenvalue
            of K with respect to \theta_u.
        """"""
        cdef npy_intp i
        for i in range(n):
            cdgemv_N(dKu, V[:, i], temp)
                          ^
    ------------------------------------------------------------

    msmbuilder/msm/_ratematrix_support.pyx:162:23: Memoryview 'const double[:]' not conformable to memoryview 'double[:]'.

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            of K with respect to \theta_u.
        """"""
        cdef npy_intp i
        for i in range(n):
            cdgemv_N(dKu, V[:, i], temp)
            cddot(temp, U[:, i], &out[i])
                        ^
    ------------------------------------------------------------

    msmbuilder/msm/_ratematrix_support.pyx:163:21: Memoryview 'const double[:]' not conformable to memoryview 'double[:]'.

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
                        dLdK[i, j] = counts[i, j] / T[i, j]

            # out = U \left(V^T dLdK U \circ X(\lambda, t))\right) V^T

            # temp2 = V^T dLdK U
            cdgemm_TN(V, dLdK, temp1)
                     ^
    ------------------------------------------------------------

    msmbuilder/msm/_ratematrix_support.pyx:210:18: Memoryview 'const double[:, ::1]' not conformable to memoryview 'double[:, ::1]'.

    Error compiling Cython file:
    ------------------------------------------------------------
    ...

            # out = U \left(V^T dLdK U \circ X(\lambda, t))\right) V^T

            # temp2 = V^T dLdK U
            cdgemm_TN(V, dLdK, temp1)
            cdgemm_NN(temp1, U, temp2)
                            ^
    ------------------------------------------------------------

    msmbuilder/msm/_ratematrix_support.pyx:211:25: Memoryview 'const double[:, ::1]' not conformable to memoryview 'double[:, ::1]'.

    Error compiling Cython file:
    ------------------------------------------------------------
    ...

            # temp2 =  (V^T dLdK U \circ X(w, t))
            hadamard_X(w, expwt, t, n, temp2)

            # dT = U \left(V^TCU \circ X(\lambda, t))\right) V^T
            cdgemm_NN(U, temp2, temp1)
                     ^
    ------------------------------------------------------------

    msmbuilder/msm/_ratematrix_support.pyx:217:18: Memoryview 'const double[:, ::1]' not conformable to memoryview 'double[:, ::1]'.

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            # temp2 =  (V^T dLdK U \circ X(w, t))
            hadamard_X(w, expwt, t, n, temp2)

            # dT = U \left(V^TCU \circ X(\lambda, t))\right) V^T
            cdgemm_NN(U, temp2, temp1)
            cdgemm_NT(temp1, V, dT)
                            ^
    ------------------------------------------------------------

    msmbuilder/msm/_ratematrix_support.pyx:218:25: Memoryview 'const double[:, ::1]' not conformable to memoryview 'double[:, ::1]'.

    Error compiling Cython file:
    ------------------------------------------------------------
    ...

        lognorm = log(norm)
        for i in range(n):
            logpi[i] = theta[i] - lognorm

        cddot(alpha, logpi, &logp)
             ^
    ------------------------------------------------------------

    msmbuilder/msm/_ratematrix_priors.pyx:53:10: Memoryview 'const double[::1]' not conformable to memoryview 'double[:]'.

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
                # dK_dtheta_u(theta, n, i, j, out=dKij)
                # cdgemv_N(covar_theta, dKij, temp)

                memset(&dKij[0], 0, size*sizeof(double))
                memset(&temp[0], 0, size*sizeof(double))
                dK_dtheta_u(theta, n, i, j, out=dKij, A=covar_theta, out2=temp)
                                                       ^
    ------------------------------------------------------------

    msmbuilder/msm/_ratematrix.pyx:539:52: Memoryview 'const double[:, ::1]' not conformable to memoryview 'double[:, :]'.

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
        for u in range(size):
            dK_dtheta_ij(theta, n, u, A=None, out=dKu)
            dw_du(dKu, U, V, n, temp1, dlambda_dtheta[:, u])

        for i in range(n):
            cdgemv_N(covar_theta, dlambda_dtheta[i, :].copy(), temp2)
                    ^
    ------------------------------------------------------------

    msmbuilder/msm/_ratematrix.pyx:680:17: Memoryview 'const double[:, ::1]' not conformable to memoryview 'double[:, ::1]'.

    Error compiling Cython file:
    ------------------------------------------------------------
    ...
            dw_du(dKu, U, V, n, temp1, dlambda_dtheta[:, u])
            for i in range(n):
                dtau_dtheta[i, u] = dlambda_dtheta[i, u] / (w[i]**2)

        for i in range(1, n):
            cdgemv_N(covar_theta, dtau_dtheta[i], temp2)
                    ^
    ------------------------------------------------------------

    msmbuilder/msm/_ratematrix.pyx:751:17: Memoryview 'const double[:, ::1]' not conformable to memoryview 'double[:, ::1]'.
    building 'msmbuilder.msm._ratematrix' extension
```"
msmbuilder,msmbuilder,359366269,are tICs unit and orthogonal vectors?,open,,https://github.com/msmbuilder/msmbuilder/issues/1100,"Hi, Msmbuilder is a great project!
I am trying to understand and use it.
There is a series of papers on tICA
For example, in ""tICA-Metadynamics: Accelerating Metadynamics by Using Kinetically
Selected Collective Variables"",  authors had pointed that tICs are orthogonal.

I note you apply scipy.linalg.eigh compute the eigenvectors, which should be unit and orthogonal.
But I had go through the ”tICA-vs-PCA.ipynb“, and I found that their tICs are neither unit nor orthogonal.
Why do you compute this kind of tICs?

As I know, the tICs should be unit or normalized in projection(dimensional reduction).

Very sorry, I am lost here.

Thank you very much!



 
"
msmbuilder,msmbuilder,343600067,Some questions about RawPositionsFeaturizer,open,,https://github.com/msmbuilder/msmbuilder/issues/1098,"I have some queries regarding `RawPositionsFeaturizer`

1. If I want to use atom indices as mentioned in the document like `RawPositionsFeaturizer(atom_indices=None, ref_traj=None)`. Is the following correct way of using it

`RawPositionsFeaturizer(atom_indices=([[18, 35])), ref_traj=None)`

So I want atoms between 18 to 35. Is this the correct way of doing it?

2. If I just want `CA` atoms in the `RawPositionsFeaturize` then should I explicitly use their atom indices or can I use something like `scheme='CA'`  ?

3. When I am using the `tica_metadynamics.plumed_writer ` how can I get the arguments printed out for metadynamics. I managed to print out the vectors. But is there a way to print out the arguments corresponding to the vectors in other word I want to make a Plumed input file. (see my Notebook attached)
[rawposition_test_bpti.pdf](https://github.com/msmbuilder/msmbuilder/files/2219161/rawposition_test_bpti.pdf)
"
msmbuilder,msmbuilder,342005506,[Question] Erratic timescale plots and what they mean,open,,https://github.com/msmbuilder/msmbuilder/issues/1096,"I'm trying to find a good lag_time for the MSM by plotting the timescales using:
```
n_clusters = 6
reassignment_ratio = 0.2
tol = 0.5
max_no_improvement = 2000
max_iter = 20000
init_size = 200
clusterer = cluster(n_clusters=n_clusters, reassignment_ratio = reassignment_ratio, tol=tol, max_no_improvement = max_no_improvement, max_iter = max_iter, init_size=init_size)
clusterer.fit(tica_trajs_orig)
clustered_trajs = clusterer.transform(tica_trajs)  # This is every 10th frame
clustered_trajs_orig = clusterer.transform(tica_trajs_orig)  # This is all the data

lag_times = [1,2,3,4,5,8,10,15,25,35,50,60,75,100,125,150,200,300,500]
msm_objs = []
for lag in lag_times:
    # Construct MSM
    msm = MarkovStateModel(lag_time=lag, n_timescales=3, verbose= False)
    msm.fit(clustered_trajs_orig)
    msm_objs.append(msm)
    for i, (ts, ts_u) in enumerate(zip(msm.timescales_, msm.uncertainty_timescales())):
        timescale_ns = ts * 20/ 1000
        uncertainty_ns = ts_u * 20 / 1000

# Plot Timescales
colors = ['pomegranate', 'beryl', 'tarragon', 'rawdenim', 'carbon','blue']
msme.plot_implied_timescales(msm_objs, color_palette=colors,
                             xlabel='Lag time (frames)',
                             ylabel='Implied Timescales (frames)')

```

This works, but the results tend to be erratic regardless of the parameters I've chosen (6 clusters gives a GMRQ score of about 5.3 and levels off). Repeating just the above lines of code I get very different results:
![image](https://user-images.githubusercontent.com/38046333/42832859-2ff0b792-89b8-11e8-8c2e-4ec40ebd6646.png)

![image](https://user-images.githubusercontent.com/38046333/42832779-edb36b68-89b7-11e8-8ec8-14e907e36a31.png)

![image](https://user-images.githubusercontent.com/38046333/42832833-17c7c034-89b8-11e8-8bd3-727206a645a7.png)

![image](https://user-images.githubusercontent.com/38046333/42833025-a9dcdc48-89b8-11e8-8aa1-237f84f3f499.png)

And almost anything in between. The middle two look like they may have the largest eigenvalue disappear and the second one replace it. 

Does this mean the model should not be used since a consistent timescale plot can't be obtained? 

Should I use a different type of MSM (Bayesian, or continuous)?

What meaning should I draw from these plots?



"
msmbuilder,msmbuilder,329937881,Tutorial Help issue,open,,https://github.com/msmbuilder/msmbuilder/issues/1093,"Hello! I'm trying to do the MSMBuilder tutorial, but when I type in ""TemplateProject"", I get the output below:

For reference, I managed to install msmbuilder per the installation instructions on MSMBuilder.org/3.8.0/ using Python 2.7 version 5.2 for 32-bit. My computer uses a 64-bit operating system, but I tried using the 64-bit version earlier and it didn't work either for Python 2.7 (wouldn't even let me install msmbuilder) nor Python 3.6 (exact same error as in python 2.7 for 32-bit). Any help is greatly appreciated!

[ERROR STARTS HERE]
```python-traceback
C:\Users\lilia\Anaconda2\lib\site-packages\sklearn\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)
C:\Users\lilia\Anaconda2\lib\site-packages\sklearn\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.
  DeprecationWarning)
C:\Users\lilia\Anaconda2\lib\site-packages\msmbuilder\io\io.py:54: BackupWarning: 0-test-install.py exists. Moving it to 0-test-install.py.bak.10
  BackupWarning)
C:\Users\lilia\Anaconda2\lib\site-packages\msmbuilder\io\io.py:54: BackupWarning: 1-get-example-data.py exists. Moving it to 1-get-example-data.py.bak.10
  BackupWarning)
C:\Users\lilia\Anaconda2\lib\site-packages\msmbuilder\io\io.py:54: BackupWarning: README.md exists. Moving it to README.md.bak.10
  BackupWarning)
C:\Users\lilia\Anaconda2\lib\site-packages\msmbuilder\io\io.py:54: BackupWarning: analysis exists. Moving it to analysis.bak.10
  BackupWarning)
---------------------
An unexpected error has occurred with MSMBuilder (version 3.8.0), please
consider sending the following traceback to MSMBuilder GitHub issue tracker at:
            https://github.com/msmbuilder/msmbuilder/issues

Traceback (most recent call last):
  File ""C:\Users\lilia\Anaconda2\Scripts\msmb-script.py"", line 5, in <module>
    sys.exit(msmbuilder.scripts.msmb.main())
  File ""C:\Users\lilia\Anaconda2\lib\site-packages\msmbuilder\scripts\msmb.py"", line 28, in main
    app.start()
  File ""C:\Users\lilia\Anaconda2\lib\site-packages\msmbuilder\cmdline.py"", line 464, in start
    instance.start()
  File ""C:\Users\lilia\Anaconda2\lib\site-packages\msmbuilder\commands\template_project.py"", line 57, in start
    self.instance.do()
  File ""C:\Users\lilia\Anaconda2\lib\site-packages\msmbuilder\io\project_template.py"", line 136, in do
    self.layout.render(self.template_dir_kwargs, self.template_kwargs)
  File ""C:\Users\lilia\Anaconda2\lib\site-packages\msmbuilder\io\project_template.py"", line 280, in render
    subdir.render(template_dir_kwargs, template_kwargs)
  File ""C:\Users\lilia\Anaconda2\lib\site-packages\msmbuilder\io\project_template.py"", line 274, in render
    os.symlink(""../{}"".format(dep), bn)
AttributeError: 'module' object has no attribute 'symlink'
```"
msmbuilder,msmbuilder,326580190,too much data seems to stop the msmbuilder,open,,https://github.com/msmbuilder/msmbuilder/issues/1087,"I've been successful at creating the msm and then plotting the free energy landscape with smaller data sets. But once I try to include all my data, I get warning and errors. Any ideas how to fix the (bolded) warnings (not the seaborn and other common warnings) and the plot error which works if I use less data?

```python-traceback
(msmb) eddie@heartofgold:/media/eddie/2e30828e-f7b9-4c45-951e-357647f832b6/code/namd/output/p450/analysis_apo$ python AnalysisScript.py &
/home/eddie/.conda/envs/msmb/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)
/home/eddie/.conda/envs/msmb/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.
  DeprecationWarning)
/home/eddie/.conda/envs/msmb/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.
  from pandas.core import datetools
/home/eddie/.conda/envs/msmb/lib/python3.6/site-packages/seaborn/apionly.py:6: UserWarning: As seaborn no longer sets a default style on import, the seaborn.apionly module is deprecated. It will be removed in a future version.
  warnings.warn(msg, UserWarning)
**WARNING:root:Too few points to create valid contours
WARNING:root:Too few points to create valid contours
WARNING:root:Too few points to create valid contours**
MSM contains 3 strongly connected components above weight=0.05. Component 2 selected, with population 48.885876%
array([[ 0.99666856,  0.13194909],
       [ 1.02115575,  0.14496381],
       [ 1.0184084 ,  0.22198013],
       ..., 
       [-1.24613583,  1.21543633],
       [-1.18217391,  1.15694605],
       [-1.18811775,  1.1854144 ]])
39149
array([ 0.06479284,  0.11444611,  0.07799189, ...,  0.10588862,
        0.0809993 ,  0.13973087])
19149
12
Traceback (most recent call last):
  File ""AnalysisScript.py"", line 141, in <module>
    main()
  File ""AnalysisScript.py"", line 135, in main
    cbar_kwargs={'format': '%.1f', 'label': 'Free energy (kcal/mol)'}
  File ""/home/eddie/.conda/envs/msmb/lib/python3.6/site-packages/msmexplorer/utils.py"", line 101, in wrapper
    return func(*args, **kwargs)
  File ""/home/eddie/.conda/envs/msmb/lib/python3.6/site-packages/msmexplorer/plots/projection.py"", line 121, in plot_free_energy
    idx = random_state.choice(range(data.shape[0]), size=n_samples, p=pi)
  File ""mtrand.pyx"", line 1126, in mtrand.RandomState.choice
ValueError: a and p must have same size
```

## The script is
```python
import mdtraj as md
from msmbuilder.io import gather_metadata, load_trajs, NumberedRunsParser, preload_tops
from msmbuilder.featurizer import DihedralFeaturizer
from msmbuilder.decomposition import tICA
from msmbuilder.cluster import MiniBatchKMeans
from msmbuilder.msm import MarkovStateModel
import msmexplorer as msme
import sys
import matplotlib
from matplotlib import pyplot as plt
from multiprocessing import Pool
import numpy as np

lagt = 20

\# Parses trajectory files for metadata
parser = NumberedRunsParser(
    traj_fmt=""s0tep7_productionC{run}.nowat.dcd"",
    top_fn=""top.pdb"",
    step_ps=20,
)

\# Gathers metadata using parser
meta = gather_metadata(""traj/s0tep7*nowat.dcd"", parser)

\# Loads topology (.pdb) file
tops = preload_tops(meta)

\# Featurizes a single trajectory
def feat(irow):
	i, row = irow
	dihed_feat = DihedralFeaturizer(types=['phi', 'psi'])
	traj = md.load(row['traj_fn'], top=tops[row['top_fn']])

	feat_traj = dihed_feat.partial_transform(traj)
	return i, feat_traj

\# Runs featurization of all trajectories in parallel
def loadSimulationDataAndFeaturize():

	with Pool() as pool:
		dihed_trajs = dict(pool.imap_unordered(feat, meta.iterrows()))

	\# Convert format of trajectories from a dictionary of numpy arrays to a list of numpy arrays
	diheds = list(dihed_trajs.values())

	return diheds

def main():
	diheds = loadSimulationDataAndFeaturize()

	rs = np.random.RandomState(42)
	
	\# Perform dimensionality reduction
	tica_model = tICA(lag_time=lagt, n_components=4)
	tica_trajs = tica_model.fit_transform(diheds)
	
	\#plot tica
	data = np.concatenate(tica_trajs, axis=0)
	fig1,ax1 = plt.subplots()
	ax1 = msme.plot_histogram(data, color='oxblood', quantiles=(0.5,),
                    labels=['$tIC1$', '$tIC2$', '$tIC3$', '$tIC4$'],
                    show_titles=True)
        
        \# Only have 2 tica for the Free Energy 
	tica_model = tICA(lag_time=lagt, n_components=2)
	tica_trajs = tica_model.fit_transform(diheds)

	\# Perform clustering
	clusterer = MiniBatchKMeans(n_clusters=50, random_state=rs)
	clustered_trajs = clusterer.fit_transform(tica_trajs)
	
	\# Construct MSM
	msm = MarkovStateModel(lag_time=lagt)
	assignments = msm.fit_transform(clustered_trajs)

	\# Plot Free Energy
	data = np.concatenate(tica_trajs, axis=0)

	print(repr(data))
	print(repr(data.shape[0]))

	pi_0 = msm.populations_[np.concatenate(assignments, axis=0)]

	print(repr(pi_0))
	print(repr(pi_0.shape[0]))
	print(repr(msm.n_states_))

	fig2,ax2 = plt.subplots()
	ax2 = msme.plot_voronoi(clusterer, xlabel='tIC1', ylabel='tIC2')

	pos = dict(zip(range(clusterer.n_clusters), clusterer.cluster_centers_))
	fig3, ax3 = plt.subplots()
	ax3 = msme.plot_msm_network(msm, pos=pos, node_color='pomegranate',edge_color='carbon')
	ax3 = msme.plot_free_energy(data, obs=(0, 1), n_samples=10000000, pi=pi_0,
						  temperature=310.0,
	                      random_state=rs,
	                      shade=True,
	                      clabel=True,
	                      clabel_kwargs={'fmt': '%.1f'},
	                      cbar=True,
	                      cbar_kwargs={'format': '%.1f', 'label': 'Free energy (kcal/mol)'}
	                      )
	fig = plt.show()

\# Make sure the thread is the main thread so that multithreading doesn't recursively run.
if __name__ == ""__main__"":
    main()
```


"
msmbuilder,msmbuilder,314191208,Intel MKL ERROR: Parameter 5 was incorrect on entry to DGEEV.,open,,https://github.com/msmbuilder/msmbuilder/issues/1078,"Hi,

When I'm running the timescale.py in the TemplateProject MSM folder, I got this error multiple times:

`Intel MKL ERROR: Parameter 5 was incorrect on entry to DGEEV.`
`ValueError: Internal work array size computation failed: -5`

Any idea what's going on?

Thanks"
msmbuilder,msmbuilder,310920631,msmb fails to load dll on clean install,open,,https://github.com/msmbuilder/msmbuilder/issues/1075,"I've just installed anaconda and msmbuilder but I cannot get it to run successfully. I get the following error (in an Administrator anaconda prompt in windows 10):
(base) D:\programs>msmb -h
Traceback (most recent call last):
  File ""D:\programs\anaconda\Scripts\msmb-script.py"", line 3, in <module>
    import msmbuilder.scripts.msmb
  File ""D:\programs\anaconda\lib\site-packages\msmbuilder\scripts\msmb.py"", line 5, in <module>
    from ..commands import *
  File ""D:\programs\anaconda\lib\site-packages\msmbuilder\commands\__init__.py"", line 2, in <module>
    from .featurizer import (AtomPairsFeaturizerCommand, ContactFeaturizerCommand,
  File ""D:\programs\anaconda\lib\site-packages\msmbuilder\commands\featurizer.py"", line 6, in <module>
    import mdtraj as md
  File ""D:\programs\anaconda\lib\site-packages\mdtraj\__init__.py"", line 53, in <module>
    from ._rmsd import rmsd
ImportError: DLL load failed: The application has failed to start because its side-by-side configuration is incorrect. Please see the application event log or use the command-line sxstrace.exe tool for more detail.

After installing the downloaded anaconda, I upgraded the navigator and anaconda. Then I did the install of msmb via conda (conda install -c omnia msmbuilder) and it worked.  Now I'm stuck. Maybe the following will help diagnose the issue.

(base) D:\programs>conda info

     active environment : base
    active env location : D:\programs\anaconda
            shell level : 1
       user config file : C:\Users\eackad\.condarc
 populated config files : C:\Users\eackad\.condarc
          conda version : 4.5.0
    conda-build version : 3.4.1
         python version : 2.7.14.final.0
       base environment : D:\programs\anaconda  (writable)
           channel URLs : http://conda.anaconda.org/omnia/win-64
                          http://conda.anaconda.org/omnia/noarch
                          https://repo.anaconda.com/pkgs/main/win-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/free/win-64
                          https://repo.anaconda.com/pkgs/free/noarch
                          https://repo.anaconda.com/pkgs/r/win-64
                          https://repo.anaconda.com/pkgs/r/noarch
                          https://repo.anaconda.com/pkgs/pro/win-64
                          https://repo.anaconda.com/pkgs/pro/noarch
                          https://repo.anaconda.com/pkgs/msys2/win-64
                          https://repo.anaconda.com/pkgs/msys2/noarch
          package cache : D:\programs\anaconda\pkgs
                          C:\Users\eackad\AppData\Local\conda\conda\pkgs
       envs directories : D:\programs\anaconda\envs
                          C:\Users\eackad\AppData\Local\conda\conda\envs
                          C:\Users\eackad\.conda\envs
               platform : win-64
             user-agent : conda/4.5.0 requests/2.18.4 CPython/2.7.14 Windows/10 Windows/10.0.14393
          administrator : True
             netrc file : None
           offline mode : False
(base) D:\programs>conda list
# packages in environment at D:\programs\anaconda:
#
# Name                    Version                   Build  Channel
_ipyw_jlab_nb_ext_conf    0.1.0            py27h9111531_0
alabaster                 0.7.10           py27h2cab13d_0
anaconda                  5.1.0                    py27_2
anaconda-client           1.6.14                   py27_0
anaconda-navigator        1.8.2                    py27_0
anaconda-project          0.8.2            py27he89e3ca_0
asn1crypto                0.24.0                   py27_0
astroid                   1.6.1                    py27_0
astropy                   1.0.5                np19py27_1
attrs                     17.4.0                   py27_0
babel                     2.5.3                    py27_0
backports                 1.0              py27h6492d98_1
backports.functools_lru_cache 1.4              py27h9586e20_1
backports.shutil_get_terminal_size 1.0.0            py27h1657125_2
backports.shutil_which    3.5.1                    py27_2
backports_abc             0.5              py27h0ec6b72_0
beautifulsoup4            4.6.0            py27hc287451_1
bitarray                  0.8.1            py27h0c8e037_1
bkcharts                  0.2              py27h92b6de3_0
blaze                     0.11.3           py27h97e5449_0
bleach                    2.1.2                    py27_0
bokeh                     0.12.13          py27h5a33001_0
boto                      2.48.0           py27h1ccb131_1
bottleneck                1.0.0                np19py27_0
bzip2                     1.0.6                h8a7aa22_4
ca-certificates           2017.08.26           h94faf87_0
cdecimal                  2.3              py27h0c8e037_3
certifi                   2018.1.18                py27_0
cffi                      1.11.4           py27h0c8e037_0
chardet                   3.0.4            py27h56c3b73_1
click                     6.7              py27hb6defca_0
cloudpickle               0.5.2                    py27_1
clyent                    1.2.2            py27h4424948_1
colorama                  0.3.9            py27hdfe4ae1_0
comtypes                  1.1.4                    py27_0
conda                     4.5.0                    py27_0
conda-build               3.4.1                    py27_0
conda-env                 2.6.0                h36134e3_1
conda-verify              2.0.0            py27h5217224_0
configparser              3.5.0            py27h2fa79a8_0
console_shortcut          0.1.1                h6bb2dd7_3
contextlib2               0.5.5            py27h42efda5_0
cryptography              2.1.4            py27h0628b04_0
curl                      7.58.0               h7a46e7a_0
cycler                    0.10.0           py27h59acbbf_0
cython                    0.27.3           py27h566c365_0
cytoolz                   0.9.0            py27h0c8e037_0
dask                      0.15.2                   py27_0
dask-core                 0.16.1                   py27_0
datashape                 0.5.4            py27h3d6e61b_0
decorator                 4.2.1                    py27_0
distributed               1.20.2                   py27_0
docutils                  0.14             py27h8652d09_0
entrypoints               0.2.3            py27h0271f2b_2
enum34                    1.1.6            py27h2aa175b_1
et_xmlfile                1.0.1            py27h1de5d23_0
fastcache                 1.0.2            py27h0c8e037_2
filelock                  2.0.13           py27h49e51d3_0
flask                     0.12.2           py27h30d9212_0
flask-cors                3.0.3            py27h4926b05_0
freetype                  2.8                  hea645e0_1
funcsigs                  1.0.2            py27h8885ae1_0
functools32               3.2.3.2          py27h0cdbcdb_1
futures                   3.2.0            py27h8b2aecd_0
get_terminal_size         1.0.0                h38e98db_0
gevent                    1.2.2            py27h1842022_0
glob2                     0.6              py27hd4eee8c_0
greenlet                  0.4.12           py27h32400d3_0
grin                      1.2.1                    py27_4
h5py                      2.7.1            py27h2dd4c20_0
hdf5                      1.10.1               h79de857_2
heapdict                  1.0.0                    py27_2
html5lib                  1.0.1            py27h5a33001_0
icc_rt                    2017.0.4             h97af966_0
icu                       58.2                 h2aa20d9_1
idna                      2.6              py27h1ea29d3_1
imageio                   2.2.0            py27h283db88_0
imagesize                 0.7.1            py27h1482bd8_0
intel-openmp              2018.0.0             hd92c6cd_8
ipaddress                 1.0.19                   py27_0
ipykernel                 4.8.0                    py27_0
ipython                   5.4.1                    py27_2
ipython_genutils          0.2.0            py27hbe997df_0
ipywidgets                7.1.1                    py27_0
isort                     4.2.15           py27hdc949c3_0
itsdangerous              0.24             py27hcf63135_1
jdcal                     1.3              py27h8c72977_0
jedi                      0.11.1                   py27_0
jinja2                    2.10             py27hba1794b_0
jpeg                      9b                   ha175dff_2
jsonschema                2.6.0            py27haaf3834_0
jupyter                   1.0.0                    py27_4
jupyter_client            5.2.2                    py27_0
jupyter_console           5.2.0            py27h6ed736b_1
jupyter_core              4.4.0            py27h1619e65_0
jupyterlab                0.31.4                   py27_0
jupyterlab_launcher       0.10.2                   py27_0
lazy-object-proxy         1.3.1            py27ha5c8080_0
libcurl                   7.58.0               h7a46e7a_0
libiconv                  1.15                 hda2e4ec_7
libpng                    1.6.34               h325896a_0
libssh2                   1.8.0                h77a7533_4
libtiff                   4.0.9                hafacce9_0
libxml2                   2.9.7                h325896a_0
libxslt                   1.1.32               h89dfad8_0
llvmlite                  0.21.0           py27h831ec56_0
llvmpy                    0.12.7                   py27_0
locket                    0.2.0            py27h1ca288a_1
lxml                      4.1.1            py27h31b8cb8_1
lzo                       2.10                 h0bb7fe3_2
markupsafe                1.0              py27h9d4480d_1
matplotlib                2.1.2            py27ha51faf0_0
mccabe                    0.6.1            py27hde0bf6e_1
mdtraj                    1.9.1                    py27_0    omnia
menuinst                  1.4.11           py27h0c8e037_0
mistune                   0.8.3                    py27_0
mkl                       2018.0.1             h2108138_4
mkl-service               1.1.2            py27h3c6b6b0_4
mpmath                    1.0.0            py27h0d59bc2_2
msgpack-python            0.5.1            py27hdc96acc_0
msmbuilder                3.4.0                np19py27_0    http://conda.binstar.org/omnia
multipledispatch          0.4.9            py27h8ebb51e_0
navigator-updater         0.1.0            py27h5e42984_0
nbconvert                 5.3.1            py27h7a573cf_0
nbformat                  4.4.0            py27hf49b375_0
networkx                  2.1                      py27_0
nltk                      3.2.5            py27h88af825_0
nose                      1.3.7            py27h84c72c6_2
notebook                  5.4.0                    py27_0
numba                     0.15.1               np19py27_0
numexpr                   2.3.1                np19py27_0
numpy                     1.9.3            py27he78448b_2
numpydoc                  0.7.0            py27hf7b062b_0
odo                       0.5.1            py27h64810b2_0
olefile                   0.45.1                   py27_0
openpyxl                  2.4.10                   py27_0
openssl                   1.0.2o               h2c51139_0
packaging                 16.8             py27hae1a450_1
pandas                    0.20.3           py27he04484b_2
pandoc                    1.19.2.1             hb2460c7_1
pandocfilters             1.4.2            py27h76461d3_1
parso                     0.1.1            py27hd69ea77_0
partd                     0.3.8            py27h1e0692f_0
path.py                   10.5             py27he482d56_0
pathlib2                  2.3.0            py27h0ae272f_0
patsy                     0.5.0                    py27_0
pep8                      1.7.1                    py27_0
pickleshare               0.7.4            py27hb5f6335_0
pillow                    5.0.0            py27h901f87c_0
pip                       9.0.3                     <pip>
pip                       9.0.1            py27hdaa76b4_4
pkginfo                   1.4.1            py27h6ce81e3_1
pluggy                    0.6.0            py27h89dc50b_0
ply                       3.10             py27h5fb8a85_0
prompt_toolkit            1.0.15           py27h3a8ec6a_0
psutil                    5.4.3            py27h0c8e037_0
py                        1.5.2            py27ha24bda0_0
pycodestyle               2.3.1            py27h24cd5d9_0
pycosat                   0.6.3            py27hcd410c5_0
pycparser                 2.18             py27hb43d16c_1
pycrypto                  2.6.1            py27h0c8e037_7
pycurl                    7.43.0.1         py27hc64555f_0
pyflakes                  1.6.0            py27h34e7826_0
pygments                  2.2.0            py27ha50f84f_0
pyhmc                     0.1.2                np19py27_1    omnia
pylint                    1.8.2                    py27_0
pyodbc                    4.0.22           py27hc56fc5f_0
pyopenssl                 17.5.0           py27h59156d7_0
pyparsing                 2.2.0            py27hc7d9fa6_1
pyqt                      5.6.0            py27h224ed30_5
pysocks                   1.6.7            py27h59bdd1e_1
pytables                  3.1.1                np19py27_1
pytest                    3.3.2                    py27_0
python                    2.7.14              h8c3f1cb_23
python-dateutil           2.6.1            py27hbdcc174_1
pytz                      2017.3           py27hca431c1_0
pywin32                   222              py27h0c8e037_0
pywinpty                  0.5              py27hc56fc5f_1
pyyaml                    3.12             py27ha287073_1
pyzmq                     16.0.3           py27he883654_0
qt                        5.6.2            vc9hc26998b_12  [vc9]
qtawesome                 0.4.4            py27h766b13d_0
qtconsole                 4.3.1            py27h77d40ac_0
qtpy                      1.3.1            py27h1ff2b4b_0
requests                  2.18.4           py27h3159eba_1
rmsd                      1.2.6                     <pip>
rope                      0.10.7           py27hb65afb6_0
ruamel_yaml               0.15.35          py27h0c8e037_1
scandir                   1.6              py27h518bda0_0
scikit-image              0.11.3               np19py27_0
scikit-learn              0.16.1               np19py27_0
scipy                     0.16.0               np19py27_0
seaborn                   0.8.1            py27hab56d54_0
send2trash                1.4.2                    py27_0
setuptools                38.4.0                   py27_0
simplegeneric             0.8.1                    py27_2
singledispatch            3.4.0.3          py27h3f9d112_0
sip                       4.18.1           py27h5ec1c1a_2
six                       1.11.0           py27ha5e1701_1
snowballstemmer           1.2.1            py27h28d3bf7_0
sortedcollections         0.5.3            py27h21b938c_0
sortedcontainers          1.5.9                    py27_0
sphinx                    1.6.6                    py27_0
sphinxcontrib             1.0              py27h0e2fb95_1
sphinxcontrib-websupport  1.0.1            py27h0d0f901_1
spyder                    3.2.6                    py27_0
sqlalchemy                1.2.1            py27h0c8e037_0
sqlite                    3.22.0               h8b3e59e_0
ssl_match_hostname        3.5.0.1          py27hea8a0f4_2
statsmodels               0.6.1                np19py27_0
subprocess32              3.2.7            py27hcc576e2_0
sympy                     1.1.1            py27hde44fae_0
tblib                     1.3.2            py27h8ae915c_0
terminado                 0.8.1                    py27_1
testpath                  0.3.1            py27h1cd488d_0
tk                        8.6.7                h144d9c4_3
toolz                     0.9.0                    py27_0
tornado                   4.5.3                    py27_0
traitlets                 4.3.2            py27h1b1b3a5_0
typing                    3.6.2            py27h9197bb0_0
unicodecsv                0.14.1           py27h0bf7bb0_0
urllib3                   1.22             py27hb9f5a07_0
vc                        9                    h7299396_1
vs2008_runtime            9.00.30729.1         hfaea7d5_1
vs2015_runtime            14.0.25123                    3
wcwidth                   0.1.7            py27hb1a0d82_0
webencodings              0.5.1            py27h4e224a2_1
werkzeug                  0.14.1                   py27_0
wheel                     0.30.0           py27ha643586_1
widgetsnbextension        3.1.0                    py27_0
win_inet_pton             1.0.1            py27hf41312a_1
win_unicode_console       0.5              py27hc037021_0
wincertstore              0.2              py27hf04cefb_0
winpty                    0.4.3                         4
wrapt                     1.10.11          py27hcd2b27d_0
xlrd                      1.1.0            py27h2b87a7f_1
xlsxwriter                1.0.2            py27h5ed79b1_0
xlwings                   0.11.5                   py27_0
xlwt                      1.3.0            py27h2271735_0
yaml                      0.1.7                h3e6d941_2
zict                      0.1.3            py27h0171463_0
zlib                      1.2.11               hbc2faf4_2

(base) D:\programs>pip show msmbuilder
Name: msmbuilder
Version: 3.4.0
Summary: MSMBuilder: Statistical models for Biomolecular Dynamics
Home-page: https://github.com/msmbuilder/msmbuilder
Author: Robert McGibbon
Author-email: rmcgibbo@gmail.com
License: UNKNOWN
Location: d:\programs\anaconda\lib\site-packages
Requires:

Anyone have any ideas?
"
msmbuilder,msmbuilder,308609974,MacroAssignments_ and labels_ are NoneType object while using APM.,open,,https://github.com/msmbuilder/msmbuilder/issues/1073,"Hi,
I am trying to use APM to cluster my featurized trajectories, for a set of parameters, even though it seems to be working fine, the cluster object does not include any labels_ or MacroAssignments_. They are both empty (NoneType object). Do you have any suggestion for why this happens?
And also can you tell me a rule of thumb on how to choose the clustering parameters for APM, such as n_macrostates, max_iter, lag_time=5, and sub_clus?
Thanks,
Zahra"
msmbuilder,msmbuilder,307650262,Macrostate or microstate structure and Transition Pathways ?,open,,https://github.com/msmbuilder/msmbuilder/issues/1071,"I was recently trying to analyze a series of MD simulation trajectories using the MSMBuilder and I encountered the following problems: 

1) how can I obtain the representative structures of the microstates or macrostates of the MSM that I built ?
2) I read about the tutorial provided on the official web site of MSMBuilder and the final step produce a trajectory msm-traj.xtc and I wonder whether the frame that I extract from this trajectory can represent the corresponding microstate or macrostate structure ? 
3) another problem about the msm-traj.xtc trajectory, according to the tutorial, each frame in it corresponds to 1 lag-time unit, so is there a limit for the number of frame of this trajectory, for example, can I generate a msm-traj.xtc trajectory consisting of more than 1000 frames or so ?
4) One of the key part of MSM analysis is to find out the transition pathways between various microstates or macrostates. I can find the tpt-related codes such as the path.py but I cannot figure out how to use them and would you please give me a hand ?

Much thanks to your help, thank you!"
msmbuilder,msmbuilder,307253946,[question] how to distill intermediate structures from MSM ?,open,,https://github.com/msmbuilder/msmbuilder/issues/1070,"after I finishing constructing MSM model for my trajectories and system, how can I distill the structures of the corresponding microstate/macrostate ?

according to the protocol, a sample trajectory from the MSM is obtained through the following line:

python microstate-traj.py

but when I load the trajectory into VMD, it seems that the trajectory is not continuous and sometime the structures/frames just transform from one conformation to a very different one. I believe I strictly follow the protocol and I am wondering why the transition between two completely different conformations can be so fast and whether the trajectory obtained here can correctly reflect the dynamic process we hope to predict by MSM model ?

that's all, thank you !"
msmbuilder,msmbuilder,294432710,GaussianHMM parameter estimation runs indefinitely,open,,https://github.com/msmbuilder/msmbuilder/issues/1061,"I am able to generate an HMM in a few seconds whenever I have 1 million or fewer datapoints in my list of trajectories. If I have 2.7 million points it runs indefinitely, and if I have 1000001 points, it runs indefintely. The number of trajectories I use does not matter. The length of each trajectory does not matter. I only have 1 coordinate per trajectory, but there is an example (http://msmbuilder.org/3.1.0/examples/hmm-and-msm.html) where there are 10 coordinates per trajectory but total number of datapoints still is 1 million, and for that case HMM estimation completes in only a few seconds (I tried it in my computer). 

I am using version 3.8.0, and estimation step does not actually consume a lot of RAM even when it is running indefinitely."
msmbuilder,msmbuilder,277568298,Implement RobustScaler with trimmed means,open,,https://github.com/msmbuilder/msmbuilder/issues/1057,inspiration: https://garstats.wordpress.com/2017/11/28/trimmed-means/
msmbuilder,msmbuilder,273146383,No lumping in msmbuilder website?,open,docs,https://github.com/msmbuilder/msmbuilder/issues/1053,"I use the msmbuilder.org website quite a lot to quickly refer to the docs. However, I've seen that there is no information on the `msmbuilder.lumping` package. However I've seen that almost everything in it does have docstrings. Maybe it could be added as a new section?"
msmbuilder,msmbuilder,271996060,Change MDTraj required version to 1.9.,open,,https://github.com/msmbuilder/msmbuilder/pull/1052," - [ ] Implement feature / fix bug
 - [ ] Add tests
 - [ ] Update changelog

Update tests to require mdtraj v1.9. If test fail I'll try to address issues, if they pass I'll make remaining updates so this can be merged.

#1051 depends on this being merged first.
"
msmbuilder,msmbuilder,270186050,Implement inverse_transform when appropriate,open,,https://github.com/msmbuilder/msmbuilder/issues/1047,"e.g. scalers, dihedral featurizers, etc."
msmbuilder,msmbuilder,262219999,APM tests failing when random seed is added,open,tests,https://github.com/msmbuilder/msmbuilder/issues/1044,"pinging @liusong299

The success of APM tests seems heavily dependent on this [random_state](https://github.com/msmbuilder/msmbuilder/blob/master/msmbuilder/cluster/apm.py#L222) being set to zero. Please review this code, and provide a more reproducible test; otherwise, it will be deprecated in the next release."
msmbuilder,msmbuilder,256871359,bad magic number in libdistance when running develop,open,,https://github.com/msmbuilder/msmbuilder/issues/1040,"I'm getting this error:

```
ImportError: bad magic number in 'msmbuilder.libdistance': b'\x03\xf3\r\n'
```

when I run `python setup.py develop`, but not when I run `python setup.py install`."
msmbuilder,msmbuilder,256734315,Inconsistency Between msmb commands and save_trajs function,open,,https://github.com/msmbuilder/msmbuilder/issues/1039,"I think there is an inconsistency in the formatting of the filenames that are saved when using subcommands from the `msmb` command line functionality and the `save_trajs` function of the `msmbuilder.io` package directly from a Python script.

My guess is they cannot work together because the `msmb` commands are using the `dataset` class and expect the input numpy arrays saved in disk with eight-digit string filenames (so `00000000.npy`). 

On the other hand, when one is using `save_trajs`, the numpy arrays are stored in disk with no left padding of the numbers at all (just `0.npy`).

## Example
I have a dictionary of featurized trajs in python that I save:
```python
save_trajs(ftrajs, 'ftrajs', meta)
```
Giving the following `ftrajs` directory:
```
$ ls ftrajs/
0.npy	10.npy	12.npy	14.npy	16.npy	18.npy	2.npy	21.npy	23.npy	25.npy	27.npy	29.npy	30.npy	32.npy	34.npy	36.npy	38.npy	5.npy	7.npy	9.npy
1.npy	11.npy	13.npy	15.npy	17.npy	19.npy	20.npy	22.npy	24.npy	26.npy	28.npy	3.npy	31.npy	33.npy	35.npy	37.npy	4.npy	6.npy	8.npy
```
Now say I want to scale this data directly using: `msmb RobustScaler -i ftrajs/ -o scaler -t ftrajs_scaled`
```
IndexError: [Errno 2] No such file or directory: 'ftrajs/00000000.npy'
```

I think it's better to change the save_trajs function so that it also uses 8 digit representation for the filenames? 

Also, why do some `msmb` commands store the transformed data in a directory with `npy` arrays and a `PROVENANCE.txt` file (for example, `msmb DihedralFeaturizer`, while others store it in a single `.h5` file (e.g: `msmb RobustScaler`)? Is there a way to change this as a user? Or is it hard coded? (Sorry if there is, must've missed it).


 "
msmbuilder,msmbuilder,249687271,Implement Path Lumping Algorithm,open,,https://github.com/msmbuilder/msmbuilder/issues/1030,"From Xuhui's group http://aip.scitation.org/doi/abs/10.1063/1.4995558

(h/t @msultan)"
msmbuilder,msmbuilder,246659512,Shape Mismatch Error on MFPT(Errors=True),open,,https://github.com/msmbuilder/msmbuilder/issues/1025,"Might be an easy fix. Here's an example of failing code (modified from the existing mfpt tests).

```
import numpy as np
from msmbuilder import tpt
from msmbuilder.msm import MarkovStateModel

def test_mfpt_match2():
    assignments = np.random.randint(10, size=(10, 2000))
    msm = MarkovStateModel(lag_time=1)
    msm.fit(assignments)

    mfpts1 = tpt.mfpts(msm, errors=True)
    
test_mfpt_match2()
```
On the latest dev version of msmbuilder it returns:
```
<ipython-input-6-efa94d602e0d> in test_mfpt_match2()
     12     # these two do different things
     13     mfpts0 = np.vstack([tpt.mfpts(msm, i) for i in range(10)]).T
---> 14     mfpts1 = tpt.mfpts(msm, errors=True)
     15 
     16     npt.assert_array_almost_equal(mfpts0, mfpts1)

/Users/cing/anaconda/lib/python3.5/site-packages/msmbuilder-3.9.0.dev0-py3.5-macosx-10.6-x86_64.egg/msmbuilder/tpt/mfpt.py in mfpts(msm, sinks, lag_time, errors, n_samples)
    113             tprob = perturb_tmat(loc, scale)
    114             populations = _solve_msm_eigensystem(tprob, 1)[1]
--> 115             output.append(_mfpts(tprob, populations, sinks, lag_time))
    116         return np.array(output)
    117     return _mfpts(msm.transmat_, msm.populations_, sinks, lag_time)

/Users/cing/anaconda/lib/python3.5/site-packages/msmbuilder-3.9.0.dev0-py3.5-macosx-10.6-x86_64.egg/msmbuilder/tpt/mfpt.py in _mfpts(tprob, populations, sinks, lag_time)
    174         # Fundamental matrix
    175         fund_matrix = scipy.linalg.inv(np.eye(n_states) - tprob +
--> 176                                        limiting_matrix)
    177 
    178         # mfpt[i,j] = (fund_matrix[j,j] - fund_matrix[i,j]) / populations[j]

ValueError: operands could not be broadcast together with shapes (10,10) (100,1) 
```"
msmbuilder,msmbuilder,239813194,Question about HierarchyParser,open,,https://github.com/msmbuilder/msmbuilder/issues/1020,"Hi,

I've got a results directory with the following structure:

```
S1P/
├── [ 714]  run01
│   ├── [ 102]  0350-0400
│   ├── [ 102]  0400-0450
│   ├── [ 102]  0450-0500
│   ├── [ 102]  0500-0550
│   ├── [ 102]  0550-0600
│   ├── [ 20M]  05_G159DS1P_newEGCG_0000-0050ns_run01.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0050-0100ns_run01.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0100-0150ns_run01.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0150-0200ns_run01.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0200-0250ns_run01.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0250-0300ns_run01.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0300-0350ns_run01.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0350-0400ns_run01.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0400-0450ns_run01.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0450-0500ns_run01.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0500-0550ns_run01.nc
│   └── [ 20M]  05_G159DS1P_newEGCG_0550-0600ns_run01.nc
├── [ 748]  run02
│   ├── [ 102]  0400-0450
│   ├── [ 102]  0450-0500
│   ├── [ 102]  0500-0550
│   ├── [ 102]  0550-0600
│   ├── [ 20M]  05_G159DS1P_newEGCG_0000-0050ns_run02.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0050-0100ns_run02.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0100-0150ns_run02.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0150-0200ns_run02.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0200-0250ns_run02.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0250-0300ns_run02.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0300-0350ns_run02.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0350-0400ns_run02.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0400-0450ns_run02.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0450-0500ns_run02.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0500-0550ns_run02.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0550-0600ns_run02.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0600-0650ns_run02.nc
│   └── [ 102]  0600-0650
├── [ 748]  run03
│   ├── [ 102]  0400-0450
│   ├── [ 102]  0450-0500
│   ├── [ 102]  0500-0550
│   ├── [ 102]  0550-0600
│   ├── [ 20M]  05_G159DS1P_newEGCG_0000-0050ns_run03.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0050-0100ns_run03.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0100-0150ns_run03.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0150-0200ns_run03.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0200-0250ns_run03.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0250-0300ns_run03.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0300-0350ns_run03.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0350-0400ns_run03.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0400-0450ns_run03.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0450-0500ns_run03.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0500-0550ns_run03.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0550-0600ns_run03.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0600-0650ns_run03.nc
│   └── [ 102]  0600-0650
├── [ 646]  run04
│   ├── [ 102]  0350-0400
│   ├── [ 102]  0400-0450
│   ├── [ 102]  0450-0500
│   ├── [ 102]  0500-0550
│   ├── [ 20M]  05_G159DS1P_newEGCG_0000-0050ns_run04.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0050-0100ns_run04.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0100-0150ns_run04.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0150-0200ns_run04.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0200-0250ns_run04.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0250-0300ns_run04.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0300-0350ns_run04.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0350-0400ns_run04.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0400-0450ns_run04.nc
│   ├── [ 20M]  05_G159DS1P_newEGCG_0450-0500ns_run04.nc
│   └── [ 20M]  05_G159DS1P_newEGCG_0500-0550ns_run04.nc
└── [2.8M]  stripped.prmtop
SEP
├── [ 238]  run01
│   ├── [ 102]  0100-0150
│   ├── [ 20M]  05_G159DSEP_newEGCG_0000-0050ns_run01.nc
│   ├── [ 20M]  05_G159DSEP_newEGCG_0050-0100ns_run01.nc
│   └── [ 20M]  05_G159DSEP_newEGCG_0100-0150ns_run01.nc
├── [ 238]  run02
│   ├── [ 102]  0100-0150
│   ├── [ 20M]  05_G159DSEP_newEGCG_0000-0050ns_run02.nc
│   ├── [ 20M]  05_G159DSEP_newEGCG_0050-0100ns_run02.nc
│   └── [ 20M]  05_G159DSEP_newEGCG_0100-0150ns_run02.nc
├── [ 238]  run03
│   ├── [ 102]  0100-0150
│   ├── [ 20M]  05_G159DSEP_newEGCG_0000-0050ns_run03.nc
│   ├── [ 20M]  05_G159DSEP_newEGCG_0050-0100ns_run03.nc
│   └── [ 20M]  05_G159DSEP_newEGCG_0100-0150ns_run03.nc
├── [ 238]  run04
│   ├── [ 102]  0100-0150
│   ├── [ 20M]  05_G159DSEP_newEGCG_0000-0050ns_run04.nc
│   ├── [ 20M]  05_G159DSEP_newEGCG_0050-0100ns_run04.nc
│   └── [ 20M]  05_G159DSEP_newEGCG_0100-0150ns_run04.nc
└── [2.8M]  stripped.prmtop

31 directories, 63 files
```


Both the top folders `S1P` and `SEP` are really the same system, with a slight change in a couple of amino acids (so the `prmtops` won't exactly match).

Can I build directly a metadata dataframe using `HierarchyParser`? At the moment, I've managed to do the following:

```python
s1p_p = HierarchyParser(levels=['ptype', 'run'], step_ps=200, top_fn='S1P/stripped.prmtop')
sep_p = HierarchyParser(levels=['ptype', 'run'], step_ps=200, top_fn='SEP/stripped.prmtop')

s1p = msmbuilder.io.gather_metadata('S1P/run*/05*nc', s1p_p)
sep = msmbuilder.io.gather_metadata('SEP/run*/05*nc', sep_p)

meta= pd.concat([s1p, sep])
```
and it works (_I think_)

I have a couple of questions:

- Can I just build one parser, specyfing that there are two different topologies? Maybe passing a list of topologies to `top_fn`. 
- I think the parser is interpreting each individual chunk of trajectory as a single one. Is it possible to indicate that they all belong to the same trajectory?

Thanks for any suggestions! "
msmbuilder,msmbuilder,231717546,Switch travis CI to depend on conda-forge,open,,https://github.com/msmbuilder/msmbuilder/issues/1017,Omnia channel is being deprecated
msmbuilder,msmbuilder,230004961,retire the msmbuilder email list? ,open,,https://github.com/msmbuilder/msmbuilder/issues/1016,How do the @msmbuilder/developers feel about retiring the email list? I think the github issue tracker is probably a better way since the issues are indexed by google and easier to find/search through. 
msmbuilder,msmbuilder,227848486,sample_msm: api change?,open,,https://github.com/msmbuilder/msmbuilder/issues/1014,"I'm trying to draw samples from an msm generated with msmbuilder, but I run into an error with the command shown in `microstate-traj.py`:

```python
inds = sample_msm(ttrajs, kmeans.cluster_centers_, msm, n_steps=200, stride=1)
```

```python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-114-8acba2b205e6> in <module>()
      1 import msmbuilder.utils
      2 
----> 3 inds = sample_msm(tica_trajs, clusterer.cluster_centers_, msm, n_steps=200, stride=1)

/ul/saladi/anaconda3/lib/python3.5/site-packages/msmbuilder/io/sampling/sampling.py in sample_msm(trajs, state_centers, msm, n_steps, state, stride, random_state)
     75 def sample_msm(trajs, state_centers, msm, n_steps, state=None,
     76                stride=1, random_state=None):
---> 77     fixed_indices = list(trajs.keys())
     78     trajs = [trajs[k] for k in fixed_indices]
     79 

AttributeError: 'list' object has no attribute 'keys'
```

Should I be doing this a different way?"
msmbuilder,msmbuilder,225977983,GMRQ test score error,open,,https://github.com/msmbuilder/msmbuilder/issues/1012,"Hi,

I'm using Pipeline and ShuffleSplit with msmbuilder3.7.0 to perform cross validation of different parameters. I'm iterating through the different parameters and printing the gmrq score of each ShuffleSplit fold to a results list. However, my code keeps breaking after a random number of iterations at the point of the test score function. 

My code is:

```
msm_pipe = Pipeline([
    ('tica', msm_decomp.tICA()),
    ('cluster', msm_clust.MiniBatchKMeans()),
    ('msm', msm_msm.MarkovStateModel(n_timescales=3))
])

from sklearn.model_selection import ShuffleSplit
ss = ShuffleSplit(n_splits=10, test_size=0.5)
n_components = [5,10,15,20]
tica_lags = [20, 40, 80, 160, 320]
n_clusters = [25, 50, 100, 200, 400, 800]
msm_lags = [5, 10, 20, 40, 80, 160, 320]
results = []
for tica_lag in tica_lags:
    for n_component in n_components:
        for n_cluster in n_clusters:
            for msm_lag in msm_lags:
                fold = 0
                msm_pipe.set_params(tica__lag_time=tica_lag, tica__n_components=n_component, cluster__n_clusters=n_cluster, msm__lag_time=msm_lag)
                for train_index, test_index in ss.split(back_trajs):
                    train_data = [back_trajs[i] for i in train_index]
                    test_data = [back_trajs[i] for i in test_index]
                    msm_pipe.fit(train_data)
                    train_score = msm_pipe.score(train_data)
                    test_score = msm_pipe.score(test_data)
                    tica_eigenvals = msm_pipe.named_steps['tica'].eigenvalues_
                    tica_timescales = msm_pipe.named_steps['tica'].timescales_
                    msm_eigenvals = msm_pipe.named_steps['msm'].eigenvalues_
                    msm_timescales = msm_pipe.named_steps['msm'].timescales_
                    results.append({
                        'train_score': train_score,
                        'test_score': test_score,
                        'fold': fold,
                        'tica_lag': tica_lag,
                        'n_components': n_component,
                        'n_clusters': n_cluster,
                        'msm_lags': msm_lag,
                        'tica_eigenvals': tica_eigenvals,
                        'tica_timescales': tica_timescales,
                        'msm_eigenvals': msm_eigenvals,
                        'msm_timescales': msm_timescales
                    })
                    fold += 1

```


and the error I receive is:



```

ValueError                                Traceback (most recent call last)
<ipython-input-26-6ff0a5e833e0> in <module>()
     17                     msm_pipe.fit(train_data)
     18                     train_score = msm_pipe.score(train_data)
---> 19                     test_score = msm_pipe.score(test_data)
     20                     tica_eigenvals = msm_pipe.named_steps['tica'].eigenvalues_
     21                     tica_timescales = msm_pipe.named_steps['tica'].timescales_

/home/lmr1u16/bin/anaconda2/lib/python2.7/site-packages/sklearn/utils/metaestimators.pyc in <lambda>(*args, **kwargs)
     52 
     53         # lambda, but not partial, allows help() to work with update_wrapper
---> 54         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
     55         # update the docstring of the returned function
     56         update_wrapper(out, self.fn)

/home/lmr1u16/bin/anaconda2/lib/python2.7/site-packages/sklearn/pipeline.pyc in score(self, X, y)
    503             if transform is not None:
    504                 Xt = transform.transform(Xt)
--> 505         return self.steps[-1][-1].score(Xt, y)
    506 
    507     @property

/home/lmr1u16/bin/anaconda2/lib/python2.7/site-packages/msmbuilder/msm/msm.pyc in score(self, sequences, y)
    423         # regularization parameters?
    424         m2 = self.__class__(**self.get_params())
--> 425         m2.fit(sequences)
    426 
    427         if self.mapping_ != m2.mapping_:

/home/lmr1u16/bin/anaconda2/lib/python2.7/site-packages/msmbuilder/msm/msm.pyc in fit(self, sequences, y)
    171             fit_method = fit_method_map[str(self.reversible_type).lower()]
    172             # step 3. estimate transition matrix
--> 173             self.transmat_, self.populations_ = fit_method(self.countsmat_)
    174         except KeyError:
    175             raise ValueError('reversible_type must be one of %s: %s' % (

/home/lmr1u16/bin/anaconda2/lib/python2.7/site-packages/msmbuilder/msm/msm.pyc in _fit_mle(self, counts)
    185 
    186         transmat, populations = _transmat_mle_prinz(
--> 187             counts + self.prior_counts)
    188         return transmat, populations
    189 

msmbuilder/msm/_markovstatemodel.pyx in msmbuilder.msm._markovstatemodel._transmat_mle_prinz (msmbuilder/msm/_markovstatemodel.c:2367)()

ValueError:  Error code=-2

```


I have seen a discussion on a gmrq error caused by the lack of overlap in training data and test data msm states but my error appears to be different and, when I re-fit the model with the same training data and test with the same test data from the point of breakage in the loop, the test score function appears to work fine.

Can anyone suggest where I'm going wrong or how I can overcome this problem?

Thanks,

Lauren"
msmbuilder,msmbuilder,224665065,Version 3.9 release schedule,open,,https://github.com/msmbuilder/msmbuilder/issues/1002,Target: end of June
msmbuilder,msmbuilder,224236312,Different tICA results from different versions of msmbuilder,open,,https://github.com/msmbuilder/msmbuilder/issues/999,"Hi,
Please check out the attachment.

I did these tICA analyses on the same system but different versions of msmbuilders (3.0 and 3.7); However, it showed a ""shifted"" folding behaviors.  Are there any particular reasons to cause these differences?  

Thanks a lot for helping.

[comparison_btw_two_version_msmbuilder.pdf](https://github.com/msmbuilder/msmbuilder/files/956180/comparison_btw_two_version_msmbuilder.pdf)
--Hongbin "
msmbuilder,msmbuilder,223521670,docs vs. example notebooks vs. tutorial: make these things consistent,open,,https://github.com/msmbuilder/msmbuilder/issues/997,"e.g. (implicitly) recommended practices for implied timescales are different in the [example notebooks](https://github.com/msmbuilder/msmbuilder/blob/master/examples/advanced/implied-timescales.ipynb), and the [tutorial](https://github.com/msmbuilder/msmbuilder/blob/master/msmbuilder/project_templates/msm/timescales-plot.py). I think we should probably update the old/""advanced"" example notebooks."
msmbuilder,msmbuilder,222438711,Errors from timescales calculations,open,,https://github.com/msmbuilder/msmbuilder/issues/995,"Hi,

I am trying to calculate the implied timescales on my systems, however, it pops up these errors that I had never seen before.  Could you help me to interpret these errors? Why that happens?

Thanks a lot.

--Hongbin

    msm_timescales = implied_timescales(sequences, lag_times, n_timescales=n_timescales, msm=MarkovStateModel(verbose=False))
  File ""/anaconda/lib/python2.7/site-packages/msmbuilder/msm/implied_timescales.py"", line 48, in implied_timescales
    verbose=verbose)
  File ""/anaconda/lib/python2.7/site-packages/msmbuilder/utils/param_sweep.py"", line 45, in param_sweep
    delayed(_param_sweep_helper)(args) for args in iter_args)
  File ""/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 659, in __call__
    self.dispatch(function, args, kwargs)
  File ""/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 406, in dispatch
    job = ImmediateApply(func, args, kwargs)
  File ""/home/hongbin/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py"", line 140, in __init__
    self.results = func(*args, **kwargs)
  File ""/home/hongbin/anaconda/lib/python2.7/site-packages/msmbuilder/utils/param_sweep.py"", line 55, in _param_sweep_helper
    model.fit(sequences)
  File ""/home/hongbin/anaconda/lib/python2.7/site-packages/msmbuilder/msm/msm.py"", line 203, in fit
    self.transmat_, self.populations_ = fit_method(self.countsmat_)
  File ""/home/hongbin/anaconda/lib/python2.7/site-packages/msmbuilder/msm/msm.py"", line 217, in _fit_mle
    counts + self.prior_counts)
  File ""msmbuilder/msm/_markovstatemodel.pyx"", line 58, in msmbuilder.msm._markovstatemodel._transmat_mle_prinz (msmbuilder/msm/_markovstatemodel.c:1956)
ValueError:  Error code=-2
"
msmbuilder,msmbuilder,221120268,Implement Bayesian HMMs,open,,https://github.com/msmbuilder/msmbuilder/issues/992,"As seen in 

- https://en.wikipedia.org/wiki/Hidden_Markov_model#Examples
- http://emma-project.org/latest/api/generated/pyemma.msm.bayesian_hidden_markov_model.html#pyemma.msm.bayesian_hidden_markov_model"
msmbuilder,msmbuilder,221119905,Add lag_time parameter to HMMs,open,,https://github.com/msmbuilder/msmbuilder/issues/991,Would be nice to have more consistency between the MSM and HMM classes.
msmbuilder,msmbuilder,219416479,fs-peptide folding gif from tutorial,open,question,https://github.com/msmbuilder/msmbuilder/issues/983,"I received this email

>  I have a brief question about using MSM Builder, specifically on creating a trajectory of the 1st tICA coordinate. In the the online tutorial, a gif is shown of the fs peptide folding and I was wondering how you generate the trajectory. I tried sampling along the 1st tICA coordinate but my videos are a bit jumpy. Thank you!

Wanted to cross-post it here so future googlers may find help
"
msmbuilder,msmbuilder,218372881,Improve HMM docstring,open,docs,https://github.com/msmbuilder/msmbuilder/issues/981,A lot of defaults are not stated....
msmbuilder,msmbuilder,217450118,revisit commute mapping NaNs?,open,,https://github.com/msmbuilder/msmbuilder/issues/977,Commute mapping generates NaNs when the timescale of the process is shorter than the tICA lag time. See #954. I mapped the NaNs to zeros in my original fix (lower bound on the timescale) but I now agree with @msultan and I think that they should actually be mapped to the lag time instead (upper bound on the timescale). Thoughts?
msmbuilder,msmbuilder,210275135,Better error detections in trajectories,open,,https://github.com/msmbuilder/msmbuilder/issues/970,"So this is based off my experiences earlier today working with a large dataset. Basically, I was trying to run tICA on a set of contacts and kept getting e-15 eigenvalues with non-sensical models. After much digging which included individually loading feature files and fitting data, I eventually found a **single** frame which had failed to image properly and caused the downstream bugs. 

Anyone got any suggestions on what we can add to ```msmb ``` to automate this? "
msmbuilder,msmbuilder,208563539,pandas error in plotting,open,,https://github.com/msmbuilder/msmbuilder/issues/968,"I am attempting to run through the [tutorial ](http://msmbuilder.org/3.7.0/tutorial.html) to ensure that MSMBuilder is working as expected. However, I have run into an error that I have been unable to resolve. I am working in linux and the first 5 steps in the tutorial executed as expected. When I tried to produce the metadata plot (paths were simplified with ""...""):

```
$ python gather-metadata-plot.py
Traceback (most recent call last):
  File ""gather-metadata-plot.py"", line 68, in <module>
    plot_lengths(ax)
  File ""gather-metadata-plot.py"", line 26, in plot_lengths
    ax.hist(lengths_ns)
  File ""/.../python/2.7.10/lib/python2.7/site-packages/matplotlib/axes/_axes.py"", line 5604, in hist
    if isinstance(x, np.ndarray) or not iterable(x[0]):
  File ""/.../python/2.7.10/lib/python2.7/site-packages/pandas/core/series.py"", line 583, in __getitem__
    result = self.index.get_value(self, key)
  File ""/.../python/2.7.10/lib/python2.7/site-packages/pandas/indexes/base.py"", line 1980, in get_value
    tz=getattr(series.dtype, 'tz', None))
  File ""pandas/index.pyx"", line 103, in pandas.index.IndexEngine.get_value (pandas/index.c:3332)
  File ""pandas/index.pyx"", line 111, in pandas.index.IndexEngine.get_value (pandas/index.c:3035)
  File ""pandas/index.pyx"", line 159, in pandas.index.IndexEngine.get_loc (pandas/index.c:4018)
  File ""pandas/hashtable.pyx"", line 303, in pandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:6610)
  File ""pandas/hashtable.pyx"", line 309, in pandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:6554)
KeyError: 0
```

Any direction on pandas version or other requirements would be appreciated.

Sincerely, "
msmbuilder,msmbuilder,204492908,describe_features shows odd behavior for FsPeptide topology,open,bug,https://github.com/msmbuilder/msmbuilder/issues/965,"When I use `DihedralFeaturizer` on the `FsPeptide` dataset, some features seem to have incorrect descriptions:

```python
>> DihedralFeaturizer().describe_features(trajectories[0])[77]
{'atominds': array([174, 176, 182, 184]),
 'featuregroup': 'psi',
 'featurizer': 'Dihedral',
 'otherinfo': 'cos',
 'resids': [16, 15],
 'resnames': ['ALA'],
 'resseqs': [16, 17]}
```

notice that the order of `resids` is wrong and `resnames` only has one residue name."
msmbuilder,msmbuilder,201998003,Added DSSPFeaturizer,open,tests,https://github.com/msmbuilder/msmbuilder/pull/957," - [x] Implement feature / fix bug
 - [ ] Add tests
 - [ ] Update changelog


"
msmbuilder,msmbuilder,194895691,tICA default update,open,,https://github.com/msmbuilder/msmbuilder/issues/952,Change the default tICA options to use all tICs and kinetic mapping
msmbuilder,msmbuilder,187626532,Permissions,open,,https://github.com/msmbuilder/msmbuilder/issues/937,"@msmbuilder/developers I guess you guys can't merge pull requests when the tests are failing. I can turn that check off, but I'd rather make the tests be able to pass. 

I think I added all you guys to have appveyor permissions (for restarting builds) and you should be able to configure travis stuff as well. Let me know if this isn't working"
msmbuilder,msmbuilder,187233209,"""MacroAssignments_""  does not have Macro state labels in APM clustering",open,,https://github.com/msmbuilder/msmbuilder/issues/934,"Hi,
I want to use APM clustering for my data (some distance metrics), but I was wondering first why should all trajectories be at the exact same length?
And second, why do both ""MacroAssignments_"" and ""labels_"" give the exact same thing? Both give assignments  in microstates space, while  ""MacroAssignments_"" supposed to be in Macrostates space.
Even I checked it with some randomly generated data on given on msm apm-test part, but I got the same result (exact same ""MacroAssignments_"" and ""labels_"").

Moreover, I was wondering if it means do_lamping function doesn't work well in APM, it would affect the whole process of APM clustering and its purpose. 

Thanks,
ZSh
"
msmbuilder,msmbuilder,184306560,Fix sklearn 0.18 failing test,open,tests,https://github.com/msmbuilder/msmbuilder/issues/926,"Can someone try to debug the test failure in sklearn 0.18

```
======================================================================
ERROR: msmbuilder.tests.test_featurizer_subset.test_that_all_featurizers_run
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/travis/miniconda3/conda-bld/conda-recipe_1476298884112/_t_env/lib/python3.5/site-packages/nose/case.py"", line 198, in runTest
    self.test(*self.arg)
  File ""/home/travis/miniconda3/conda-bld/conda-recipe_1476298884112/_t_env/lib/python3.5/site-packages/msmbuilder-3.7.0.dev0-py3.5-linux-x86_64.egg/msmbuilder/tests/test_featurizer_subset.py"", line 77, in test_that_all_featurizers_run
    X_all = featurizer.transform(trajectories)
  File ""/home/travis/miniconda3/conda-bld/conda-recipe_1476298884112/_t_env/lib/python3.5/site-packages/msmbuilder-3.7.0.dev0-py3.5-linux-x86_64.egg/msmbuilder/featurizer/featurizer.py"", line 1354, in transform
    for name, trans in self.transformer_list)
  File ""/home/travis/miniconda3/conda-bld/conda-recipe_1476298884112/_t_env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 758, in __call__
    while self.dispatch_one_batch(iterator):
  File ""/home/travis/miniconda3/conda-bld/conda-recipe_1476298884112/_t_env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 608, in dispatch_one_batch
    self._dispatch(tasks)
  File ""/home/travis/miniconda3/conda-bld/conda-recipe_1476298884112/_t_env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 571, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File ""/home/travis/miniconda3/conda-bld/conda-recipe_1476298884112/_t_env/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 109, in apply_async
    result = ImmediateResult(func)
  File ""/home/travis/miniconda3/conda-bld/conda-recipe_1476298884112/_t_env/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 322, in __init__
    self.results = batch()
  File ""/home/travis/miniconda3/conda-bld/conda-recipe_1476298884112/_t_env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/home/travis/miniconda3/conda-bld/conda-recipe_1476298884112/_t_env/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in <listcomp>
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/home/travis/miniconda3/conda-bld/conda-recipe_1476298884112/_t_env/lib/python3.5/site-packages/sklearn/pipeline.py"", line 570, in _transform_one
    res = transformer.transform(X)
  File ""/home/travis/miniconda3/conda-bld/conda-recipe_1476298884112/_t_env/lib/python3.5/site-packages/msmbuilder-3.7.0.dev0-py3.5-linux-x86_64.egg/msmbuilder/featurizer/featurizer.py"", line 159, in transform
    return [self.partial_transform(traj) for traj in traj_list]
TypeError: 'NoneType' object is not iterable
```

See also #918 
"
msmbuilder,msmbuilder,183827531,symlink problems,open,,https://github.com/msmbuilder/msmbuilder/issues/923,"Dear all,
I installed msmb as it's written in tutorial and tried to explore example, but faced with problem attached. I checked symlinks, they are not broken (when I press on top.pdb it opens right structure in DSViewer). I tried to change symlinks and wrote the whole way to files but obtained the same error... What have I to do to fix problem? Thank you so much

I use Windows 7 if it does matter

```
(O:\Program Files\miniconda) O:\phd\mgf\test_msmb\analysis>python gather-metadat
a.py
Traceback (most recent call last):
  File ""O:\Program Files\miniconda\lib\site-packages\pandas\indexes\base.py"", li
ne 1945, in get_loc
    return self._engine.get_loc(key)
  File ""pandas\index.pyx"", line 137, in pandas.index.IndexEngine.get_loc (pandas
\index.c:4154)
  File ""pandas\index.pyx"", line 159, in pandas.index.IndexEngine.get_loc (pandas
\index.c:4018)
  File ""pandas\hashtable.pyx"", line 675, in pandas.hashtable.PyObjectHashTable.g
et_item (pandas\hashtable.c:12368)
  File ""pandas\hashtable.pyx"", line 683, in pandas.hashtable.PyObjectHashTable.g
et_item (pandas\hashtable.c:12322)
KeyError: 'run'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""gather-metadata.py"", line 18, in <module>
    meta = gather_metadata(""trajs/*.xtc"", parser)
  File ""O:\Program Files\miniconda\lib\site-packages\msmbuilder\io\gather_metada
ta.py"", line 204, in gather_metadata
    return meta.set_index(parser.index).sort_index()
  File ""O:\Program Files\miniconda\lib\site-packages\pandas\core\frame.py"", line
 2837, in set_index
    level = frame[col]._values
  File ""O:\Program Files\miniconda\lib\site-packages\pandas\core\frame.py"", line
 1997, in __getitem__
    return self._getitem_column(key)
  File ""O:\Program Files\miniconda\lib\site-packages\pandas\core\frame.py"", line
 2004, in _getitem_column
    return self._get_item_cache(key)
  File ""O:\Program Files\miniconda\lib\site-packages\pandas\core\generic.py"", li
ne 1350, in _get_item_cache
    values = self._data.get(item)
  File ""O:\Program Files\miniconda\lib\site-packages\pandas\core\internals.py"",
line 3290, in get
    loc = self.items.get_loc(item)
  File ""O:\Program Files\miniconda\lib\site-packages\pandas\indexes\base.py"", li
ne 1947, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File ""pandas\index.pyx"", line 137, in pandas.index.IndexEngine.get_loc (pandas
\index.c:4154)
  File ""pandas\index.pyx"", line 159, in pandas.index.IndexEngine.get_loc (pandas
\index.c:4018)
  File ""pandas\hashtable.pyx"", line 675, in pandas.hashtable.PyObjectHashTable.g
et_item (pandas\hashtable.c:12368)
  File ""pandas\hashtable.pyx"", line 683, in pandas.hashtable.PyObjectHashTable.g
et_item (pandas\hashtable.c:12322)
KeyError: 'run'
```
"
msmbuilder,msmbuilder,182512801,SparseTICA pseudoeigenvalues,open,,https://github.com/msmbuilder/msmbuilder/issues/917,"The stored pseudoeigenvalues in SparseTICA are different from those computed for GMRQ scores, so `stica.score_ != stica.score(X_train)`.

I think this is because `stica.eigenvalues_` [are computed using deflated versions of the problem](https://github.com/msmbuilder/msmbuilder/blob/3.6.0/msmbuilder/decomposition/sparsetica.py#L149-L155), and deflating with approximate eigenvectors introduces some error.
"
msmbuilder,msmbuilder,180613570,Spectral Clustering error,open,question,https://github.com/msmbuilder/msmbuilder/issues/914,"Hello,
When using SpectralClustering to cluster my data I'm running into an error. Other clusterers seem to work fine. The error is:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-25-860e0b0ab1c9> in <module>()
      2 clusterer = SpectralClustering(n_clusters=100)
      3 clustered_trajs = tica_trajs.fit_transform_with(
----> 4     clusterer, 'spectrals3/', fmt='dir-npy'
      5 )
      6 

/Users/dvr/anaconda/lib/python3.5/site-packages/msmbuilder/dataset.py in fit_transform_with(self, estimator, out_ds, fat)
    234         """"""
    235         self.fit_with(estimator)
--> 236         return self.transform_with(estimator, out_ds, fmt=fmt)
    237 
    238     @property

/Users/dvr/anaconda/lib/python3.5/site-packages/msmbuilder/dataset.py in transform_with(self, estimator, out_ds, fat)
    201             raise ValueError(err)
    202         for key in self.keys():
--> 203             out_ds[key] = estimator.partial_transform(self[key])
    204         return out_ds
    205 

/Users/dvr/anaconda/lib/python3.5/site-packages/msmbuilder/cluster/base.py in partial_transform(self, X)
    167     def partial_transform(self, X):
    168         """"""Alias for partial_predict""""""
--> 169         return self.partial_predict(X)
    170 
    171     def fit_transform(self, sequences, y=None):

/Users/dvr/anaconda/lib/python3.5/site-packages/msmbuilder/cluster/base.py in partial_predict(self, X, y)
    133         if isinstance(X, md.Trajectory):
    134             X.center_coordinates()
--> 135         return super(MultiSequenceClusterMixin, self).predict(X)
    136 
    137     def fit_predict(self, sequences, y=None):

AttributeError: 'super' object has no attribute 'predict'
```

Thanks in advance,

Dries
"
msmbuilder,msmbuilder,178125740,support conda-forge,open,,https://github.com/msmbuilder/msmbuilder/issues/911,"We should build packages on conda-forge
"
msmbuilder,msmbuilder,177664727,Multisequence support,open,,https://github.com/msmbuilder/msmbuilder/issues/909,"I think having ability to juggle multiple sequences within msmbuilder would be awesome. I have been writing some code to that effect in one of my [repos](https://github.com/msultan/kinase_msm) though it would nice to move that into the main msmb pipeline. 
"
msmbuilder,msmbuilder,177083378,fast kmeans,open,,https://github.com/msmbuilder/msmbuilder/issues/904,"could be interesting to add this to msmb: https://github.com/idiap/eakmeans
"
msmbuilder,msmbuilder,167697558,Link examples <--> api reference,open,docs,https://github.com/msmbuilder/msmbuilder/issues/866,"Each api reference page in the docs (e.g. featurization) should have one or many examples to go with it, and they should be linked.

Each example should link to the relevant api reference page
"
msmbuilder,msmbuilder,164622134,Pledge to drop support for python 2,open,question,https://github.com/msmbuilder/msmbuilder/issues/852,"http://python3statement.github.io/

We should sign this pledge!
"
msmbuilder,msmbuilder,164136363,Implement Median Absolute Deviation (MAD) scaling,open,,https://github.com/msmbuilder/msmbuilder/issues/849,"see https://en.wikipedia.org/wiki/Robust_measures_of_scale#IQR_and_MAD

We can make this an online algorithm fairly easily as shown in this [post](http://stackoverflow.com/questions/1058813/on-line-iterator-algorithms-for-estimating-statistical-median-mode-skewnes)
"
msmbuilder,msmbuilder,162802251,Use canonical links for docs,open,docs,https://github.com/msmbuilder/msmbuilder/issues/843,"https://en.wikipedia.org/wiki/Canonical_link_element

In addition to `msmbuilder.org/3.5/msm.html` have `msmbuilder.org/latest/msm.html` and use the canonical tag to point to that one
"
msmbuilder,msmbuilder,162793653,Trajectory finder module.,open,,https://github.com/msmbuilder/msmbuilder/issues/841,"One of the things that i have been struggling with in everyday work is identifying cross over trajectories that go from one region of phase space to another. Ideally i would like to identify a state or a region in tic space and have something in msmb tell me which of my underlying trajectories get closest in crossing over. I could imagine this being useful for visualization, figure making and perhaps even reseeding projects. 

I have done this in various ways but i am not sure what the right way to tackle this would be. 
"
msmbuilder,msmbuilder,162784138,Transition towards using Pandas DataFrames?,open,question,https://github.com/msmbuilder/msmbuilder/issues/840,"I wanted to gauge what the pros and cons are for using `pandas.DataFrame` objects as attributes rather than `numpy.ndarrays`.

Personally, I think the switch would be useful to keep track of units, striding, state labels, etc., which would help for later analysis and plotting; however, there would also be a higher barrier to entry for new msmb users who aren't already familiar with `pandas`.

cc @msultan @mpharrigan @brookehus @rmcgibbo @jchodera
"
msmbuilder,msmbuilder,160295904,PCCA+ doesn't work with continuous MSMs,open,bug,https://github.com/msmbuilder/msmbuilder/issues/827,"Where `msm_mdl` is a continuous time MSM, upon running `pcca = PCCAPlus.from_msm(msm_mdl, 3)`, I get:

```
TypeError                                 Traceback (most recent call last)
<ipython-input-17-2f6b108794f0> in <module>()
      1 from msmbuilder.lumping import PCCAPlus
----> 2 pcca = PCCAPlus.from_msm(msm_mdl, 3)

/home/bhusic/miniconda3/envs/bunnies/lib/python3.5/site-packages/msmbuilder/lumping/pcca.py in from_msm(cls, msm, n_macrostates)
    115         """"""
    116         params = msm.get_params()
--> 117         lumper = cls(n_macrostates, **params)
    118 
    119         lumper.transmat_ = msm.transmat_

/home/bhusic/miniconda3/envs/bunnies/lib/python3.5/site-packages/msmbuilder/lumping/pcca_plus.py in __init__(self, n_macrostates, do_minimization, objective_function, **kwargs)
     94                  objective_function='crisp_metastability', **kwargs):
     95 
---> 96         super(PCCAPlus, self).__init__(n_macrostates, **kwargs)
     97         obj_functions = dict(
     98                 crispness=crispness,

/home/bhusic/miniconda3/envs/bunnies/lib/python3.5/site-packages/msmbuilder/lumping/pcca.py in __init__(self, n_macrostates, pcca_tolerance, **kwargs)
     30         self.n_macrostates = n_macrostates
     31         self.pcca_tolerance = pcca_tolerance
---> 32         super(PCCA, self).__init__(**kwargs)
     33 
     34     def fit(self, sequences, y=None):

TypeError: __init__() got an unexpected keyword argument 'guess'
```
"
msmbuilder,msmbuilder,160072108,msm.core._transition_counts large memory usage,open,,https://github.com/msmbuilder/msmbuilder/issues/825,"Hi,

I have a large dataset and when I tried to fit my data, I was running into MemoryErrors when building the transition counts matrix.

When I looked in the code, it looks like there are two pieces that duplicate all the sequences in _transition_counts. The first is when generating the unique classes, the function concatenates all the sequences and then finds the unique items from the large array. Instead it could be done sequence by sequence.

`classes = np.unique(np.concatenate([np.unique(seq) for seq in sequences]))`

The other location that uses a lot of memory is generating an intermediate _transitions list that has all the sequences appended to it. This seems to require enough memory to hold all the from_states and to_states and then when the hstack is called, it doubles the memory requirement. 

It seems like adding coo_matricies directly eliminates the need to put all to_states and from_states in one array. This reduces the memory load considerably. In the current implementation, it seems you need enough memory to hold all sequences four times in a worst case scenario. By adding the coo_matricies, you only need to hold the to_states and from_states of one sequence in memory at any time. This method is faster than the code before #438 but slightly slower than the current implementation. The following is the proposed code for implementation.

```
counts = np.zeros((n_states, n_states), dtype=float)
C = coo_matrix((n_states,n_states), [int])

for y in sequences:
    y = np.asarray(y)
    from_states = y[: -lag_time: 1]
    to_states = y[lag_time::1]

    if contains_none:
        from_states = none_to_nan(from_states)
        to_states = none_to_nan(to_states)

    if contains_nan or contains_none:
        # mask out nan in either from_states or to_states
        mask = ~(np.isnan(from_states) + np.isnan(to_states))
        from_states = from_states[mask]
        to_states = to_states[mask]

    if (not mapping_is_identity) and len(from_states) > 0 and len(to_states) > 0:
        from_states = mapping_fn(from_states)
        to_states = mapping_fn(to_states)

    transitions = np.row_stack((from_states, to_states))

    C = C + coo_matrix((np.ones(transitions.shape[1], dtype=int), transitions),
    shape=(n_states, n_states))

counts = counts + np.asarray(C.todense())

return counts / float(lag_time), mapping`
```

Here's an ipython notebook where I have done some timing tests:

https://gist.github.com/ryjin/05d4fa5836a66395f9df547fe8cfefc2

What do you think of this change?
"
msmbuilder,msmbuilder,157755654,mini-batch normalization,open,,https://github.com/msmbuilder/msmbuilder/issues/820,"Might speed up our workflow for larger datasets:

http://arxiv.org/pdf/1502.03167v3.pdf
"
msmbuilder,msmbuilder,155958375,Use stride in clustering step in Pipeline,open,,https://github.com/msmbuilder/msmbuilder/issues/813,"I was wondering if there was a trick one could use to cluster on a subset of frames (strided) during that step in a `Pipeline`, but then use the full data set for all other steps? The context I'm thinking about this is when using Osprey. This might just be a limitation of the `Pipeline` scheme, but wanted to check if anyone has figured out a workaround.
"
msmbuilder,msmbuilder,154066586,[WIP] Add SymmetryRMSDFeaturizer,open,tested,https://github.com/msmbuilder/msmbuilder/pull/808,"- [x] Implement feature / fix bug
- [x] Add tests
- [ ] Update changelog

For homo-multi-mers, there's rotational or permutational symmetry. This featurizer takes the minimum distance from a defined set of atom index permutations.
"
msmbuilder,msmbuilder,147212580,Add Feature Selection Algorithms,open,,https://github.com/msmbuilder/msmbuilder/issues/797,"We can start by:
- [x] porting from [sklearn](http://scikit-learn.org/stable/modules/feature_selection.html)
- [ ] adding a few from [this resource](http://featureselection.asu.edu/algorithms.php) (esp the decision tree-based approaches)
- [ ] implementing [Gradient Boosted Feature Selection (GBFS)](http://alicezheng.org/papers/gbfs.pdf)

cc @msultan
"
msmbuilder,msmbuilder,145820757,RMSD clustering segfault?,open,bug,https://github.com/msmbuilder/msmbuilder/issues/789,"@jchodera reports on pyemma issue that they want to do rmsd clustering but:

> @maxentile can't actually cluster our data with MSMBuilder without it throwing a segfault

Can you provide more info john?
"
msmbuilder,msmbuilder,144908758,Desired GMRQ behavior when scoring discrete trajectories?,open,,https://github.com/msmbuilder/msmbuilder/issues/787,"Hi!

If I ask a `MarkovStateModel` to score discrete trajectories that never enter its largest strongly connected component, MSMBuilder raises an error instead of returning `0`.

``` python
import numpy as np
import numpy.random as npr
from msmbuilder.msm import MarkovStateModel

# a lot of trajectories exploring one ergodic subspace
dtrajs_a = [npr.randint(low=0,high=50,size=10000) for i in range(100)]

# a few trajectories exploring a different ergodic subspace
dtrajs_b = [npr.randint(low=50,high=100,size=10000) for i in range(10)]

# fit MSM to all trajectories
dtrajs = dtrajs_a + dtrajs_b
msm = MarkovStateModel()
msm.fit(dtrajs)

# score on trajectories that never entered the largest strongly connected component
msm.score(dtrajs_b)
```

Results in:

```

MSM contains 2 strongly connected components above weight=1.00. Component 0 selected, with population 90.909091%
MSM contains 1 strongly connected component above weight=1.00. Component 0 selected, with population 100.000000%
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-aedb3cc56530> in <module>()
      1 # score on trajectories that never entered the largest strongly connected component
----> 2 msm.score(dtrajs_b)

.../msmbuilder/msm/msm.pyc in score(self, sequences, y)
    456 
    457         if self.mapping_ != m2.mapping_:
--> 458             V = self._map_eigenvectors(V, m2.mapping_)
    459             # we need to map this model's eigenvectors
    460             # into the m2 space

.../msmbuilder/msm/msm.pyc in _map_eigenvectors(self, V, other_mapping)
    475         self_inverse_mapping = {v: k for k, v in self.mapping_.items()}
    476         transform_mapping = _dict_compose(self_inverse_mapping, other_mapping)
--> 477         source_indices, dest_indices = zip(*transform_mapping.items())
    478 
    479         #print(source_indices, dest_indices)

ValueError: need more than 0 values to unpack
```

Should this return `0` instead?

Josh
"
msmbuilder,msmbuilder,144234999,Add option to return sparse matrices in Featurization/Decomposition ,open,,https://github.com/msmbuilder/msmbuilder/issues/779,"This could help out with larger datasets?
"
msmbuilder,msmbuilder,139735389,implement MAP estimate for the bayesian models. ,open,,https://github.com/msmbuilder/msmbuilder/issues/752,"We need to be able to get the MAP estimate from the set of models being sampled from posteriors by all the Bayesian models to make the analysis somewhat tractable. i think the api should be something like 

``` python
mdl = Bayesianmodel()
mdl.fit(data)
mdl.map_estimate_ 
```
"
msmbuilder,msmbuilder,139468016,try to implement some sort of convergence tests,open,,https://github.com/msmbuilder/msmbuilder/issues/749,"a la pymc: https://pymc-devs.github.io/pymc/modelchecking.html

It's not perfect but at least it's a start. We should also consult the stats dept. at some point.
"
msmbuilder,msmbuilder,133314077,Parallel transformer. ,open,,https://github.com/msmbuilder/msmbuilder/issues/731,"It would be nice to get a Transfomer class with the capability to parallelize over the input set of seq. Ideally it should be capable of parallelizing over ipython cluster or multiprocessing. 
"
msmbuilder,msmbuilder,107056036,Compatibility with PyEMMA,open,,https://github.com/msmbuilder/msmbuilder/issues/654,"Related to #653, there was discussion on making sure our high-level API continues to be convergent with that of PyEMMA's. Some ideas put forth:
- Make an equivalent tutorial ipython notebook to show how to do the same thing in both packages
- Add methods to convert msmbuilder objects <--> pyemma objects
"
msmbuilder,msmbuilder,107055622,Integration with msmtools,open,,https://github.com/msmbuilder/msmbuilder/issues/653,"To facilitate greater code re-use, msmbuilder will start using msmtools (https://github.com/markovmodel/msmtools) for algorithms and functionality. msmbuilder will act as a ""front end"" to deal with UI, dataset management, documentation, and others.

PyEMMA (https://github.com/markovmodel/PyEMMA) has already been refactored in this model. PyEmma and MSMBuilder will both be built on mdtraj (for trajectory processing, managed by pandegroup) and msmtools (for msm algorithms, managed by prof. Noe's group)

Let's start thinking about how to transition some algorithms to msmtools. Something like the following:
1. Identify algorithms in msmtools that are not present in msmbuilder and expose in msmbuilder
2. Identify algorithms that are better(?) in msmtools and switch our implementation to that
3. Identify algorithms in msmbuilder that aren't in msmtools, contribute them, and switch over (yay collaboration!)
4. Switch over remaining algorithms

cc @rmcgibbo @franknoe @marscher
"
msmbuilder,msmbuilder,85652861,[ENH] Calculating uncertainty in MFPT,open,,https://github.com/msmbuilder/msmbuilder/issues/597,"Could probably be useful.
"
msmbuilder,msmbuilder,83723055,PCCA from command line,open,,https://github.com/msmbuilder/msmbuilder/issues/596,"1. The lumping code is missing from the command line API. We should add this feature.
2. We should also add a lumping example to the docs.
3. What about BACE?
"
msmbuilder,msmbuilder,82708397,Wrap sklearn's DBSCAN,open,,https://github.com/msmbuilder/msmbuilder/issues/595,"cc msmbuilder-user mailing list
"
msmbuilder,msmbuilder,68015644,Speed up `utils.map_drawn_samples`,open,,https://github.com/msmbuilder/msmbuilder/issues/543,"- This function is slow. See https://mailman.stanford.edu/pipermail/msmbuilder-user/2015-April/000070.html
- For many trajectory formats like `xtc`, `seek` is not an O(1) operation, so repeatedly calling `load_frame()` to load  a single frame (random access) is slow. If you need to load multiple frames from the file, it would be much better to sort the indices, load the trajectory, and then run through it and grab all of the frames.
- Also, the name `""map_drawn_samples""` is really awkward. Maybe we can come up with a better one?
"
msmbuilder,msmbuilder,62194910,lumping for rate matrices,open,,https://github.com/msmbuilder/msmbuilder/issues/505,"The API for lumping is a bit hacky, particularly for people using rate matrices.  I imagine that the advantages of rate matrices might lead to improved stability in PCCA-like models, so this might be something to fix longer term.

For example, my current pipeline for PCCA with rates looks like this:

```
clusterer = cluster.MiniBatchKMedoids(n_clusters=n_states, metric=""rmsd"")
microstate_model = msm.ContinuousTimeMSM()

pipeline0 = make_pipeline(clusterer, microstate_model)
labels = pipeline0.fit_transform(trajectories)

pcca = lumping.PCCAPlus.from_msm(microstate_model, n_macrostates=n_macrostates)
macrostate_model = msm.ContinuousTimeMSM()
macrostate_model.fit(pcca.transform(labels))
```
"
msmbuilder,msmbuilder,60185735,[FR] Minimum traj length in ConvertChunkedProject,open,,https://github.com/msmbuilder/msmbuilder/issues/494,"So back in the day, ConvertDataToHDF had the ability to specify the minimum number of frames you wanted in a trajectory.  That doesn't appear to be an option in msmb ConvertChunkedProject.  Is there a way to put it in so I don't have to remove single-frame trajectories manually?
"
msmbuilder,msmbuilder,59139008,tICA plot command?,open,,https://github.com/msmbuilder/msmbuilder/issues/488,"Might be useful to make a command for making scatter plots of transformed coordinates that are in a vector space.

We have a example notebook like http://msmbuilder.org/latest/examples/plot-tica-heatmap.html, but it would be nice to integrate these examples with the command line layer.
"
msmbuilder,msmbuilder,51212392,Reorganize _ratematrix.sigma_X to not require matrix inverse,open,,https://github.com/msmbuilder/msmbuilder/issues/386,"It should be possible to reorganize the loops in `sigma_timescales`, `sigma_K`, and `sigma_K` to use the cholesky of the hessian instead of its inverse. For sigma_pi, it's pretty obvious, since it's already written using `H^{-1} * v` products, but the others need a little more coaxing.

http://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/
"
msmbuilder,msmbuilder,49655987,[ENH] Speed up calculation of MSM count matrix,open,,https://github.com/msmbuilder/msmbuilder/issues/318,"@mpharrigan: you mentioned at lunch that you thought calculating the count matrix from the sequences was slow. Do you have any benchmarks / timing on that. If so, I can do it in cython/cpp
"
msmbuilder,msmbuilder,46142881,[FR] Implement IPython network visualizations,open,,https://github.com/msmbuilder/msmbuilder/issues/271,"I found [this](https://github.com/fkling/JSNetworkX) awesome javascript port of NetworkX. Maybe we can implement a webview of networks generated by mixtape?
"
msmbuilder,msmbuilder,38693986,GaussianFusionHMM.draw_samples,open,bug,https://github.com/msmbuilder/msmbuilder/issues/228,"`GaussianFusionHMM.draw_samples()` does not return `sample_features` as advertised by the docstring
"
msmbuilder,msmbuilder,33444580,GaussianFusionHMM.find_centroids pull multiple centroids per state,open,,https://github.com/msmbuilder/msmbuilder/issues/150,"See https://github.com/rmcgibbo/mixtape/pull/149#issuecomment-43017061
"
msmbuilder,msmbuilder,28287778,discrete_approx_mvn (sample-ghmm) is kind of slow,open,,https://github.com/msmbuilder/msmbuilder/issues/54,"- BFGS optimization over the lagrange multipliers on the constraints. Not clear that it can be hot-started any better.
- Stochastic gradient descent?
- Profile the performance of the objective function / gradient calculation in detail
- Possible things that could help: 
  - Call sklearn.utils.ext_math.fast_dot, which -- depending on the version of numpy -- link against a faster blas.
  - Write the inner loop for computing the obj / grad in C.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMMoC-BFloat-Algebra,pedlefsen,153295445,Understand and quantify the drift,open,,https://github.com/pedlefsen/HMMoC-BFloat-Algebra/issues/2,"Due to the binary representation of decimal numbers, the number stored in the computer is slightly different than the one the user specified.

These small issues become very noticeable when large exponents are applied to these numbers.

For example when using bfloats if 1 is multiplied by 10^10 ten times in a for loop, the resulting number is:
9.999892e+99
Which is already incorrect in the 5th significant digit. If the for loop ran for 100 or 1000 iterations instead of only 10, then the resulting numbers are:
100:   9.998843e+999
1000:  9.98847e+9999

Similar behaviour occurs when dividing instead of multiplying.

A number of questions arise:

0) How does drift differ between the different types? bfloat vs logspace etc.

1) How relevant is this drift to computation in general. It is only this extreme in these examples because we are repeatedly performing the same operation with the same numbers. In general, one would expect the 'down-rounding' to cancel out the 'up-rounding' when different numbers gets converted their binary representations.

2) Are there specific computations that should be avoided? For example is doing 1_10^100 better than doing 1_10^10^10?

3) When computing probabilities related to genomic analyses, certain numbers will occur often (0; 0.25; 0.5; 0.75; 1.0) - how accurate are the binary representations for these numbers? Are there easy tricks to use that makes the issue go away? for example compute the denominators separately and only calculate the decimal value in the last step? for example instead of 0.25^10 do 1/(4^10) since 4 can be represented perfectly in binary?
"
HMMoC-BFloat-Algebra,pedlefsen,2127179,Update HMMoC to work with g++ v.>=4.5  AND test on SUSE linux,open,,https://github.com/pedlefsen/HMMoC-BFloat-Algebra/issues/1,"The HMMoC-BFloat-Algebra library failed to compile under SUSE linux g++ v. 4.5.0  The current version under OSX 10.6.2 appears to be g++ v. 4.2.1.  This URL details some of the problems to be expected in upgrading from 4.2 to 4.5:

http://lists.debian.org/debian-devel-announce/2011/02/msg00012.html

The code in Algebra.hpp exercised one of the problems documented above.

In Algebra.hpp there are four operator overload functions (starting on line 1560) defined like this:

/\* old style */
inline
BFloat::BFload&
BFloat::
   operator= ...

This fails with error message: 
   ‘hmmoc::BFloat::BFloat’ names the constructor, not the type

It turns out that the initial BFloat:: is unnecessary and confusing.  This syntax works:

/\* new style */
inline
BFloat&
   BFloat::operator= ... 

---

Another problem:  starting on line 1653 is a somewhat misleading error message: ""declaration of ‘operator=’ as non-function ""

This derives from the fact that uint32_t is not defined by default (under SUSE g++).  It can be repaired by adding 
# include <stdint.h>

near the top of the .hpp file.

---

Linkage problem.  In the Makefile, when creating the ""tests"" executable, it seems to work better to include the libHMMoC-BFloat-Algebra library AFTER including the Tests.o object file on the g++ or ld command.

These changes allow compilation, linking and running the tests executable on both the Mac and SUSE platforms, with identical results. 
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
ioBroker.hmm,husky-koglhof,285692046,Make adapter ready for Admin v3,open,,https://github.com/husky-koglhof/ioBroker.hmm/issues/15,For Admin3 some changes are needed: see http://forum.iobroker.net/viewtopic.php?f=24&t=10268
ioBroker.hmm,husky-koglhof,276416258,Update io-package.json,open,,https://github.com/husky-koglhof/ioBroker.hmm/pull/14,added translations
ioBroker.hmm,husky-koglhof,266975313,Please add author field,open,,https://github.com/husky-koglhof/ioBroker.hmm/issues/13,Like this one: https://github.com/ioBroker/ioBroker.hm-rpc/blob/master/io-package.json#L103
ioBroker.hmm,husky-koglhof,197285454,allow a start without having configured without crashing hard,open,,https://github.com/husky-koglhof/ioBroker.hmm/pull/12,… and add basic adapter testing … could be used as basis also for the other adapters :-)
ioBroker.hmm,husky-koglhof,112739243,Update io-package.json,open,,https://github.com/husky-koglhof/ioBroker.hmm/pull/11,
ioBroker.hmm,husky-koglhof,111044513,Update io-package.json,open,,https://github.com/husky-koglhof/ioBroker.hmm/pull/10,
ioBroker.hmm,husky-koglhof,109277175,Update io-package.json,open,,https://github.com/husky-koglhof/ioBroker.hmm/pull/9,
ioBroker.hmm,husky-koglhof,108593735,Dokumentation anpassen,open,,https://github.com/husky-koglhof/ioBroker.hmm/issues/8,
ioBroker.hmm,husky-koglhof,108593718,NLS für Englisch und Russisch einpflegen,open,,https://github.com/husky-koglhof/ioBroker.hmm/issues/7,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
profillic-hmmer,galosh,2149878,"prolific, DynamicProgramming.hpp, argument incompatibility",open,,https://github.com/galosh/profillic-hmmer/issues/7,"in the DynamicProgramming.hpp file, on line 8087 there is a min() function call.  The arguments are of different types that C++ finds incompatible.  Changing the line to read:

m_sequence_count = ( ( sequence_count == 0 ) ? sequences->size() : min( (size_t) sequence_count, sequences->size() ) );

i.e. adding (size_t)  cast, repairs it.
"
profillic-hmmer,galosh,2138885,profillic-hmmer doesn't link under (SUSE) linux without some changes to the Makefile,open,,https://github.com/galosh/profillic-hmmer/issues/6,"profillic-hmmer needs to have the -leasel link library in the last position of the the link commands.  Otherwise the SUSE linux loader throws unsatisfied external symbol errors.

(I have a modified Makefile on hand)
"
profillic-hmmer,galosh,1848821,Doxygen,open,,https://github.com/galosh/profillic-hmmer/issues/5,"Consider using Doxygen to create and document code simultaneously?
"
profillic-hmmer,galosh,1848812,m_parameters in ProfuseTest,open,,https://github.com/galosh/profillic-hmmer/issues/4,"What if we extended the Parameter type to include the text of a potential commandline parameter?  Eg:

--saveResultsToFile=""fname""

and let parameters be dynamic (i.e. keys in a dictionary) instead of fixed field names?
"
profillic-hmmer,galosh,1847404,"test harness for profillic, etc.",open,,https://github.com/galosh/profillic-hmmer/issues/3,"It would probably be a good idea to refactor the tests in seqantests, quicktests, profusetests, etc. into individual unit tests within a test harness.  There are (at least) two test harness packages which would work pretty quickly.  CppUnit is a port of JUnit, and seems to have good support assertions at test-time.  Known advantage: I'm somewhat familiar with JUnit.  The Boost Test Library is another option.  Known advantage: we're already using Boost and won't have to add another 3rd party product.
"
profillic-hmmer,galosh,1847217,ProfileGibbs compilation,open,,https://github.com/galosh/profillic-hmmer/issues/2,"ProfileGibbs.hpp requires the definition of doublerealspace for some member functions it defines.

Adding lines like this:
# ifndef HMMOC_BFLOAT_ALGEBRA_HPP
# include ""Algebra.hpp""
# endif

or, more in line with the installation instructions:
# include ""../HMMoC-BFloat-Algebra/Algebra.hpp""
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Hmm-NERTagger,gugug,374712609,求数据集,open,,https://github.com/gugug/Hmm-NERTagger/issues/5,您好，能提供一下带标签的数据集么
Hmm-NERTagger,gugug,374712320,求数据集,open,,https://github.com/gugug/Hmm-NERTagger/issues/4,您好，您方便提供一下数据集么，万分感谢！
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
SpeechRecognitionHMM,Sciss,248318233,Cepstral Mean Normaliztion in Feature Extraction does not work/,open,,https://github.com/Sciss/SpeechRecognitionHMM/issues/2,"It is calculated and stored in mCeps[][] in the FeatureExtract Class but never writen back in mfccFeature[][], so it will never be used i guess."
SpeechRecognitionHMM,Sciss,74723754,MFCC Java implementation and Preemphasis,open,auto-migrated,https://github.com/Sciss/SpeechRecognitionHMM/issues/1,"```
On file MFCC.java on line 40 and 41.
How to Work the ""preEmphasis""? if the call to the method is after of the 
""magnitudeSpectrum"" and ""magnitudeSpectrum"" return a new instance of ""double[]"" 
name ""bin"".

Sorry for my bad English.

Best Regards.
Sebastian Rasilla.


```

Original issue reported on code.google.com by `srasi...@gmail.com` on 19 Apr 2013 at 1:09
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
MoGeRe,bandinigo,213454799,start.m4a / stop.m4a not accessible on your Dropbox,open,,https://github.com/bandinigo/MoGeRe/issues/1,Just an FYI...I had to link in the github raw versions of these files by modifying the acceldatacollect/templates/discreteRecorder.html when running locally....
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
bioruby-hmmer3_report,wwood,6471395,Argument error (support hmmscan default output),open,enhancement,https://github.com/wwood/bioruby-hmmer3_report/issues/1,"using bio 1.4.2, ruby 1.9.3, OS X 10.7.4 

``` ruby
require 'bio-hmmer3_report'

reportfile = ARGV[0]

report = Bio::HMMER::HMMER3::Report.new(File.open(reportfile))

report.hits.each do |hit|
  #puts hit.target_name
  #puts hit.target_accession
  #puts hit.query_name
  #puts hit.query_accession
  #puts hit.query_length
  #puts hit.full_sequence_e_value
  #puts hit.full_sequence_score
  #puts hit.domain_number
  #puts hit.domain_sum
  #puts hit.domain_c_e_value
  #puts hit.domain_i_e_value
  #puts hit.domain_score
  #puts hit.domain_bias
  #puts hit.hmm_coord_from
  #puts hit.hmm_coord_to
  #puts hit.ali_coord_from
  #puts hit.ali_coord_to
  #puts hit.env_coord_from
  #puts hit.env_coord_to
  #puts hit.acc
  #puts hit.target_description
end

```

I get this error while reading a hmmscan report.

```
/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bio-hmmer3_report-0.0.1/lib/bio/appl/hmmer/hmmer3/report.rb:117:in `initialize': line 10 is in an unrecognized format [Query:       1A  [L=127] (ArgumentError)
/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bio-hmmer3_report-0.0.1/lib/bio/appl/hmmer/hmmer3/report.rb:57:in `new'
/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bio-hmmer3_report-0.0.1/lib/bio/appl/hmmer/hmmer3/report.rb:57:in `parse_line'
/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bio-hmmer3_report-0.0.1/lib/bio/appl/hmmer/hmmer3/report.rb:29:in `block in initialize'
/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bio-hmmer3_report-0.0.1/lib/bio/appl/hmmer/hmmer3/report.rb:29:in `each_line'
/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/bio-hmmer3_report-0.0.1/lib/bio/appl/hmmer/hmmer3/report.rb:29:in `initialize'
    from hmmer_report.rb:5:in `new'
    from hmmer_report.rb:5:in `<main>'

```
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
unsupervised-pos-tagging,musyoku,291520906,Seems like makefile's result is broken!,open,,https://github.com/musyoku/unsupervised-pos-tagging/issues/1,"Hi! I translated your instruction by Google Translate and tried to compile .so files as described, but `/bayesian_hmm/run/train_en.py` returns the following:
```Traceback (most recent call last):
  File ""train_en.py"", line 2, in <module>
    import treetaggerwrapper
ModuleNotFoundError: No module named 'treetaggerwrapper'
```
I also found out that `import bhmm` doesn't work also:
```
ImportError: /home/*****/unsupervised-pos-tagging/bayesian-hmm/run/bhmm.so: undefined symbol: _ZN5boost7archive23basic_binary_oprimitiveINS0_15binary_oarchiveEcSt11char_traitsIcEE4saveERKNSt7__cxx1112basic_stringIcS4_SaIcEEE
```
`make install` didn't work for me, so I've used  `make install_ubuntu` and it didn't give me any error message, and `bhmm.so` exists in `run` directory.
I used boost 1.64
Any suggestions how to fix this?"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
pyVSR,georgesterpu,309066611,Avletters,open,,https://github.com/georgesterpu/pyVSR/pull/1,"Hi,

some code changes for importing AVLetters database."
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
ebfret-gui,ebfret,292221592,struct2array() call,open,,https://github.com/ebfret/ebfret-gui/issues/26,"File/Export/Traces calls struct2array(), which does not seem to exist in Matlab 2017b:

![image](https://user-images.githubusercontent.com/7998057/35484896-1e1b2b7e-0425-11e8-8175-9beaa3ff4a3e.png)

"
ebfret-gui,ebfret,202282668,File loading problem,open,,https://github.com/ebfret/ebfret-gui/issues/25,"Running Matlab 2016b on Mac OS Sierra 10.12.2 and cannot load files saved from iSMS (both in SMD or vbFRET .mat format). 

Get error:

>> ebFRET

ans = 

  MainWindow with properties:

     handles: [1×1 struct]
    controls: [1×1 struct]
      series: [0×0 struct]
    analysis: [1×6 struct]
       plots: []

Reference to non-existent field 'series'.

Error in ebfret.ui.MainWindow/load_data (line 56)
        if all(cellfun(@isnumeric, {session.series.group}))

Error in ebfret.ui.MainWindow>@(source,event)load_data(self)
 
Error while evaluating Menu Callback"
ebfret-gui,ebfret,56954233,file loading problem,open,duplicate,https://github.com/ebfret/ebfret-gui/issues/24,"My macbook runs Yosemite with Matlab-R2014b. The only problem that I have is it fails to load .tsv files.
The following is the error message that it returns with an ""Error Dialog saying, Edge vector must be monotonically non-decreasing."":

> > ebf = ebFRET();
> > Warning: Could not set property value
> > Property: Interpreter
> > Value: none
> 
> In waitbar>createWaitbar at 198
>    In waitbar at 101
>    In MainWindow.load_data at 75
>    In MainWindow.MainWindow>@(source,event)load_data(self)
> Preference to nonexistent field 'viterbi'.

Error in ebfret.ui.MainWindow/refrersh (line 36)
              if ~self.series(n).exclude &&
              ~isempty(analysis(a).viterbi(n).state)

Error in ebfret.ui.MainWindow/load_data (line 216)
              self.refresh('ensemble', 'series');

Error in ebfret.ui.MainWindow/MainWindow/@(source,event)load_data(self)

Error while evaluating Menu Callback
"
ebfret-gui,ebfret,15987361,loading and saving files,open,,https://github.com/ebfret/ebfret-gui/issues/23,"New version (06/24/2013) fails to load .mat files previously saved. Repeating ""load"" opens the file but does not show graphs. It's only when the ""Posterior"" was unchecked, it shows the plots.

Saving summary or path results still not working.
"
ebfret-gui,ebfret,15249112,Add option to specify frame rate and calculate transition rates,open,,https://github.com/ebfret/ebfret-gui/issues/21,
ebfret-gui,ebfret,15249085,Time series sometimes shows wrong viterbi mean,open,,https://github.com/ebfret/ebfret-gui/issues/20,
ebfret-gui,ebfret,15242690,"""Analysis Finished"" dialog",open,,https://github.com/ebfret/ebfret-gui/issues/18,"Can you have a box pop up when the analysis is finished? Sometimes I think it is done because the graphs are not changing, but after a while they start changing again. 
"
ebfret-gui,ebfret,14856187,Write instruction manual,open,,https://github.com/ebfret/ebfret-gui/issues/14,
ebfret-gui,ebfret,14856142,Export plot data,open,,https://github.com/ebfret/ebfret-gui/issues/13,
ebfret-gui,ebfret,14855661,Create status bar display,open,,https://github.com/ebfret/ebfret-gui/issues/10,
ebfret-gui,ebfret,14855570,Batch mode operation,open,,https://github.com/ebfret/ebfret-gui/issues/9,
ebfret-gui,ebfret,14855553,Create controls for exporting parameter spreadsheet,open,,https://github.com/ebfret/ebfret-gui/issues/8,
ebfret-gui,ebfret,14855537,Create gui controls for adjusting prior,open,,https://github.com/ebfret/ebfret-gui/issues/7,
ebfret-gui,ebfret,14855525,Create gui controls for merging states,open,,https://github.com/ebfret/ebfret-gui/issues/6,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
E-Vaporate,EwyBoy,300017026,Correct some translation errors,open,,https://github.com/EwyBoy/E-Vaporate/pull/3,"Correct some translation errors and polish the words.
It seems like 'E-Vaporate' comes from 'evaporate', using 'E' to emphasis 'electric'.
Since I haven't found better translation for 'E-Vaporate' , I think it's better to keep the original name."
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
move.HMM,benaug,23890525,updated request: cosmetic plus a few features,open,,https://github.com/benaug/move.HMM/pull/3,"this is everything I've done so far.  Mostly cosmetic/changes to allow passing R CMD check, but a few new features (devFunOnly to return the negative log-likelihood function; allow non-parallel boot CI calculation)
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
riboHMM,rajanil,310573073,Installation Error,open,,https://github.com/rajanil/riboHMM/issues/3,"Hello,

I (and others) have been having the :

<Long 
string
of error 
text>

error: command 'gcc' failed with exit status 1

This was reported in a previous post, but was not addressed. Any ideas on how to fix this?"
riboHMM,rajanil,167263992,test folder,open,,https://github.com/rajanil/riboHMM/issues/2,"Hi Rajani,
Where is the test files that are referred to in the READMe file? Alternatively could you please include a sample gtf format that is acceptable to riboHMM since the all the mouse gtfs from ucsc/ncbi/ensembl don't seem to work in my hands.
"
riboHMM,rajanil,143920961,Installation problem,open,,https://github.com/rajanil/riboHMM/issues/1,"Hi,

I tried installing riboHMM using 
`sudo python setup.py install` 
but it always throws me error like following

```
Error compiling Cython file:
------------------------------------------------------------
...
    return x_final, optimized

@cython.boundscheck(False)
@cython.wraparound(False)
@cython.nonecheck(False)
cdef tuple alpha_func_grad(np.ndarray[np.float64_t, ndim=2] x, list data, list states, list frames, np.ndarray[np.float64_t, ndim=3] rescale, np.ndarray[np.float64_t, ndim=2] beta):
    ^
------------------------------------------------------------

ribohmm.pyx:1197:5: Function signature does not match previous declaration

Error compiling Cython file:
------------------------------------------------------------
...
    return func, gradient

@cython.boundscheck(False)
@cython.wraparound(False)
@cython.nonecheck(False)
cdef tuple alpha_func_grad_hess(np.ndarray[np.float64_t, ndim=2] x, list data, list states, list frames, np.ndarray[np.float64_t, ndim=3] rescale, np.ndarray[np.float64_t, ndim=2] beta):
    ^
------------------------------------------------------------

ribohmm.pyx:1265:5: Function signature does not match previous declaration
building 'ribohmm' extension
x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/lib/python2.7/dist-packages/numpy/core/include -I. -I/usr/include/python2.7 -c ribohmm.c -o build/temp.linux-x86_64-2.7/ribohmm.o
ribohmm.c:1:2: error: #error Do not use this file, it is the result of a failed Cython compilation.
 #error Do not use this file, it is the result of a failed Cython compilation.
  ^
error: command 'x86_64-linux-gnu-gcc' failed with exit status 1

```

Any guidance in installing the riboHMM would be highly appreciable.
Thanks in advance
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hercules,BilkentCompGen,365742603,Can we simply run for specific chromosome?,open,,https://github.com/BilkentCompGen/hercules/issues/6,"Hi,

I have a specific question to find phasing of tow mutations on chr21. I would like to know can I simply do some filtering in advance to get those reads of chr21 from short&long reads and then use Hercules to correct reads of chr21 only. This has the assumption that the bam files of short and long reads are mapping correctly in advance.

Many thanks!


"
hercules,BilkentCompGen,350088923,preprocessing step doesn't produce bowtie2 compatable fasta reference from input long fastq,open,bug,https://github.com/BilkentCompGen/hercules/issues/5,"hercules help suggests that it can use a fastq as input for the long reads. the preprocessing step works fine, but then the suggested bowtie2 steps fail because bowtie2-build requires a fasta reference and compressed_long is still a fastq (albeit one with all the quality information removed)."
hercules,BilkentCompGen,350070809,"Does hercules work with compressed fast{a,q}?",open,,https://github.com/BilkentCompGen/hercules/issues/4,"If not, it'd be good to allow gzipped input files."
hercules,BilkentCompGen,345795598,Install error,open,,https://github.com/BilkentCompGen/hercules/issues/3,"Hi,

I tried installing this in a clean conda environment but I hit a compilation error. Do you need a particular compiler for it or should it be OK with the stick gcc?

```
bash-4.1$ source activate hercules
(hercules) bash-4.1$ git clone https://github.com/BilkentCompGen/hercules.git
Cloning into 'hercules'...
remote: Counting objects: 33, done.
remote: Compressing objects: 100% (5/5), done.
remote: Total 33 (delta 0), reused 3 (delta 0), pack-reused 28
Unpacking objects: 100% (33/33), done.
Checking connectivity... done.
(hercules) bash-4.1$ cd hercules/src/
(hercules) bash-4.1$ make
mkdir -p ../utils/include/
mkdir -p ../utils/lib/
tar -xf ../utils/zlib-1.2.11.tar.gz -C ../utils/
cd ../utils/zlib-1.2.11/; prefix=../ ./configure; make install prefix=../
Checking for gcc...
Checking for shared library support...
Building shared library libz.so.1.2.11 with gcc.
Checking for size_t... Yes.
Checking for off64_t... Yes.
Checking for fseeko... Yes.
Checking for strerror... Yes.
Checking for unistd.h... Yes.
Checking for stdarg.h... Yes.
Checking whether to use vs[n]printf() or s[n]printf()... using vs[n]printf().
Checking for vsnprintf() in stdio.h... Yes.
Checking for return value of vsnprintf()... Yes.
Checking for attribute(visibility) support... Yes.
make[1]: Entering directory `/cluster/gjb_lab/nschurch/NOBACK/herculestest/hercules/utils/zlib-1.2.11'
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o adler32.o adler32.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o crc32.o crc32.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o deflate.o deflate.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o infback.o infback.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o inffast.o inffast.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o inflate.o inflate.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o inftrees.o inftrees.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o trees.o trees.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o zutil.o zutil.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o compress.o compress.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o uncompr.o uncompr.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o gzclose.o gzclose.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o gzlib.o gzlib.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o gzread.o gzread.c
gcc -O3 -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -c -o gzwrite.o gzwrite.c
ar rc libz.a adler32.o crc32.o deflate.o infback.o inffast.o inflate.o inftrees.o trees.o zutil.o compress.o uncompr.o gzclose.o gzlib.o gzread.o gzwrite.o 
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/adler32.o adler32.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/crc32.o crc32.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/deflate.o deflate.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/infback.o infback.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/inffast.o inffast.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/inflate.o inflate.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/inftrees.o inftrees.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/trees.o trees.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/zutil.o zutil.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/compress.o compress.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/uncompr.o uncompr.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/gzclose.o gzclose.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/gzlib.o gzlib.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/gzread.o gzread.c
gcc -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN  -DPIC -c -o objs/gzwrite.o gzwrite.c
gcc -shared -Wl,-soname,libz.so.1,--version-script,zlib.map -O3 -fPIC -D_LARGEFILE64_SOURCE=1 -DHAVE_HIDDEN -o libz.so.1.2.11 adler32.lo crc32.lo deflate.lo infback.lo inffast.lo inflate.lo inftrees.lo trees.lo zutil.lo compress.lo uncompr.lo gzclose.lo gzlib.lo gzread.lo gzwrite.lo  -lc 
rm -f libz.so libz.so.1
ln -s libz.so.1.2.11 libz.so
ln -s libz.so.1.2.11 libz.so.1
rm -f ..//lib/libz.a
cp libz.a ..//lib
chmod 644 ..//lib/libz.a
cp libz.so.1.2.11 ..//lib
chmod 755 ..//lib/libz.so.1.2.11
rm -f ..//share/man/man3/zlib.3
cp zlib.3 ..//share/man/man3
chmod 644 ..//share/man/man3/zlib.3
rm -f ..//lib/pkgconfig/zlib.pc
cp zlib.pc ..//lib/pkgconfig
chmod 644 ..//lib/pkgconfig/zlib.pc
rm -f ..//include/zlib.h ..//include/zconf.h
cp zlib.h zconf.h ..//include
chmod 644 ..//include/zlib.h ..//include/zconf.h
make[1]: Leaving directory `/cluster/gjb_lab/nschurch/NOBACK/herculestest/hercules/utils/zlib-1.2.11'
rm -rf ../utils/zlib-1.2.11/
mkdir -p ../utils/include/
mkdir -p ../utils/lib/
tar -xf ../utils/bzip2-1.0.6.tar.gz -C ../utils/
cd ../utils/bzip2-1.0.6/; make install PREFIX=../
make[1]: Entering directory `/cluster/gjb_lab/nschurch/NOBACK/herculestest/hercules/utils/bzip2-1.0.6'

If compilation produces errors, or a large number of warnings,
please read README.COMPILATION.PROBLEMS -- you might be able to
adjust the flags in this Makefile to improve matters.

Also in README.COMPILATION.PROBLEMS are some hints that may help
if your build produces an executable which is unable to correctly
handle so-called 'large files' -- files of size 2GB or more.

gcc -Wall -Winline -O2 -g -D_FILE_OFFSET_BITS=64 -c blocksort.c
gcc -Wall -Winline -O2 -g -D_FILE_OFFSET_BITS=64 -c huffman.c
gcc -Wall -Winline -O2 -g -D_FILE_OFFSET_BITS=64 -c crctable.c
gcc -Wall -Winline -O2 -g -D_FILE_OFFSET_BITS=64 -c randtable.c
gcc -Wall -Winline -O2 -g -D_FILE_OFFSET_BITS=64 -c compress.c
gcc -Wall -Winline -O2 -g -D_FILE_OFFSET_BITS=64 -c decompress.c
gcc -Wall -Winline -O2 -g -D_FILE_OFFSET_BITS=64 -c bzlib.c
rm -f libbz2.a
ar cq libbz2.a blocksort.o huffman.o crctable.o randtable.o compress.o decompress.o bzlib.o
ranlib libbz2.a
gcc -Wall -Winline -O2 -g -D_FILE_OFFSET_BITS=64 -c bzip2.c
gcc -Wall -Winline -O2 -g -D_FILE_OFFSET_BITS=64  -o bzip2 bzip2.o -L. -lbz2
gcc -Wall -Winline -O2 -g -D_FILE_OFFSET_BITS=64 -c bzip2recover.c
gcc -Wall -Winline -O2 -g -D_FILE_OFFSET_BITS=64  -o bzip2recover bzip2recover.o
if ( test ! -d ..//bin ) ; then mkdir -p ..//bin ; fi
if ( test ! -d ..//lib ) ; then mkdir -p ..//lib ; fi
if ( test ! -d ..//man ) ; then mkdir -p ..//man ; fi
if ( test ! -d ..//man/man1 ) ; then mkdir -p ..//man/man1 ; fi
if ( test ! -d ..//include ) ; then mkdir -p ..//include ; fi
cp -f bzip2 ..//bin/bzip2
cp -f bzip2 ..//bin/bunzip2
cp -f bzip2 ..//bin/bzcat
cp -f bzip2recover ..//bin/bzip2recover
chmod a+x ..//bin/bzip2
chmod a+x ..//bin/bunzip2
chmod a+x ..//bin/bzcat
chmod a+x ..//bin/bzip2recover
cp -f bzip2.1 ..//man/man1
chmod a+r ..//man/man1/bzip2.1
cp -f bzlib.h ..//include
chmod a+r ..//include/bzlib.h
cp -f libbz2.a ..//lib
chmod a+r ..//lib/libbz2.a
cp -f bzgrep ..//bin/bzgrep
ln -s -f ..//bin/bzgrep ..//bin/bzegrep
ln -s -f ..//bin/bzgrep ..//bin/bzfgrep
chmod a+x ..//bin/bzgrep
cp -f bzmore ..//bin/bzmore
ln -s -f ..//bin/bzmore ..//bin/bzless
chmod a+x ..//bin/bzmore
cp -f bzdiff ..//bin/bzdiff
ln -s -f ..//bin/bzdiff ..//bin/bzcmp
chmod a+x ..//bin/bzdiff
cp -f bzgrep.1 bzmore.1 bzdiff.1 ..//man/man1
chmod a+r ..//man/man1/bzgrep.1
chmod a+r ..//man/man1/bzmore.1
chmod a+r ..//man/man1/bzdiff.1
echo "".so man1/bzgrep.1"" > ..//man/man1/bzegrep.1
echo "".so man1/bzgrep.1"" > ..//man/man1/bzfgrep.1
echo "".so man1/bzmore.1"" > ..//man/man1/bzless.1
echo "".so man1/bzdiff.1"" > ..//man/man1/bzcmp.1
make[1]: Leaving directory `/cluster/gjb_lab/nschurch/NOBACK/herculestest/hercules/utils/bzip2-1.0.6'
rm -rf ../utils/bzip2-1.0.6/
rm -rf utils/include/seqan/
mkdir -p ../utils/include/
mkdir -p ../utils/lib/
tar -xf ../utils/seqan-library-2.4.0.tar.xz -C ../utils/
mv ../utils/seqan-library-2.4.0/include/* ../utils/include/
rm -rf ../utils/seqan-library-2.4.0
g++  -I../utils/include/ -std=c++14 -O3 -DNDEBUG -DSEQAN_HAS_ZLIB=1 -DSEQAN_HAS_BZIP2=1 -DSEQAN_DISABLE_VERSION_CHECK=YES -W -Wall -pedantic -fopenmp -c -o main.o main.cpp  -L../utils/lib/ -lz -lbz2 -lpthread -lrt -lgomp
cc1plus: error: unrecognized command line option ""-std=c++14""
make: *** [main.o] Error 1
```"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
ham,psathyrella,56172833,Get DOI,open,,https://github.com/psathyrella/ham/issues/13,"https://guides.github.com/activities/citable-code/
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
CL-HMM,juanmirocks,22831787,Create package in Quicklisp,open,,https://github.com/juanmirocks/CL-HMM/issues/3,
CL-HMM,juanmirocks,22827936,Make cbook-alphabet type = simple-vector (see Issue #1),open,,https://github.com/juanmirocks/CL-HMM/issues/2,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,timdream,319224115,unable to run this project ,open,,https://github.com/timdream/hmm/issues/1,"i tried to run this project with 
npm start and npm run but failed tell the other way to run it "
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,asamsulfat,182777270,add fragment,open,,https://github.com/asamsulfat/hmm/pull/1,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmmIBD,glipsnort,377507841,IBD tracts not fully reciprocal between isolates,open,,https://github.com/glipsnort/hmmIBD/issues/2,"Hi,

I'm not sure if this is a bug or simply a feature of the program, but when running hmmIBD with no missing data I was expecting the resulting IBD tracts to be fully reciprocal i.e. at a given site if isolates A and B are IBD, and isolates A and C are IBD, then B and C must also be IBD. However this doesn't seem to be the case at all sites (happy to supply example data). I will write a script to manually correct any such cases as I would like to calculate summary statistics that require tracts to be reciprocal, but I thought I would mention it unless it is a bug you would like to look at."
hmmIBD,glipsnort,341405471,compiling on ubuntu,open,,https://github.com/glipsnort/hmmIBD/issues/1,"`cc -o hmmIBD -O3 -lm -Wall hmmIBD.c` do not work on ubuntu, it fails with the following messages:
```
/tmp/ccz0D5Ur.o: In function `main':
hmmIBD.c:(.text.startup+0x3c28): undefined reference to `log'
hmmIBD.c:(.text.startup+0x3c38): undefined reference to `log'
hmmIBD.c:(.text.startup+0x3ea3): undefined reference to `exp'
hmmIBD.c:(.text.startup+0x3ed2): undefined reference to `exp'
hmmIBD.c:(.text.startup+0x4116): undefined reference to `exp'
hmmIBD.c:(.text.startup+0x4157): undefined reference to `exp'
hmmIBD.c:(.text.startup+0x4488): undefined reference to `exp'
/tmp/ccz0D5Ur.o:hmmIBD.c:(.text.startup+0x44cb): more undefined references to `exp' follow
/tmp/ccz0D5Ur.o: In function `main':
hmmIBD.c:(.text.startup+0x45bc): undefined reference to `log'
hmmIBD.c:(.text.startup+0x45dc): undefined reference to `log'
hmmIBD.c:(.text.startup+0x466f): undefined reference to `log'
```

`cc -o hmmIBD -O3 -Wall hmmIBD.c -lm` resolves the issue.


"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Bis-tools,dnaase,357542940,big error ！！！！,open,,https://github.com/dnaase/Bis-tools/issues/2,"##### ERROR ------------------------------------------------------------------------------------------
##### ERROR A USER ERROR has occurred (version 3.8-1-0-gf15c1c3ef): 
##### ERROR
##### ERROR This means that one or more arguments or inputs in your command are incorrect.
##### ERROR The error message below tells you what is the problem.
##### ERROR
##### ERROR If the problem is an invalid argument, please check the online documentation guide
##### ERROR (or rerun your command with --help) to view allowable command-line arguments for this tool.
##### ERROR
##### ERROR Visit our website and forum for extensive documentation and answers to 
##### ERROR commonly asked questions https://software.broadinstitute.org/gatk
##### ERROR
##### ERROR Please do NOT post this error to the GATK forum unless you have really tried to fix it yourself.
##### ERROR
##### ERROR MESSAGE: Invalid command line: Malformed walker argument: **Could not find walker with name: BisulfiteCountCovariates**
##### ERROR ------------------------------------------------------------------------------------------
### "
Bis-tools,dnaase,318489250,Can't find BisulfiteCountVariant Walker,open,,https://github.com/dnaase/Bis-tools/issues/1,"Hello,

**I ran Bis-SNP1.0.0 from he prompt line using the following command:
(Java version: JDK8)**
java -Xmx10g -jar BisSNP-1.0.0.jar \
-R mm10.fa \
-I file.bam \
-T BisulfiteCountCovariates \
-knownSites dbSNP-150.vcf \
-cov ReadGroupCovariate \
-cov QualityScoreCovariate \
-cov CycleCovariate \
-recalFile File.csv

**and got the following result:**
 ERROR A USER ERROR has occurred (version 3.8-1-0-gf15c1c3ef): 
 ERROR
 ERROR This means that one or more arguments or inputs in your command are incorrect.
 ERROR The error message below tells you what is the problem.
 ERROR
 ERROR If the problem is an invalid argument, please check the online documentation guide
 ERROR (or rerun your command with --help) to view allowable command-line arguments for this tool.
 ERROR
 ERROR Visit our website and forum for extensive documentation and answers to 
 ERROR commonly asked questions https://software.broadinstitute.org/gatk
 ERROR
 ERROR Please do NOT post this error to the GATK forum unless you have really tried to fix it yourself.
 ERROR
 ERROR MESSAGE: Invalid command line: Malformed walker argument: Could not find walker with name: BisulfiteCountCovariates
 ERROR

I can't find the BisulfiteCountCovariate.java walker in BisSNP files could that at least part of the probelm?

Thanks for helping

"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Gesture-recognition-HMM,ankitvora7,168080500,low performance with a huge dataset.,open,,https://github.com/ankitvora7/Gesture-recognition-HMM/issues/2,"Dear Mr.Ankit Vora,

I have a huge skeleton dataset. it is 27 activities performed by  8 subjects and 4 times. ""http://www.utdallas.edu/~cxc123730/UTD-MHAD.html"". first of all, i trained i used the kmeans for extracting the cluster center for the whole training data which is 721 activities. after that I computer forward-backword algorithms and save the parameters ""Observations""a, b ,c , cluster centers. i only modified the number of states to 54 and number of the cluster to 265.
when I tested it I did not obtain a low accuracy which is 5%.
- I want to know how can i tuning the paramenters # states and # of clusters?
- can i use GMM instead of K-means?
- I want to use this method for classifying online . my idea instead of training all the data together. every activity will be trained individually. what is your opinion ? i should use anther Classifiers like Maximum Entropy Markov Model ""MEMM""or HMM is enough.?

thanks in advanced :)
"
Gesture-recognition-HMM,ankitvora7,162347694,explanation of the code,open,,https://github.com/ankitvora7/Gesture-recognition-HMM/issues/1,"Dear Mr.Ankit Vora,

Could you please send me a hint or paper to understand the code or how it works?or any paper explains the concept.
thanks in advance 

Best regards,
Kerolos Ghobrial
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMMextra0s,athenatingwang,383701910,Fortran iface,open,,https://github.com/athenatingwang/HMMextra0s/pull/15,"R-Fortran interface improvements:

- Renamed ```loop1``` to ```fwdeqns```
- Renamed ```loop2``` to ```bwdeqns```
- Use R objects to call Fortran function instead of string (e.g., ```fwdeqns``` instead of ```""fwdeqns""```) - this is recommended by the R developers to improve reliability
- This change requires R >=3.0.0 - added requirement to ```DESCRIPTION```
- Prefix Fortran function objects with ""F_"" in R (e.g., ```F_fwdeqns``` instead of ```fwdeqns```) - this is recommended by the R developers to avoid accidental name clashes"
HMMextra0s,athenatingwang,371338474,Convert R code to Fortran to optimise performance,open,,https://github.com/athenatingwang/HMMextra0s/issues/7,"The following loops could be converted to Fortran code to optimise performance (runtime fractions measured for hmm1dtest.R):

- [ ] E-Step - typically 50% of total runtime
- [x] M-Step - typically 10% of total runtime
- [x] Gamma estimate - typically 10% of total runtime
- [ ] Normal distribution - typically 7% of total runtime"
HMMextra0s,athenatingwang,369964512,continuous integration?,open,,https://github.com/athenatingwang/HMMextra0s/issues/6,It would be nice to have continuous integration turned on so tests are automatically run every time one pushes changes. Chris might be able to help setting this up. 
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
make_full_labels,bajibabu,289624365,SIOD ERROR: unbound variable : eof,open,,https://github.com/bajibabu/make_full_labels/issues/1,"error while trying to build utterance file

root@mahmoud:/home/mahmoud/TTS/make_full_labels-master# /home/mahmoud/TTS/Enviroment/festvox/src/promptselect/text2utts -all -level Text -odir tempTest/utts -otype utts -itype data tempTest/test2.data 

output error : 
SIOD ERROR: unbound variable : eof
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
giza-pp,moses-smt,243984683,Null-Word Alignment Probabilities,open,,https://github.com/moses-smt/giza-pp/issues/2,"hi, i want to know how to set the null-word probability in IBM Model 1,  the probability of inserting a null after a  source word. I have just seen the setting of IBM Model 3,
Hoping your response , thank you"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
PyHMM,nipunbatra,13723251,Improper implicit array assignment in mk_stochastic,open,bug,https://github.com/nipunbatra/PyHMM/issues/2,"Currently workaround done by using temp variable
"
PyHMM,nipunbatra,12305020,vectorize step#2 in viterbi_path.py,open,enhancement,https://github.com/nipunbatra/PyHMM/issues/1,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMMG,ISO-TC211,382602419,Hostname mismatch causes certificate validation failure message,open,,https://github.com/ISO-TC211/HMMG/issues/27,"To whom it may concern:
SVN updating the https://inspire-twg.jrc.it:443 resource always returns above error message. It requires me to manually accept the certificate each time I want to use the ISO TC211 models in EA for our own Project. This is annoying. Can someone fix this, please?
Thanks a lot and kind regards
Jörg Klausen"
HMMG,ISO-TC211,352621202,Missing Model : ISO 19145: Registry of representations of geographic point location,open,,https://github.com/ISO-TC211/HMMG/issues/25,
HMMG,ISO-TC211,328457659,Terms of reference,open,,https://github.com/ISO-TC211/HMMG/issues/24,Noting the resolution from the plenary in Copenhagen. Shall include overall responsibillity for resources for implementation
HMMG,ISO-TC211,309040617,Cleaning ISO19163-1,open,,https://github.com/ISO-TC211/HMMG/issues/22,"The UML model ISO 19163-1 has dependencies on the ISO 19159-1 UML model. These dependencies shall be fixed when ISO 19159-1 will be fixed.

A number of association roles and multiplicities need to be set properly in Enterprise Architect.

The class IE_Georeferencable as two aggregation associations with the class SD_SensorModel with the same role name (i.e. geometricCorrection). These associations doesn't appear in diagrams.

Finally, an interface named 'interface1' is defined but is not used and should probably be removed."
HMMG,ISO-TC211,280451034,Missing modell: ISO19118:2011 Encoding,open,Published,https://github.com/ISO-TC211/HMMG/issues/21,"_From @jetgeo on February 2, 2016 9:30_

This standard is not in the HM


_Copied from original issue: ISO-TC211/HMMG_old#2_"
HMMG,ISO-TC211,280450976,Cleaning ISO 19159-1,open,Published,https://github.com/ISO-TC211/HMMG/issues/20,"_From @jetgeo on November 10, 2016 8:34_

The model was not in the HM. Received from editor, need cleaning

[ ] Elements in diagrams
[ ] Duplicate element names 
[ ] Data type connections
[ ] Definitions


_Copied from original issue: ISO-TC211/HMMG_old#16_"
HMMG,ISO-TC211,280450929,Missing model:  19152 Land Administration Domain Model (LADM),open,Missing models,https://github.com/ISO-TC211/HMMG/issues/19,"_From @jetgeo on November 27, 2017 1:28_



_Copied from original issue: ISO-TC211/HMMG_old#37_"
HMMG,ISO-TC211,280450635,Use of author and language,open,,https://github.com/ISO-TC211/HMMG/issues/18,"_From @jetgeo on June 15, 2016 20:28_

Model metadata: Set author and language (blank) for models. 

Define how to use author.


_Copied from original issue: ISO-TC211/HMMG_old#10_"
HMMG,ISO-TC211,280450591,Data types in  ISO 19153:2013,open,Published,https://github.com/ISO-TC211/HMMG/issues/17,"_From @jetgeo on November 9, 2016 21:15_

Hello John,
I went through the ISO 19153:2013 model of the Harmonized Model. Many attribute types were not connected to their appropriate class. I fixed the easy ones because the classes are existing in the model. But below, you will see a table of issues that needs more attention from yourself to be fixed. Let me know when you will have fixed them so that I can reprocess the whole ISO 19153:2013 package as ontologies.
Package	Class / Interface	Issue
 	 	 
ISO 19153:2013 … / Logical Model / GeoLicense	Agent	The navigability is broken in the aggregation with the class 'Principal'
ISO 19153:2013 … / Logical Model / GeoLicense	Licensor	The Type 'Signature' of the attribute 'signature' is not defined
ISO 19153:2013 … / Logical Model / GeoLicense	PrincipalGroup	The navigability is broken in the aggregation with the class 'Principal'
ISO 19153:2013 … / Logical Model / GeoLicense	Request	The Type 'NameValuePair' of the attribute 'parameters' is not defined
ISO 19153:2013 … / Data Model / XMLSchema	numFacet	The Type 'nonNegativeInteger' of the attribute 'value' is not defined
ISO 19153:2013 … / Data Model / XMLSchema	occurs	The Type 'nonNegativeInteger' of the attribute 'minOccurs' is not defined
ISO 19153:2013 … / Data Model /Rel-mx	ResourceSignedBy	The Types of all attributes are not defined
ISO 19153:2013 … / Data Model /Rel-r	digitalResource	The Type of the attribute 'secureIndirect' is not defined
ISO 19153:2013 … / Data Model /Rel-r	keyHolder	The Type of the attribute 'info' is not defined
ISO 19153:2013 … / Data Model /Rel-r	LinguisticString	The Type of the attribute 'ext_ref_4' is not defined
ISO 19153:2013 … / Data Model /Rel-r	ModelGroup22	The Type of the attribute 'ext_ref_1' is not defined
ISO 19153:2013 … / Data Model /Rel-r	NonSecureReference	The Type of the attribute 'ext_ref_5' is not defined
ISO 19153:2013 … / Data Model /Rel-r	revocable	The Types of the attributes 'ext_ref_2' and  'ext_ref_3' are not defined
ISO 19153:2013 … / Data Model /Rel-sx	validityTimePeriodic	The Type 'nonNegativeInteger' of the attribute 'periodCount' is not defined
ISO 19153:2013 … / Data Model /Rel-sx	wsdlComplete	The Type of the attribute 'wsdl' is not defined

Cheers,
Jean,


_Copied from original issue: ISO-TC211/HMMG_old#15_"
HMMG,ISO-TC211,280450560,Model cleaning,open,,https://github.com/ISO-TC211/HMMG/issues/16,"_From @jetgeo on February 2, 2016 9:29_

- [x] Update all models with status, phase and version
- [x] Clean up all model names


_Copied from original issue: ISO-TC211/HMMG_old#1_"
HMMG,ISO-TC211,280450451,"Missing model: 19130-2 Imagery sensor models for geopositioning -- Part 2: SAR, InSAR, lidar and sonar",open,Missing models,https://github.com/ISO-TC211/HMMG/issues/15,"_From @jetgeo on May 2, 2017 9:9_

 Imagery sensor models for geopositioning -- Part 2: SAR, InSAR, lidar and sonar

_Copied from original issue: ISO-TC211/HMMG_old#23_"
HMMG,ISO-TC211,280450419,New model: 19167 The Application of “Ubiquitous Public Access--to--Geographic Information” for Air Quality Information,open,Missing models,https://github.com/ISO-TC211/HMMG/issues/14,"_From @jetgeo on May 2, 2017 11:2_

The Application of “Ubiquitous Public Access--to--Geographic Information” for Air Quality Information

Note: This is not a standard, just a technical report. The UML is based on ISO19154 Ubiquitous Public Access - Reference Model. The model for 19167 should also be in the HM, probably under Informative. 

_Copied from original issue: ISO-TC211/HMMG_old#24_"
HMMG,ISO-TC211,280450360,Missing model: 19165 Preservation of digital data and metadata,open,Missing models,https://github.com/ISO-TC211/HMMG/issues/13,"_From @jetgeo on May 15, 2017 18:34_

Preservation of digital data and metadata, part 1 and 2

_Copied from original issue: ISO-TC211/HMMG_old#25_"
HMMG,ISO-TC211,280450319,Missing model: 19142 Web Feature Service,open,Missing models,https://github.com/ISO-TC211/HMMG/issues/12,"_From @jetgeo on May 30, 2017 20:31_



_Copied from original issue: ISO-TC211/HMMG_old#26_"
HMMG,ISO-TC211,280450273,Missing model: 19143 Filter encoding,open,Missing models,https://github.com/ISO-TC211/HMMG/issues/11,"_From @jetgeo on May 30, 2017 20:31_



_Copied from original issue: ISO-TC211/HMMG_old#27_"
HMMG,ISO-TC211,280450230,Missing model: 19123-1 Schema for coverage geometry and functions -- Part 1,open,Missing models,https://github.com/ISO-TC211/HMMG/issues/10,"_From @jetgeo on May 30, 2017 20:33_



_Copied from original issue: ISO-TC211/HMMG_old#29_"
HMMG,ISO-TC211,280450174,Missing model: 19126 Feature concept dictionaries and registers,open,Missing models,https://github.com/ISO-TC211/HMMG/issues/9,"_From @jetgeo on May 30, 2017 20:34_



_Copied from original issue: ISO-TC211/HMMG_old#30_"
HMMG,ISO-TC211,280450126,Missing model: 19127 Geodetic register,open,Missing models,https://github.com/ISO-TC211/HMMG/issues/8,"_From @jetgeo on May 30, 2017 20:34_



_Copied from original issue: ISO-TC211/HMMG_old#31_"
HMMG,ISO-TC211,280450073,Resource management,open,,https://github.com/ISO-TC211/HMMG/issues/7,"_From @jetgeo on October 18, 2017 10:26_

Email from Andrew Dryden to Therése Andrén, August 14th 2017:

> Hi Therése: There is no new additional news in general on maintenance portals.  However, I have a small project to do these next months to try to re-assess the use of the maintenance portal for all the standards that are using and pointing to it.
> 
> From what I can tell, in TC211 we have the following standards that have data posted on the Maintenance Portal, the date the folders were created and the date that files were last uploaded.
> 
> Also below is the TMB resolution that we are asked to follow.
> 
> TECHNICAL MANAGEMENT BOARD RESOLUTION 38/2017
> 
> Use and updating of RFC 5141-compliant URLs (based on URN namespace) in ISO deliverables 
> 
> Adopted at the 68th meeting of the Technical Management Board, Kuala Lumpur (Malaysia), 21-22 February 2017
> 
> The Technical Management Board,
> 
> Notes that the use of RFC 5141-compliant URLs in ISO deliverables can continue provided that a framework for their use is clearly defined,
> 
> Requests that ISO/CS prepare instructions for the use of RFC 5141-compliant URLs in ISO standards with the following clarifications: 
> 
> RFC 5141-compliant URLs in ISO deliverables:
> 
> 1.    Can be used to store information referred to in an informative manner from the ISO deliverable provided that a good reason is given for why this information cannot be included in the master (PDF) file;
> 
> 2.    Can be used to store normative parts of the ISO deliverable only if there is a good reason why these parts cannot be included as part of  the master (PDF) file (for example, Excel format), on the condition that any modification of this information is subject to the same process as for any other part of the standard (i.e. the revision process) and no changes can be made in between revisions (no access to modify this site is provided outside ISO/CS);
> 
> 3.    Can be used to store information requiring regular updates and that is part of a normative element of the ISO deliverable, provided that the corresponding process has been followed in line with the ISO/IEC Directives (e.g. establishment of a Maintenance agency or Registration Authority) and validated by ISO/CS;
> 
> 4.    Shall appear in the ISO deliverable with their full web address (ex  http://standards.iso.org/iso/tr/9999/-1/ed-1/en/) and not with a hidden hyperlink;
> 
> Instructs ISO committee secretaries who need to request the use of such RFC 5141-compliant URLs to contact their ISO TPM who will ensure that the above process has been followed before allocating the space for this RFC 5141-compliant URLs; and
> 
> Requests that this subject be revisited and included in the next edition of the ISO/IEC Directives, Part 2.
> 
> **
> I know we have already addressed the issue for 19160-1, so that one is taken care of.  19155-2 is ongoing…  I think for the other documents that are highlighted, we would need to know the answers for these.
> 
> Also, I see that 19111, 19115-2, 19136, and 19139 are under revision, so it is an especially good opportunity now to re-confirm that the maintenance portal is really needed for each…or if we can take the extra files and just make them into a zip file to be published with the standard and get rid of their maintenance portal folder.
> 
> I’ll try to go through each of these documents quickly soon to see whether their current use of the folder is normative or informative, and whether I think these files could be suitable just to go in a zip file….
> 
> In the meantime, if you can share any information on these, let me know at your convenience…
> 
> Thanks!
> Andy


_Copied from original issue: ISO-TC211/HMMG_old#33_"
HMMG,ISO-TC211,280449950,Resource management,open,,https://github.com/ISO-TC211/HMMG/issues/6,"_From @jetgeo on October 18, 2017 10:26_

Email from Andrew Dryden to Therése Andrén, August 14th 2017:

> Hi Therése: There is no new additional news in general on maintenance portals.  However, I have a small project to do these next months to try to re-assess the use of the maintenance portal for all the standards that are using and pointing to it.
> 
> From what I can tell, in TC211 we have the following standards that have data posted on the Maintenance Portal, the date the folders were created and the date that files were last uploaded.
> 
> Also below is the TMB resolution that we are asked to follow.
> 
> TECHNICAL MANAGEMENT BOARD RESOLUTION 38/2017
> 
> Use and updating of RFC 5141-compliant URLs (based on URN namespace) in ISO deliverables 
> 
> Adopted at the 68th meeting of the Technical Management Board, Kuala Lumpur (Malaysia), 21-22 February 2017
> 
> The Technical Management Board,
> 
> Notes that the use of RFC 5141-compliant URLs in ISO deliverables can continue provided that a framework for their use is clearly defined,
> 
> Requests that ISO/CS prepare instructions for the use of RFC 5141-compliant URLs in ISO standards with the following clarifications: 
> 
> RFC 5141-compliant URLs in ISO deliverables:
> 
> 1.    Can be used to store information referred to in an informative manner from the ISO deliverable provided that a good reason is given for why this information cannot be included in the master (PDF) file;
> 
> 2.    Can be used to store normative parts of the ISO deliverable only if there is a good reason why these parts cannot be included as part of  the master (PDF) file (for example, Excel format), on the condition that any modification of this information is subject to the same process as for any other part of the standard (i.e. the revision process) and no changes can be made in between revisions (no access to modify this site is provided outside ISO/CS);
> 
> 3.    Can be used to store information requiring regular updates and that is part of a normative element of the ISO deliverable, provided that the corresponding process has been followed in line with the ISO/IEC Directives (e.g. establishment of a Maintenance agency or Registration Authority) and validated by ISO/CS;
> 
> 4.    Shall appear in the ISO deliverable with their full web address (ex  http://standards.iso.org/iso/tr/9999/-1/ed-1/en/) and not with a hidden hyperlink;
> 
> Instructs ISO committee secretaries who need to request the use of such RFC 5141-compliant URLs to contact their ISO TPM who will ensure that the above process has been followed before allocating the space for this RFC 5141-compliant URLs; and
> 
> Requests that this subject be revisited and included in the next edition of the ISO/IEC Directives, Part 2.
> 
> **
> I know we have already addressed the issue for 19160-1, so that one is taken care of.  19155-2 is ongoing…  I think for the other documents that are highlighted, we would need to know the answers for these.
> 
> Also, I see that 19111, 19115-2, 19136, and 19139 are under revision, so it is an especially good opportunity now to re-confirm that the maintenance portal is really needed for each…or if we can take the extra files and just make them into a zip file to be published with the standard and get rid of their maintenance portal folder.
> 
> I’ll try to go through each of these documents quickly soon to see whether their current use of the folder is normative or informative, and whether I think these files could be suitable just to go in a zip file….
> 
> In the meantime, if you can share any information on these, let me know at your convenience…
> 
> Thanks!
> Andy


_Copied from original issue: ISO-TC211/HMMG_old#33_"
HMMG,ISO-TC211,280449915,Missing model: 19166 BIM to GIS conceptual mapping (B2GM),open,Missing models,https://github.com/ISO-TC211/HMMG/issues/5,"_From @jetgeo on November 27, 2017 1:10_



_Copied from original issue: ISO-TC211/HMMG_old#34_"
HMMG,ISO-TC211,280449883,New model: 19161-1 Geodetic references -- Part 1:  The international terrestrial reference system (ITRS),open,Missing models,https://github.com/ISO-TC211/HMMG/issues/4,"_From @jetgeo on November 27, 2017 1:24_



_Copied from original issue: ISO-TC211/HMMG_old#35_"
HMMG,ISO-TC211,280449802,New model: 19130-3 Imagery sensor models for geopositioning -- Part 3: Implementation Schema,open,Missing models,https://github.com/ISO-TC211/HMMG/issues/3,"_From @jetgeo on November 27, 2017 1:26_



_Copied from original issue: ISO-TC211/HMMG_old#36_"
HMMG,ISO-TC211,280449683,Missing model: 19119:2016 Services,open,Published,https://github.com/ISO-TC211/HMMG/issues/1,"_From @jetgeo on November 30, 2017 1:39_



_Copied from original issue: ISO-TC211/HMMG_old#38_"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
UncoveringTemporalStructureHippocampus,kemerelab,359907290,AttributeError: 'AnalogSignalArray' object has no attribute '_data',open,,https://github.com/kemerelab/UncoveringTemporalStructureHippocampus/issues/2,"Hi, 

Thanks for sharing the notebooks. I am trying to run them, however, the code breaks because of the missing `_data` attribute. A simple example:

```python
import nelpy as nel

datadirs = ['data/']
fileroot = next( (dir for dir in datadirs if os.path.isdir(dir)), None)

if fileroot is None:
    raise FileNotFoundError('datadir not found')

load_from_nel = True

if load_from_nel:
    jar = nel.load_pkl(fileroot + 'fig1.nel')
    exp_data = jar.exp_data
    aux_data = jar.aux_data
    del jar

print(exp_data)

```

Fails with: 

```
Traceback (most recent call last):
  File ""/home/pietro/Envs/basic36/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-35-1adcaa0a1d8f>"", line 22, in <module>
    print(exp_data)
  File ""/home/pietro/Envs/basic36/lib/python3.6/site-packages/nelpy/core/_analogsignalarray.py"", line 1053, in __repr__
    if self.isempty:
  File ""/home/pietro/Envs/basic36/lib/python3.6/site-packages/nelpy/core/_analogsignalarray.py"", line 1917, in __getattr__
    return object.__getattribute__(self, name)
  File ""/home/pietro/Envs/basic36/lib/python3.6/site-packages/nelpy/core/_analogsignalarray.py"", line 1194, in isempty
    return self.data.shape[1] == 0
  File ""/home/pietro/Envs/basic36/lib/python3.6/site-packages/nelpy/core/_analogsignalarray.py"", line 1917, in __getattr__
    return object.__getattribute__(self, name)
  File ""/home/pietro/Envs/basic36/lib/python3.6/site-packages/nelpy/core/_analogsignalarray.py"", line 1104, in data
    return self._data
  File ""/home/pietro/Envs/basic36/lib/python3.6/site-packages/nelpy/core/_analogsignalarray.py"", line 1917, in __getattr__
    return object.__getattribute__(self, name)
AttributeError: 'AnalogSignalArray' object has no attribute '_data'
```

Essentially, nothing inside `exp_data` seems to have this `_data` attribute and so it breaks everywhere. "
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmmcopy_utils,shahcompbio,355336915,BamStandardIndex ERROR: could not generate a summary of index file,open,,https://github.com/shahcompbio/hmmcopy_utils/issues/1,"readCounter cannot generate a summary of index file after building the index:

```bash
$ readCounter -b FDAV_COLO829rep1_m100x0_CL_Whole_T1_KAS5U_L03520.bam
Building index for FDAV_COLO829rep1_m100x0_CL_Whole_T1_KAS5U_L03520.bam, please hold
Index creation successful
$ readCounter FDAV_COLO829rep1_m100x0_CL_Whole_T1_KAS5U_L03520.bam
BamStandardIndex ERROR: could not generate a summary of index file FDAV_COLO829rep1_m100x0_CL_Whole_T1_KAS5U_L03520.bam.bai, aborting index load
BamRandomAccessController ERROR: could not load index data from file: FDAV_COLO829rep1_m100x0_CL_Whole_T1_KAS5U_L03520.bam.bai
Could not locate valid BAM index for input file. Build index with -b
$ ls -l FDAV_COLO829rep1_m100x0_CL_Whole_T1_KAS5U_L03520.bam.bai
-rw-r--r-- 1 rrichholt domainuser 6168448 Aug 29 11:09 FDAV_COLO829rep1_m100x0_CL_Whole_T1_KAS5U_L03520.bam.bai
```

It seems to work fine when using samtools to create the index."
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMM,dohmatob,4943710,"baumwelch seems to converge to wrong model, on multiple o sequences",open,bug,https://github.com/dohmatob/HMM/issues/1,"baumwelch works on a single observation sequence but seems to converge to wrong model, on multiple o sequences.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
mouse-gesture-hmm,OmarBarakat1995,288141386,Fixed for Python 3.6.,open,,https://github.com/OmarBarakat1995/mouse-gesture-hmm/pull/2,"Fixed multiple issues faced  when using the code with Python 3.6.
Also removed *.pyc files which are generated at user site using .gitignore."
mouse-gesture-hmm,OmarBarakat1995,260120848,Add sampling feature to gui_collect.py,open,,https://github.com/OmarBarakat1995/mouse-gesture-hmm/pull/1,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
ZZZ-metal-poetry-skill,JarbasAI,272259024,Syntax error on intent invocation,open,,https://github.com/JarbasAI/ZZZ-metal-poetry-skill/issues/2,"Whenever I say 'Hey Mycroft, recite a poem' I receive the following error:

```
/opt/mycroft/skills/mycroft-poetry-skill/__init__.py"", line 95, in handle_poetry_intent
style = message.data.get(""Style"", style = random.choice(self.styles))
TypeError: get() takes no keyword arguments
```"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
WorkstationCrafting,Terasology,280197258,Update fonts to Noto Sans,open,,https://github.com/Terasology/WorkstationCrafting/pull/6,
WorkstationCrafting,Terasology,185461578,Migrate the generator items to NeoTTA,open,long term,https://github.com/Terasology/WorkstationCrafting/issues/2,"Currently, under items/generator, there are a bunch of spawn definitions, item generators, and a world generator. Not to mention all of the deltas and overrides in this module.

All of the first group, and some of the second group should probably be moved into their own world-based module (like TTA was) so that this doesn't conflict with other modules. In the future, I'm intending this module to be a baseline Workstation Crafting module that other modules expand upon. That is, a content-focused module rather than a game or adventure-focused one. But having these generator files can mess with other modules that intend to provide a specific experience.

So moving these plus some of the things left behind in WoodAndStone (like the Journal system and ModifyBlockSystemDestruction) will help this module become easier to extend and have less side-effects. 

This will be a long-term project, as the NeoTTA module will have to be slowly built up bythe pieces from old and new modules.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
learning-hmm,mnacamura,167052135,[Help] Is there any example which uses this library?,open,,https://github.com/mnacamura/learning-hmm/issues/7,"I need to use it for my project but can't seem to find any example/guide
"
learning-hmm,mnacamura,49682021,Add tests,open,,https://github.com/mnacamura/learning-hmm/issues/2,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
DupHMM,KorfLab,50392978,Window size above 500bp fails to find CNV,open,,https://github.com/KorfLab/DupHMM/issues/1,"Hi, 

I ran DupHMM with four window sizes selected (500, 1000, 5000, 10000).  The 500 bp size worked fine, I saw plenty of CNVs in the results files for each chromosome.  However, the other three sizes only had the entire length of the chromosome as one CNV in the results file.  The window sizes are much shorter than the length of the chromosomes, so this clearly shouldn't have happened.  Hopefully there is a fix for this problem!

Thank you
Jess
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
NLP-participle-and-speech-tagging,jimth001,315390568,请问数据在哪找,open,,https://github.com/jimth001/NLP-participle-and-speech-tagging/issues/2,
NLP-participle-and-speech-tagging,jimth001,198656781,readme.md中的语料文件能push到github不,open,,https://github.com/jimth001/NLP-participle-and-speech-tagging/issues/1,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMM-Persian-OCR,Kianenigma,200805713,"I have run the test example, but I got a quite different result..",open,,https://github.com/Kianenigma/HMM-Persian-OCR/issues/1,"I think there must be something wrong but I don't know where it is. I have followed your readme instructions to install all the dependencies. Then I ran the test example and get the following result:

```
====================== HTK Results Analysis =======================
  Date: Mon Jan  9 16:17:27 2017
  Ref : samplesRef.mlf
  Rec : res0.mlf
------------------------ Overall Results --------------------------
SENT: %Correct=3.00 [H=3, S=97, N=100]
WORD: %Corr=3.00, Acc=3.00 [H=3, D=0, S=97, I=0, N=100]
===================================================================
```

I can't figure out why the accuracy would be so low. And I notice that you mentioned in the Usage example that test/text_repo contains a set of 1000 persian word. But I find the folder I download contains 3315 lines of persian word. Does that have something to do with the low accuracy?
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
toybox,podhmo,218285122,try plaster,open,,https://github.com/podhmo/toybox/issues/4,
toybox,podhmo,214880203,swiching wsgi server example,open,,https://github.com/podhmo/toybox/issues/3,"- [ ] gunicorn
- [ ] uwsgi

or with asyncio feature?"
toybox,podhmo,214880007,more examples,open,,https://github.com/podhmo/toybox/issues/2,"- [x] websocket
- [x] static serve
- [ ] cache
- [ ] simple CRUD(ORM?)
- [x] oauth2 login
- [x] jwt
- [x] jsonrpc
- [ ] using toybox from pyramid (via pserve and .ini)
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
SVM_HMM,sumanvravuri,23889430,Background paper to understand this implementation,open,,https://github.com/sumanvravuri/SVM_HMM/issues/1,"Hi

Is there a background paper or tutorial which can used to understand the formulation of structured SVM?
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
corHMM,thej022214,375724775,Add support for multi-state individual traits in rate.mat.maker,open,,https://github.com/thej022214/corHMM/issues/9,"The command
`rate.mat.maker(rate.cat = 1, ntraits = 1, nstates = 3, model = ""ARD"")`
Results in the following rate index matrix:
```
    (0) (1)
(0)  NA   2
(1)   1  NA
```
However, for a three-state character, should the output not be something like:
```
    (0) (1) (2)
(0)  NA   3   5
(1)   1  NA   6
(2)   2   4  NA
```
"
corHMM,thej022214,346394917,rayDISC assumes first column is taxon names,open,,https://github.com/thej022214/corHMM/issues/8,"This needs to be made clearer in documentation or a workaround / check should be implemented. Right now it results in a ""Character X is invariant"" message if the first column is not taxon names."
corHMM,thej022214,301924814,Error with plotRECON,open,bug,https://github.com/thej022214/corHMM/issues/5,"Calling plotRECON with a matrix holding ancestral states produces:
```
Error in as.graphicsAnnot(legend) : 
  argument ""legend"" is missing, with no default
```"
corHMM,thej022214,261114390,Missing states ,open,,https://github.com/thej022214/corHMM/issues/4,"Turns out different functions have different requirements for how to score missing characters. In corHMM ""NA"" is considered missing, whereas in rayDISC ""?"" assumes it is missing. 

This needs to be standardized. "
corHMM,thej022214,258816659,Dealing with tip states with joint recon under a HMM,open,,https://github.com/thej022214/corHMM/issues/3,"Unclear to me how the joint algorithm would get the tip states under the HMM. In Pupko, the tip states are known, and thus the first calculation involves determining the likeliest state for the ancestor given the knowledge of the tip state. Under the HMM, the tip is in multiple states. So, is the proper way to do this, to simply assume that the tip is in each of the possible states and then take the one that maximizes the probability for it and it's ancestor? That seems right, but a bit more tricky than I had originally implemented this."
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
mingle,Ecogenomics,104571037,Verify sequence names,open,,https://github.com/Ecogenomics/mingle/issues/23,"There are a number of characters that are invalid within user supplied sequences. It would be good to do an initial check for these characters: ,;()|~.
"
mingle,Ecogenomics,104168722,Show orientation of pre- and post-genes,open,,https://github.com/Ecogenomics/mingle/issues/22,"It would be helpful if the pre- and post-genes reported by mingle indicated their orientation on the contig. This would allow assessment of conserved gene order and orientation across genomes.
"
mingle,Ecogenomics,82851986,Update annotatedHitFasta.py,open,,https://github.com/Ecogenomics/mingle/pull/19,
mingle,Ecogenomics,55555672,Re-add output .greengenes file format removed as part of Joel's commit,open,,https://github.com/Ecogenomics/mingle/issues/3,"commit
https://github.com/geronimp/mingle/commit/31e5d2c61db6e173ef34122ba9856f6592d9c51f
"
mingle,Ecogenomics,54274672,additional ; at the end of taxonomy string confuses t2t,open,,https://github.com/Ecogenomics/mingle/issues/1,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm-la-bd,Mogmi95,66718753,Number strips with leading zeroes.,open,,https://github.com/Mogmi95/hmm-la-bd/pull/1,"This helps with sorting the strips when doing `ls` or when browsing on
github.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMM_Python,long-johnson,307906884,Can you share the data?,open,,https://github.com/long-johnson/HMM_Python/issues/10,"Hi,
in your code:
root_path = 'data\Daily and Sports activity recognition'

Can you share the data above?
Thanks! & Best Regards,
Ardeal Liang"
HMM_Python,long-johnson,207044197,Add method for dumping HMM to file and loading from file,open,,https://github.com/long-johnson/HMM_Python/issues/9,
HMM_Python,long-johnson,207041315,Write a better procedure for initial params estimation,open,,https://github.com/long-johnson/HMM_Python/issues/8,"http://stats.stackexchange.com/questions/47846/how-to-define-initial-probabilities-for-hmm
http://stackoverflow.com/questions/13966699/hidden-markov-model-initial-guess
http://scikit-learn.org/stable/modules/mixture.html"
HMM_Python,long-johnson,124599989,Think over and maybe fix the log-sum-exp scaling procedure,open,question,https://github.com/long-johnson/HMM_Python/issues/3,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
IMCoalHMM,mailund,240394524,Make a conda package,open,enhancement,https://github.com/mailund/IMCoalHMM/issues/2,Make a conda package for mini-ziphmm and this package to make it easier to install it.
IMCoalHMM,mailund,38503795,Kill the entire job when a sub-process dies,open,bug,https://github.com/mailund/IMCoalHMM/issues/1,"In the multiprocess tasks if one child dies the program hangs. It should terminate instead.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
teHmm,glennhickey,150960418,ImportError: No module named teHmm.common,open,,https://github.com/glennhickey/teHmm/issues/1,"Hi there,
i tried to run ./setup.sh under teHmm, however, there is an error popping up saying 
"" from teHmm.common import checkRequirements
ImportError: No module named teHmm.common""

I installed other dependencies, but I am not sure where to get this teHmm.common module. Do you have an idea?

Thanks very much!

Zhenzhen
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
chromatin_states_chromHMM_mm9,gireeshkbogu,136176613,can't unzip file,open,,https://github.com/gireeshkbogu/chromatin_states_chromHMM_mm9/issues/2,"Hi there,
I can't unzip the file.  Please help.
Thanks,
Clayton
1. 
   [ckc620@qhimem0001 Kai]$ ls
   mESC_cStates_HMM.zip

2.
[ckc620@qhimem0001 Kai]$ unzip mESC_cStates_HMM.zip 
Archive:  mESC_cStates_HMM.zip
  End-of-central-directory signature not found.  Either this file is not
  a zipfile, or it constitutes one disk of a multi-part archive.  In the
  latter case the central directory and zipfile comment will be found on
  the last disk(s) of this archive.
unzip:  cannot find zipfile directory in one of mESC_cStates_HMM.zip or
        mESC_cStates_HMM.zip.zip, and cannot find mESC_cStates_HMM.zip.ZIP, period.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
sinsy,zyamusic,254647818,"when try to run the ./configure , it gives"" configure: error: Cannot find HTS_engine.h""",open,,https://github.com/zyamusic/sinsy/issues/1,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
WordSegment,liuhuanyong,379404374,测试hmm例子的时候出现bug求告知,open,,https://github.com/liuhuanyong/WordSegment/issues/2,
WordSegment,liuhuanyong,358900002,这里很容易出现超出最小值 得到-Infinity,open,,https://github.com/liuhuanyong/WordSegment/issues/1,https://github.com/liuhuanyong/WordSegment/blob/b0486271215a08f4cf859689ecc10974cde799d6/max_ngram.py#L49
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,klangner,336705829,Implementation of forward and backward algorithm,open,,https://github.com/klangner/hmm/issues/5,"Hi

I found your library and it appears to be the only HMM library in Rust. It's a bit of a shame that only viterbi is implemented and I'll see whether I can find the time and imement the forward backward algorithm.

Cheers"
hmm,klangner,213274213,Replace matrices.rs with ndarray,open,,https://github.com/klangner/hmm/issues/3,[ndarray](https://docs.rs/ndarray/0.8.0/ndarray/) has become the de-facto standard for matrix handling in Rust. I suggest to use that instead of building your own matrix implementation.
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
BP-AR-HMM,HongminWu,366418944,copyright of this repo,open,,https://github.com/HongminWu/BP-AR-HMM/issues/1,"Hi, 
I  want to know if this repo comes from E. B. Fox and modified by Scott Niekum? Because I searched the website and found no information about this package in Scott Niekum's personal homepage or his paper. Actually, I want to reproduce Scott Niekum's experiment in one of his paper, so I need to confirm he used this package in his experiment. Any feedback is welcomed. 
Thank you."
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMM-Aligner,sfu-natlang,240741003,Implementing Toutanova improved HMM alignment models,open,,https://github.com/sfu-natlang/HMM-Aligner/issues/9,"As described in this paper:

https://nlp.stanford.edu/manning/papers/hmmalign.pdf

"
HMM-Aligner,sfu-natlang,236271735,hmm with numpy ,open,question,https://github.com/sfu-natlang/HMM-Aligner/issues/2,"Here is a sample source code to examine for how to implement hmm algorithms using numpy operations.

http://www.cs.toronto.edu/~rfm/code/hmm.py

"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
lvm4j,dirmeier,254431380,"Add jprofile, mockito, visualvm and jmeter to test mvn lifecycle",open,,https://github.com/dirmeier/lvm4j/issues/3,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
april-ann,april-org,188680115,build error,open,,https://github.com/april-org/april-ann/issues/192,"I tried to build april-ann in ubuntu 16.04. After executing DEPENDENCIES-INSTALLER.sh which was successful, there was some errors during make:

`/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `zggsvd3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `cggsvd3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `sgghd3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `cggev3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `zgghd3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `cgges3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `cgetrf2_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `zgesvj_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `dgetrf2_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `cggsvp3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `zggsvp3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `sggev3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `zggev3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `zgesvdx_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `spotrf2_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `sgges3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `sgetrf2_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `cgesvdx_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `zpotrf2_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `dggsvd3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `dgghd3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `dggev3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `zgejsv_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `sbdsvdx_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `dpotrf2_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `sggsvp3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `dgesvdx_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `dgges3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `zgges3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `dbdsvdx_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `zgetrf2_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `cgesvj_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `sgesvdx_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `sggsvd3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `dggsvp3_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `cpotrf2_'
/usr/lib/gcc/x86_64-linux-gnu/5/../../../../lib/liblapacke.so: undefined reference to `cgghd3_'
collect2: error: ld returned 1 exit status
Error 1 in .
`

How can i fix this error?"
april-ann,april-org,184345504,Solved bug at masked_fill and masked_copy Lua bindings,open,,https://github.com/april-org/april-ann/pull/191,
april-ann,april-org,110438228,"Smart pointers, Lua calls and protected calls",open,task,https://github.com/april-org/april-ann/issues/179,"In order to allow capturing errors in Lua side (via pcall or xpcall), objects allocated in C++ should be smart enough to free all pending memory in the moment of the error. LuaPkg and APRIL-ANN have been changed to implement this procedure in some automatic way, LuaPkg wraps every C++ call using a try/catch block, and APRIL-ANN macros ERROR_EXIT throw an exception consisting in a simple char \* message. This procedure allows to execute automatic C++ cleanup when the exception is thrown followed by the longjmp due to Lua errors.
Our problem is that not all C++ objects are error-safety, that is, some of them allocate memory in its methods and in case of error the memory stills allocated. A simple way to face this problem is to wrap all C++ allocations using `AprilUtils::SharedPtr` or `AprilUtils::UniquePtr`, so in case of error the allocated memory would freed automatically.
"
april-ann,april-org,107463349,OMP + fork hanging up child process,open,bug,https://github.com/april-org/april-ann/issues/173,"It is known that there are problems with fork and OMP, we still looking a way to solve this problem in APRIL. The problem shows up when using `parallel_foreach()` function because it uses fork to create new childs.
"
april-ann,april-org,61992847,Using train.lua with silence switches and  --initial_hmm complains for missing --num-states  switch,open,bug,https://github.com/april-org/april-ann/issues/136,"While trying to execute the train.lua script using  --begin-sil, --end-sil, --silence  switches although I set variable number of states for each model using the switch --initial_hmm the train.lua command is complaining for missing the --num-states switch(Number of states per each HMM).
"
april-ann,april-org,60151577,Silent error in dataset.union,open,bug,https://github.com/april-org/april-ann/issues/133,"It fails with segment violation when it is get a dataset.token instance. Instead of the segmentation error, it would be better to throw a message error.
"
april-ann,april-org,57688676,String parser for CNNs construction,open,new feature,https://github.com/april-org/april-ann/issues/129,"Current syntax in all_all MLPs (`ann.mlp.generate.all_all`):

`120 inputs 20 logistic 12 tanh 40 softmax`

The new format should work with linear projection layers (as dense or kernels keywords?) and activation function layers. For instance:

`120 inputs 20 dense logistic 12 dense tanh 40 dense softmax`

would splitted by the parser into:

`120 inputs | 20 dense | logistic | 12 dense | tanh | 40 dense | softmax`

CNNs would need:
- data dimensionality structure
- kernels
- pooling layers

Something like this:

`61x61 inputs 8 3x3 kernels logistic 3x2 maxpool 20 4x4 kernels logistic 256 dense logistic 61 dense softmax`

The parser interprets strings like:

`12`

`12x5`

in general numberXnumberXnumber... assuming it is a matrix where number of dimension and its sizes is given by the previous format. Additionally, the stride of the kernel can be added as follows:

`6x6+1+1`

A greedy parser can traverse in left-to-right the string, storing the last dimension sizes and strides.

Example:

`61x61 inputs` builds a window/matrix with 2 dimensions

`8 6x6 kernels` builds 8 kernels with a receptive field of 6x6

We need two use cases:
- giving the parser a string, it returns an ANN to train (an ANN component).
- giving the parser a string and an ANN (usually trained), it returns another ANN which can be used to speed-up evaluation phase.

Syntax of the format:

```
CNN -> BEGIN LAYER+
BEGIN -> MATRIX inputs
LAYER -> NUM MATRIX kernels
        MATRIX maxpool
        NUM dense
        ACTF
ACTF -> logistic | tanh | softmax | ...
any of the activation functions in Lua table ann.components.actf
```
- A matrix is promoted to a vector when dense is applied.
- NUM MATRIX kernels builds NUM times a kernel with receptive field size given by MATRIX.
- ACTF applied to a matrix or vector keeps its shape.
"
april-ann,april-org,43155735,"Travis CI, Homebrew and shared library",open,compiling,https://github.com/april-org/april-ann/issues/93,"It is found an issue when compiling APRIL-ANN as shared library in Travis CI MacOS X platform, which uses Homebrew for libraries installation. The linker fails when looking for `-llua`.
"
april-ann,april-org,42693233,Const arguments in interest_points and other similar packages,open,task,https://github.com/april-org/april-ann/issues/88,"I have noticed that some functions in this package are receiving input arguments as pointers to **non-const** memory. It will be safe and more idiomatic for programming to declare input arguments as pointers to **const** memory. Check it that is possible.
Thanks!
"
april-ann,april-org,29063083,Coding style guide,open,informative,https://github.com/april-org/april-ann/issues/73,"Please, follow (and complete if necessary) the `STYLEGUIDE.md` file. It describes the coding style of April-ANN for new and fresh code. Old code could be in a different style, but it will be homogenized as far as we have time to expend into.
"
april-ann,april-org,20932460,April-ANN packages => Lua modules,open,informative,https://github.com/april-org/april-ann/issues/40,"In order to convert current April-ANN packages in Lua modules we need to:
1. Remove any lateral effect, as the declaration of GLOBALs. The `util` package could be an exception to this rule, because it replaces some Lua functions as `type` or `table.insert`, and extends other Lua modules, as `math`, `string`, etc.
2. Packages must be developed self-contained, in the sense of the Lua tables where the packages bind their functionalities. One package means one Lua table with the name `aprilann.PACKAGE_NAME`.
3. Bindings must declare ONLY one Lua table in the GLOBALs.
4. Formiga will prepare luaopen_aprilann_PACKAGE_NAME functions to return the table declared with the name aprilann.PACKAGE_NAME.

Any suggestions?
"
april-ann,april-org,20883954,Development protocol,open,informative,https://github.com/april-org/april-ann/issues/39,"1. Every feature has to be developed in a new branch. Contributors can have its own fork and work in new branches for new features.
2. Once a feature is finished, a pull requests into `devel` branch is mandatory, unless commits with bug or memory leak problems solving which can be done directly into `devel` without a pull request. Please, avoid to do merge of feature branches into `devel`, a pull request allow other developers and contributors to talk about the feature before it is merged into `deve`.
3. After the pull request, the branch must to be deleted.
4. Every friday, the stable version in `devel` branch will be merged with `master` branch.
5. Every friday, after previous step, your stable changes could be committed to the branch `devel`.
6. During the week, `devel` branch must be checked and mark as stable.

Pushes to devel are throwing a travis process to ensure the good compilation and testing the tool. The same is configured to pushes and pull requests to master branch.
"
april-ann,april-org,19396275,Move dataset classes to other tables,open,enhancement,https://github.com/april-org/april-ann/issues/37,"I'm thinking in  dataset Lua table reorganization, for  example, putting dataset.salt_noise in dataset.filter.salt_noise. It will be better to create some hierarchy in dataset, which could denote better the purpose of the class.
"
april-ann,april-org,19143226,Class defined in utils/context.h,open,enhancement,https://github.com/april-org/april-ann/issues/36,"I don't like the implementation of contextualizer in utils/context.h. It is implemented sliding the elements or a fixed size vector, so every insertion, one time the vector ismfull, needs to move all the elelemnts.

It will be better to use a circular vector, so insertion cost will be constant. The oprator[] must be re-written to,return the correct position in the circular vector.
"
april-ann,april-org,18546353,Addition of changes to CHANGELIST.md,open,informative,https://github.com/april-org/april-ann/issues/34,"Please, when the master branch is changed, add to the top of CHANGELIST.md
file your updates. In this way, when a release is deployed, it is easier to produce
the list of changes from previous version.
"
april-ann,april-org,16685655,Problem with OPENMP and MAC OS X,open,informative,https://github.com/april-org/april-ann/issues/27,"The official apple version of gcc is llvm-gcc-4.2, which has an important BUG with OpenMP. Currently, the macosx targets are compiled with NO_OMP definition. In the future this BUG could be solved, and the NO_OMP definition will be removed. 
"
april-ann,april-org,12506429,"Move function get_replacement_dataset(randObject, replacementSize, ...) to utils/dataset",open,question,https://github.com/april-org/april-ann/issues/13,"This function could be useful for many scripts. It should be available on utils. The function gets a randomObject, replacement size and a variable number of datasets and returns a tables with the same replacement for each dataset.
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
cursiveHMM-cpp,mistycheney,120720755,Error,open,,https://github.com/mistycheney/cursiveHMM-cpp/issues/1,"Error as this image show
http://postimg.org/image/cpsvjqrlr/
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
linearham,matsengrp,362838720,biological validation data sets,open,,https://github.com/matsengrp/linearham/issues/46,"Following Duncan's idea in lab meeting, let's talk about biological validation data. Simulation data is straightforward.

* Chaim I guess has some data that would be appropriate due to depth of sampling?
* There are experiments in which a mouse is engineered with a given naive sequence.
* There are situations for HIV bnabs in which there are some samples that are close to the root, and some farther away. The samples close to the root ""reveal"" the naive sequence. Do we agree?"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
SpeechRecognition,SKantar,140611716,How to use this application ,open,,https://github.com/SKantar/SpeechRecognition/issues/1,"I just downloaded this project, but I cant use it on my computer. I clicked the start button and said something then stop button. When I click recognition button, nothing happened. Any instruction about how to use this app?
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
cmv,eggzilla,343342045,Add directedness to transitions,open,,https://github.com/eggzilla/cmv/issues/3,Thx @choener for reporting!
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
cpg-island-prediction-HMM,devanshdalal,140747349,viterbi,open,,https://github.com/devanshdalal/cpg-island-prediction-HMM/issues/3,"![hmm-viterbi-algorithm-normal](https://cloud.githubusercontent.com/assets/5080310/13754012/72248180-ea3b-11e5-922b-506d94b6f554.png)
"
cpg-island-prediction-HMM,devanshdalal,140738186,Screenshots,open,,https://github.com/devanshdalal/cpg-island-prediction-HMM/issues/2,"![cpg_island_evolution svg](https://cloud.githubusercontent.com/assets/5080310/13752836/8f7fecb0-ea36-11e5-8ed9-f8836b098ad8.png)
![cpg_vs_c-g_bp svg](https://cloud.githubusercontent.com/assets/5080310/13752837/8fe1039c-ea36-11e5-8320-7d3b7ef53f3e.png)
![cpg_vs_c-g_bp svg5](https://cloud.githubusercontent.com/assets/5080310/13752838/8fe3c96a-ea36-11e5-8853-b40892369481.png)
![cpg_vs_c-g_bp svg75](https://cloud.githubusercontent.com/assets/5080310/13752839/8fe4a63c-ea36-11e5-9b9c-d8b1e3c84702.png)
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hello-world,Thatgeo42,50494250,Just do it!!!,open,,https://github.com/Thatgeo42/hello-world/issues/1,"Make dot.tk domain
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Notgay,Akigay,233061028,Pls help !!,open,,https://github.com/Akigay/Notgay/issues/1,Can anyone tell me what problem is this ? [https://s3.postimg.org/482r1f52b/image.jpg](url)
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
h-w,caitlin-woods,190318894,Rename README.md to README-edits.md,open,,https://github.com/caitlin-woods/h-w/pull/1,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Magictech,rohit29121990,264275258,player,open,,https://github.com/rohit29121990/Magictech/issues/1,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hello-world,Godzillaa,82103167,Bummer,open,,https://github.com/Godzillaa/hello-world/issues/1,"I forgot how to code...
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
ssg15,v0l,93996209,Not enough honey roasted cashew nuts,open,bug,https://github.com/v0l/ssg15/issues/2,"Expected output: 6
Actual output: 5
"
ssg15,v0l,92136623,Add mob level flow,open,enhancement,https://github.com/v0l/ssg15/issues/1,"http://dev.steamdb.party/analysis
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
testailua2,Sartani,228029763,Testi issue 1,open,question,https://github.com/Sartani/testailua2/issues/4,Kahtotaan 
testailua2,Sartani,210833251,esimerkki1,open,bug,https://github.com/Sartani/testailua2/issues/2,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hello-world,akosho,146909777,do something,open,,https://github.com/akosho/hello-world/issues/2,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
repository,zadanie,51777454,feature3,open,enhancement,https://github.com/zadanie/repository/issues/7,
repository,zadanie,51777445,feature2,open,enhancement,https://github.com/zadanie/repository/issues/6,
repository,zadanie,51777424,feature1,open,enhancement,https://github.com/zadanie/repository/issues/5,
repository,zadanie,51070121,bug2,open,bug,https://github.com/zadanie/repository/issues/3,
repository,zadanie,51070067,bug1,open,bug,https://github.com/zadanie/repository/issues/2,
repository,zadanie,51070029,ab,open,bug,https://github.com/zadanie/repository/issues/1,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
recipes,jsocol,43784342,[book club] September 2014 Menu,open,,https://github.com/jsocol/recipes/issues/1,"[Egusi Soup](http://allrecipes.com/recipe/egusi-soup/)
- [x] 3/4 C pumpkin seeds
- [x] 1½ lbs stew beef, cubed
- [x] ½ C peanut (safflower) oil
- [x] 2 large tomatoes, chopped
- [x] 1 small onion, chopped
- [x] 2 habenero peppers, minced and seeded
- [x] 18oz tomato sauce
- [x] 3 tbps tomato paste
- [x] 1½ C water
- [x] 2 lbs shrimp, peeled and deveined
- [x] 1 lbs spinach, chopped

[Semolina Fufu](http://www.nigerianfoodtv.com/2012/09/how-to-make-semo-fufu-alternativeusing.html)
- [x] Semolina flour
- [x] Water

[Veggies](http://recipes.fortwayne.com/recipe/whole-wheat-pappardelle-roasted-butternut-squash-broccoli-rabe-pumpkin-seeds) (skip pasta portion)
- [x] Broccoli Rabe, stems removed
- [x] 1-2 butternut squash
- [x] 1½ C veg. stock

Vegetarian Jollof Rice
- [x] 1 lbs rice
- [x] 28-oz can diced/crushed tomato
- [ ] small can tomato paste
- [ ] 2 onions, diced
- [ ] 4 C vegetable stock
- [ ] 3 large red bell pepper
- [ ] white pepper
- [ ] thyme
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Hello-world-,ArpeDK,64552341,test,open,,https://github.com/ArpeDK/Hello-world-/issues/1,"FInish readme file!
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
notGHMM,hwp,68684804,k-means initiate estimate. # sample = 1,open,enhancement,https://github.com/hwp/notGHMM/issues/6,"if number of samples in one cluster is one, it is not possible to estimate the variance.
current solution is set to a very small number (diagonal correction).
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
Semi_Tied_GMMs-HMMs,ajaytanwani,383964991,Undefined function or variable 'Quaternion',open,,https://github.com/ajaytanwani/Semi_Tied_GMMs-HMMs/issues/1,"When I run your matlab code Main_TP_ST_HSMM_BaxterPickPlace and Main_TP_ST_HSMM_BaxterValve. there are errors ：Undefined function or variable 'Quaternion'
error DrawPlotsStateDriven (line 273)
			Orient_Frame = Quaternion(r(i).Data(4:7,j));
error Main_TP_ST_HSMM_BaxterPickPlace (line 143)
DrawPlotsStateDriven(model, Data, [s stest], [r rtest], r_new);

Could you  re-upload the M file of Quaternion？
Thank you!"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm3,wintor12,214171551,Can you explain _reestimateMixtures method in _ContinuousHMM.py ?,open,,https://github.com/wintor12/hmm3/issues/1,"Hi,

Your code is much helpful in understanding HMM gaussain mixtures, but can you please explain the **_reestimateMixtures** method?   In this method **means_new** was initialized, but not updated with computed values. Similarly, **covars_new**  was updated with previous values, 
covars_new = self.covars    ## fix covars matrix
Can you please explain why these covars assigned to old values again?

Regards,
Anil
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,divadlo,51069576,abc,open,,https://github.com/divadlo/hmm/issues/1,
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,JNU2Deep2learning2,208626691,it's bloody difficult to use github!!!,open,,https://github.com/JNU2Deep2learning2/hmm/issues/1,hahaha
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
hmm,JNU2Deep2learning2,208626691,it's bloody difficult to use github!!!,open,,https://github.com/JNU2Deep2learning2/hmm/issues/1,hahaha
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
ofxZtS,andrebaltaz,150460984,How to run?,open,,https://github.com/andrebaltaz/ofxZtS/issues/1,"Where is description? Which IDE should i use? OS? Libraries? (OpenCV/EmguCV) etc
Please, make a simple manual.
Thank you!
"
repo_name,repo_owner,issue_id,issue_title,issue_state,issue_label,issue_html_url,issue_body
HMM,andreydung,19586694,NLP with HMM,open,,https://github.com/andreydung/HMM/issues/2,
HMM,andreydung,19586689,Speech Recognition with HMM,open,,https://github.com/andreydung/HMM/issues/1,
